Data Science:
Study of data

Study where valuable data comes from, what it represents and how it can be turned into valuable resource in the creation 
of different business strategies, increase effeciencies and recoganize new market oppurtunities

Backed by ML for analysis

Can get us incredible insights.



==============================================

Every Big Company uses Data Science

Google: Personalized adds
Facebook: Which Pages to like
Amazon and Netflix: Recomendation systems
Hedge Funds: Predict Stock Market



==========================

Work of Data Scientists:

Understand the Problem
Collect Enough Data(files/databases, APIs, Web Scraping)
Process and clean the data(Remove all the garbage and clutter)
Explore and visualize the data(draw different plots)
Analyse the data using ML
Communicate the result

API: Application Programming Interface
Two applications can communicate with one another using API.
We can request the desired data through API

Web Scraping:  Before scraping we should check the privacy policy of websites
as some websites don't allow us to scrape data from their websites


====================================

Example: We have a bank, which is facing too much loss of customers

Understand the problem: bank may want to reconect with old customers, bank wants to avoid churn
collect enough data: data present in databases of bank

Process and clean the data: Get rid of inconsistencies. 

Explore and visualize the data: Might want to get the factors responsible for decision making

Use ML:



==================================================


Data Science and ML:

ML: Making machines capable of learning by their own i.e. without 
being explicitly programmed. This learning is done from past data and experiences


AI , subset of AI is ML, subset of ML is DL



Data Science: Not a subset, rather uses ML to analyse the data
It is one of the practical applications of ML
It is much more than machine learning


data science: produces insights
ml: produces predictions



=========================================


First program in Python:

Why Python so popular:
Web Dev
Data Science
Great Starting Language


//////////////////////////////////


helloWorld.py:

print("Hello World!!")


//////////////////////////////////


We will use Jupyter Notebooks

////////////////////////////

By default print , prints in new line

///////////////////
Can use help function,  help(print)


======================================

To start Jupyter notebook server from CMD:
Type:
jupyter notebook

Locally a server runs


Cells: We can add code and text(markdown) in cells

 

Add ## in the front of text you want to convert as markdown Header
We can make notes in Jupyter notebooks



==========================================

Variables in Python

Variables: Memory Locations where we can store the data.
Everything in python is object so we don't need to declare the variables

for a cell, the last line of the cell  gives us the output

a=10
b=10
(a+b)


Here (a+b) becomes the output


Varaiable naming rules: 
Lower case, Upper case, Digit, underscore as well

Restriction: Cannot start variable name with digit.(undersc0re is fine)



===========================================

Data Types:

What happens when we say a=10


Comments: Use # instead of // as we have in Cpp

In cpp/ Java we have to define data type of variable before we start working with the variable

In python we are not required to do so.
Also same variable can store any data type.
a=10
a=20
a='string'


This is because in Python, everything is an object

In Java:

int a=10;

Java says that you need 4 bytes of space and you can store any int data there. You have to define data type as then 
java decides how much space is required.

In python:
Different here.
When we have   a=10 in Python
First we create a storage which stores 10. Then 'a' simply stores the address of storage containing 10.
Now if you say a=20, a different storage is created and a stores the new address.
In Cpp/Java same memory is overwritten.

That's why we don't need to define data type of variables.


===============================================================

type function: 
Gets us the data type currently stored in a varaiable

===========================================


Numbers:

a1=23

a2=3.4
a3=4+5j




Output:  a3: (5+6j)    // Complex numbers
//////////////////////////////////////////////////////////////


a=10  ..................................1
a=a+1....................................2

What happens here??
When 1 executed a space was created , 10 was stored there and a contains the address of that space.
Also with 10, the data, some metadata is also stored.

So in total this memory block will have the actual data and some metadata

When 2 is executed, a+1 is evaluated first. A new storage with some other metadata is created which stores the result.
Now a , instead of storing the first address, stores this new address.



/////////////////////////////////////////////////////////////////////

id Function: Gives us a unique number with regards to memory location of a variable.

For previous example:


a=10
print(id(a))
a=a+1
print(id(a))


Output:
140737068410960
140737068410992


We can clearly see the ids of a is different in two cases. That means two seperate memories were involved.


//////////////////////////////////////////////

Another peculiar thing:

a=10
b=10
print(id(a))
print(id(b))

Output:
140737068410960
140737068410960


Both are exactly same. This means both a and b are carrying the address of same memory location.
This is a optimization that python does.
This optimization is done only for numbers in the range -5 to 256, inclusive.  Numbers within this range are more common.
///////////////////////////////////////////////////////////////////////////////////////


Limit of integers

How large ints , floats etc can be stored. This is because memory in the system is limited


Java/ Cpp:  int a, some storage fixed for a, generally 4 bytes.
Python: Not really a problem. Depending on data type python reserves memory. The metadata of the variable will store how big the block
is which is storing the value.
Memory restriction not there in python. No limit to how large ints we can store by python. Only restriction is our system's memory itself.


 In Cpp , Java there is this restriction because space is reserved first. For example: long reserves 8 bytes whereas short only reserves 2 bytes.


///////////////////////////////////////////////////////////////

Arithmetic Ops:

Ops on numbers
+,-,*,/,  // (int division), **(exponent), %(Mod)

+,-,*: Pretty straight forward

/  :    10/3:   Returns a float,   3.3333333333333335.; This is different in 
other languages. In Cpp this would have been only 3. In python we get the exact value.

To get only the integer value, we need to do int division

10//3: 3



10**3:  Return 1000, Exponentiation


10%3:   returns 1, the remainder of ops



What happens first and what happens later??
We should put brackets to avoid confusion


2+3*4  # Multiplication has more priority than +
14


2*3/4 # doesn't matter which op done first, both have same priority
1.5

2+3//4  # // has better priority than + 

Output: 2


2*3//4  # * has better priority than //
Output: 1

///////////////////////////////////


No need to remember priorities. Just use brackets


///////////////////////////////////

Codes
simple interest


========================================================

Taking input from user:


input() function: takes input from user.
The data type is string

a=input()
a
# Data type of a is string by default



Convert string to int:

Use int() function,  can use help function to know more about int
Actually int() is a class
The string that we give as arguement to int() serves as arguement to the constructor of int() class

We can convert float to int as well and so on.
If we don't pass valid int, we get an error

////////////////////////////////////////
We can use float class for getting float values


//////////////////////////////////////////////////////

===================================================================

Boolean Data Type:

True and False

Why bool needed??
To check conditions


Relational Operators: Ops to check conditions

a=10
b=20
a>20

Output: False


Other ops: <, >=, <=, ==, !=


Logical Ops: Combine Multiple boolean conditions
and , or, not 


=====================================================


if else:


To implement control flow

"if this happens do this, else do that"

if boolean(condition)  :
	do something

else: 
	do something else


//////////////////////////////////////

a=True

if a:
    print("I am inside if")
else:
    print("I am inside else")


result: I am inside if


Take care of indentation. No need to add curly braces.

////////////////////////////////


Check if number is even or odd

n=int(input())
if n%2==0:
    print("No is even")
else:
    print("No is odd")

Output:
No is odd


/////////////////////////////////////////////



================================================
Using Relational and logical Ops



Take 2 inputs from user, print yes if both greater than 10

a=int(input())
b=int(input())

if a>10 and b>10:
    print("Yes")
else:
    print("No")


We are using logical op (and over here)



///////////////////////////////////////////////////////////////////////////////////

Using elif:

Allows us to implement deeper if else ladders



///////////////////////////////////////////////////
# Print largest of three nos

a=int(input())
b=int(input())
c=int(input())


if a>=c and a>=b:
    print("a is largest")
    
elif b>c and b>a:
    print("b is largest")


elif c>b and c>a:
    print("c is largest")

///////////////////////////////


Order of if, else if, else matters a lot, take care of it as well.


///////////////////////////////////////////

Nested Conditionals:

if something is true, then check if something, then execute something
#if n is odd print odd, n even then print even , if zero print 0


n=int(input())
if n%2==0:
    print("No is even")
    if n==0:
        print("n is zero")
else:
    print("No is odd")


/////////////////////////////////////////////


Logical op precedence:
Not > and > or


////////////////////////////////////

====================================================

While loops:

Want to do something repetedly
Want to execute some code repeatedly till a condition is valid.

///////////////////////////


Print 1 n times:


n =int(input())
i=1

while i<=n:
    print(1)
    i=i+1
    


/////////////////////////////////////

Check if a no is prine or not

Code:
n =int(input())
i=2
check=True

while i<=n-1:
    if n%i==0:
        print("Not Prime")
        check=False
        break
    i=i+1
if check:
    print("Prime")



///////////////////////////////////////////////


Nested while loops:

loop inside loop


Print all primes from 2 to n

n= int(input())
i=2

while(i<=n):
    j=2
    flag=True
    while(j<=i-1):
        if i%j==0:
            flag=False
            break
        j=j+1
    if flag:
        print(i)
    i=i+1    

///////////////////////////////////////////////////////////////////

Patterns:


How to approach??

Given N=4
Print:
****
****
****
****


4 rows and 4 cols

Finish 1st line, move to next line and so on


Pattern can be assumed as rectangle with some rows and cols

Answer 3 ques:

1) How many rows
2) How many cols to print for ith row
3) What to print

This can be a function of f(N,i,j)
Ques

1111
2222
3333
4444



1) N rows
2) ith row has n cols
3) printing i , this depends on row numbers



To print in sameline:  print(x,end=" ")




Triangular Patterns:


1 
1 2
1 2 3
1 2 3 4

1) N
2) i
3) j


1
2 3
3 4 5
4 5 6 7

1) N
2) i
3) for ith row: start by i,keep incrementing for i values


1
2 3
4 5 6
7 8 9 10

1) N
2) i
3)



Character Patterns:

ABCD
ABCD
ABCD
ABCD

1) N
2) N
3)  'A'+j-1    // won't wok in python

ABCD
BCDE
CDEF
DEFG

1) N
2) N
3)



Chars represnted by ASCII value and stored in memory

ord('A'): gIVES aSCII of 'A'

chr(65): Gives char corresponding to this ascii value


ord only works with single length strings

////////////////////////////////
# print kth alphabet
# 'A'+k-1
# 1) Find ASCII of 'A'
# 2) Add k-1 in int
# 3) Find char corresponding to number in 2

k=int(input())
print(chr(ord('A')+k-1))


ord only works with single length strings

///////////////////////////////



Printing ABCD n times:

n=int(input())
i=0
while(i<n):
    j=0
    while j<n:
        print(chr(ord('A')+j),end="")
        j=j+1
    print()
    i=i+1
    
        

Output:
ABCD
ABCD
ABCD
ABCD


////////////////////////////////////////////////////////////////



ABCD
BCDE
CDEF
DEFG

n=int(input())

for i in range(n):
    for j in range(n):
        print(chr(ord('A')+j+i),end="")
    print()


////////////////////////////////////

inverted Numbers:


****
***
**
*

1) Rows: n
2) Cols: n-i (0 based indexing)
3) *


Code:

n=int(input())
for i in range(n):
    for j in range(n-i):
        print('*',end="")
    print()

//////////////////////////////

      *
    ** 
  ***
****
Two different internal while loops to deal with spaces and stars


1) n rows
2) i stars, n-i spaces
3) starts 



///////////////////////////////////////


Isocles Pattern


        1
    1   2  1
1   2   3   2  1




1) n rows
2) 3 different things, n-i spaces; i numbers increasing; i-1 numbers decreasing
3) numbers


Code:

n= int(input())

for i in range(1,n+1):
    for j in range(n-i,0,-1):
        print(" ",end="")
    
    for j in range(i):
        print(j+1,end="")
    
    for j in range(i-1,0,-1):
        print(j,end="")
    print()



Output:
   1
  121
 12321
1234321


/////////////////////////////////////////////////////


Lecture 5: More on Loops

for i in x:
	do something


x is a collection of multiple things
/////////////////////////////////////////////
range func
range(start,stop,stride): start is inclusive but stop is exclusive

We can chose to give only one value.
By default start is picked 0 and stride/step is picked as 1

If two args are given, it is assumed as start and stop. Stride by default is 1
First one is picked as start and second one is picked as stop



/////////////////////////////////////////////////



Print multiples of 3:


//////////////////////////////////////////////

is prime:

///////////////////////////////////////

Pattern:

	1
        2      3     2
     3     4    5  4  3


rows: n
cols:  spaces: n-1, increasing  :  i to 2*i-1
dec: 2i-2, to i, stride=-1


///////////////////////////////////////////////


Break keyword

break:
Lets us break out of the loop, from within the loop if some specific condn is met.

Allows us to optize the code in a better way.



Break in 2 loops :

break will break the closest loop only



for i in range(x)
	for j in range(y)
		# do job
		break


Here only the inner loop breaks
/////////////////////////////////////////////////////////

Else Keyword:

We can use else with loops

while condn:
	do something
else:
	do something else


While condn you remain in the while loop but as soon as condn becomes false you come into the while loop.


////////////////////////////////////////

for i in range(5):
    print(i)
else:
    print("at the end")


Output:
0
1
2
3
4
at the end


////////////////////////////////////////////


Why this needed??

for i in range(5):
    print(i)
print("at the end")

This also performs in the same way.


But this else becomes interesting when we combine it with break

If a break is triggered, else isn't executed
for i in range(5):
    if i==3:
        break
    print(i)
else:
    print("at the end")


Output:
0
1
2


/////////////////////////////////////


Best way to implement Prime function by using for and else:

n=int(input())

for i in range(2,n-1):
    if n%i==0:
        print("Not Prime")
        break
else:
    print("Prime")


//////////////////////////////////////////////////////


continue:
simmilar to break.
break used to completely break the loop
continue skips only the current iteration


///////////////////////////////////////////////////

Pass:

let's you pass a block
dummy statement, can use it if you dont want to add anything in the block


you can skip the block and indentation won't create a problem.

if i<7:

Error
 File "<ipython-input-29-a88d01040444>", line 1
    if i<7:
           ^
SyntaxError: unexpected EOF while parsing

If this is the,  code we will get indentation error

To avoid this we can do:


if i<7:
    pass


Now , no error

////////////////////////////////////////////////////

pass can help us to build the structure of our code initially and we won't have to populate each block there and then


///////////////////////////////////////////

================================================================

Strings:

Combination of chars


Lists:
Collection of elements


Python doesn't have char data type

Creating strings:
a='RaCHINDE'

or

b="Rachinder"

Both single and double quotes do the job


///////////////////////////////////////////////////////

char is a single length string in python


//////////////////////////////////////////////////////


Indexing works in strings


///////////////////////////////////////////


******
Strings are immutable in Python
Item assignment not supported in strings



//////////////////////////////


Concatenation and reassigning an already created string is valid


a="abc"
a="def"   # This is valid


a="abc"
a[0]=d  # This is invalid


////////////////////////////////////////////

==============================================================

String inbuilt Functions


To add single quotes as a part of string , signiofy your strings with double quotes


a=" Rachinder's" 

In this way we can have ' part of string itself

If you want " to be a part of string, we denote string with single quotes


a=' "Rachinder" '
String is "Rachinder"

//////////////////////////////////////
Can use escape chars as well to signify this char is a ' and not start of string

x= 'adad\'s'

///////////////////////////

Functions:
concatenation:   +

a="abcd"
b="fghi"

a+b

This is also ok
a=a+b  // string is immutable, variable is not



/////////////////////////////


Comparison ops: 
a> b : Compares string and b lexicographically

a="abcd"
b="efgh"

a> b:
True


Capital letters come ahead of lower case characters.

So 'z' < 'a' lexicographically


///////////////////////////////////////////

Split: 

Splits a string into list on the char that we provide

a= "a bc def"

a.split(" "): Will split on the white space. We will get output as [a , bc, def]

Default is white space
We can use split on any char


////////////////////////////////////////////



replace(old , new) 
Replaces all the occurences

Ex:  
x="Rachinder is Rachinder is"
x.replace("is","are")

Output:  'Rachinder are Rachinder are'

/////////////////////////////////////////////////////


find() function:

Returns the index of the elemennt in loop:
x.find("is")

Returns index of first occurence of is in string x

-1 if substring is absent
=====================================================


lower(): converts string to lower case
upper(): converts string to upper case
isLower(): checks if string is lower
isUpper(): checks if string is upper case
isalpha(): if all elements of string are alphabet
startsWith(): checks if a string starts with a particular number 

====================================================


Strings slicing:

Getting substrings

a="mhugsudos"

a[si:ei]
chars from si to ei, si is inclusive, ei is exclusive



a="this is slicing"

a[5:10]

Output:
'is sl'
////////////
a[2:]

By default end is the last index of list

a[:4]
By default start is the first index of list
 

/////////////////////////////

3rd arg also possible
string[si:ei:stride]

//////////////////////


Can reverse list with reverse strides


/////////////////////////////

Given a string replace all occurencies of a char with some other char

Use find and concatenate

/////////////////////////////


=============================================================================

Lists:
Collection of elements

1 variable can store multiple elements. 


Creating a list:
a=[1,2,3]
List having name 'a' with 1,2 and 3 as its elements

Can use list() class as well

a=list([1,2,3])  # This will return a list object

////////////////////////////

*****
Lists are mutable


/////////////////////////////////////////

No need to define size of list as well as datatype.
In python list can store any datatype in single list
List can have hetrogeneous data
We can have list in another list

///////////////////////////

By default python starts with some size.
When space is exhausted, python creates another list and copies the data of old list into new list and we get a greater capacity list
Lists are dynamic in nature

///////////////////////////////////////

************
b=[0 for i in range(10)]

Will create list with 10 0s
****************
Instead of 0 we can put i as well

We will get a list like [0 , 1, 2, 3, 4.....]


We can put i^2 as well

We will get list like [0,1,4,9,....]
/////////////////////////////////////

==================================================================

List inbuilt functions:

add:  append , search

/////////////////////////////

Length of list:  len(list_name)

//////////////////////////

Concatenation of 2 lists:

Simply use + operator. Creates a new object. to reflect the change in one othe strings we will have to use assign


/////////////////////////////////////////

* operator: List grows by n times

For ex:  a=[1,2,3]
a*2= [1,2,3,1,2,3]  a multiplied by twice



/////////////////////////////////


Add data to existing list:

append(element): adds to the end of list
insert(index,element): adds element at index
extend: Simmilar to concatenate but changes reflected in a itself. In normal concatenation, a new list is created with a+b.



///////////////////////////////////////


Delete Data:

.remove(element): Removes the particular element from the list. In case of multiple occurencies, removes the first occurence
.pop(index): Removes element from particular index and returns it. We may chose to not give any index and By default it removes at the end
del: Generic Python keyword. Can be used to delete anything.

////////////////////////////


Some other funcs:

list.sort() : sorts the list. Chnages reflected in parent list
sorted(list): sorts the list. Creates a new list object and changes reflected in the new list
max(list): returns max of the list. Make sure to use it only when list is homogenous(containing same data types)
index(element): Gets the index of element in the list. Gives error if element not in the list. Like find() of strings
in keyword: Check if an elemnt in the list. For ex 100 in b. returns a booleans


=============================================

Print the list

a=[2,3,4,5,6,7,7]

for i in a:
    print(i)


Output:
2
3
4
5
6
7
7


///////////////////////////////////////////////



Input in List:

size and all elements

Three ways:

entire input in one one line		size and input array		size and input array in same line 
				in diff lines		
1 2 3 4 5				5				5 1 2 3 4 5 , Can ingest as string, use split method and then convert all elements to int		
				1 2 3 4 5



3rd way of ingesting: 
a=input()
a=a.split(" ")
a=[int(s) for s in a ]


////////////////////////////////////////////////

In one line:


a= [int(s) for s in input().split()]

=========================================

Even odd elements:

Find diff b/w sum of even and odd elemenets

==================================================

List Slicing

Same as strings


a=[1,2,3,4,5,5]

a[:3:-1]:   // Default value in this case for start index index is end of list


a[::-1]: Reverses the list



reverse(): Reverses the list, changes reflexted in base list itself
=========================================================


Multidimensional Lists:

Nothing extra, multidimensional lists are just a list of lists

////////////////////////////////////////////////////
m*n list means m, n sized lists in a single list

a=[[1,2,3],[4,5,6]]

2*3 list


///////////////////////////////////////////////////

Indexing just like in other languages


/////////////////////////////////////////


Taking input

Can get m,n 
Then m n sized lists in seperate lines

Something like:

2 3 
1 2 3
4 5 6


To do this:

sizes=input().split()    # First line will be a string , so spliting the string
m=int(sizes[0]) 
n=int(sizes[1])

mdlist=[]

for i in range(m):
    mdlist.append([int(s) for s in input().split()])   # Just reading the string, converting it to list, making all elements of list as int and appending to the end of list

/////////////////////////////////////

input() function breaks at new line.
So whenever we use a new instance of input() function is used, a new line is read.


////////////////////////////////////////////////

Other way to take input:
Sizes given in a line. Suppose m and n
Then n values given in a new line signifying values for 1 row. This is repeated m times.

Over here we will need to have two for loops.
inner for loop for creating a single row list.


Something like:

2  3
1
2
3
4
5
6



//////////////////////////////////


Suppose get everything in sing;e line:

Something like 
2 3 1 2 3 4 5 6


x=input().split()
m=int(x[0])  // Getting the dimension of list
n=int(x[1])
x=x[2:] // Only the values of the list
x= [int(s) for s in x]  // Converting every value in last to int


mylist=[]
count=0
for i in range(m):
    
    startIndex=n*i   # n is the no of cols
    endIndex=n*(i+1)  # i is the row we are on
    mylist.append(x[startIndex:endIndex])  # Using simple slicing to achieve this



==================================================

Multidimensional Lists:

Slicing : same as normal lists


/////////////////////////////////////////////
Creating 2d list with for loop with default value

mdlist=[[0 for i in range(5)] for  j in range(5)]  # Have assumed 5*5 array

///////////////////////////////////////////////

Problem:
Given a 2d list print sum column wise:
List is m*n, each row has equal columns


def print_column_wise_Sum(a):
    m=len(a)
    n=len(a[0])
    for j in range(n):
        column_Sum=0
        for i in range(m):
            column_Sum+=a[i][j]
        print(column_Sum,end=" ")

print_column_wise_Sum([[1,2,3],[4,5,6]])


Answer:
5 7 9 


Approach : Simply have outer loop take care of cols and have inner loop take care of rows.


/////////////////////////////////////////////////

****************
a = [10,23,56,[78]]
b = list(a)
a[3][0] = 95
a[1] = 34
print(b)

Observe changes are done in a and then b has been asked to be printed

Answer:  [10,23,56,[95]]

Solution Description
They only create a copy on the 0th level. That's called shallow copy. The elements of a nested element are copied by reference.
At 0th level copy is created. After that in deeper levels copies aren't created rather same reference is used
***********************

////////////////////////////////////////////

***********
points = [[1, 2], [3, 1.5], [0.5,7]]
points.sort()
print(points)

Output:
Sort function by default sort the list by first column

So output:   [ [0.5,7], [1, 2], [3, 1.5]]


///////////////////////////////////////////////////


Functions:
Code for nCr

n!/r!*(n-r)!

We will have a seperate loop for n!, (n-r)! and r!. We can do this in one go with functions


def fn_name(arguements):
	body of function


===============================================================

Why do we need functions:

a) Avoid repetition of code
b) Readability
c) easy testing, fn can be tested seperately


========================================================

Few examples:

def isPrime(n):
	for i in range(1,n):
		if n%i==0:
			return(false)

	return(true)

===================================================================


How func calling works:


A(): A calls Fn B()

B(): B calls Fn C()

C():



A() ------->A():  ( print('x')     B()     print('ac'))---------------------> BC(): (print('BS' )   C()   print('BC'))  ----------------->  C() ( print('CS'))

x
BS
CS
BC
ac

Stack is used.  Specifically the call stack
D
C
B
A   

First in Last Out



/////////////////////////////////////////////////////////////////

Functions using Strings and Lists:


def increment_list_first(l):
	l[0]=l[0]+1
a=[1,2,3]
increment_list_first(a)


Changes will be reflexted in the list


a stores the address of list
We go to first element and change it.
Changes will be reflected in a as well as we haven't changed the list itself. Rather we have changed just one element.

Instead if this was done:


def change_list(l):
	l=[2]

l=[1,2,3]
change_list(l)


Here changes wont be reflected  as list itself is changed, that means a new list is created at a new address. The main list still stores the old address


For strings:

Strings are immutable.
So we cannot reflect change in main.string. A new list/reference is created instead

=================================

Return list from func:


def create_rev_Array(n):
l=[]
for i in range(n,0,-1):
l.append(i)

return(l)



==================================================

Swap alternate:

def swap_consecutive(l):
	i=0
	while i+1<len(l):
		l[i],l[i+1]=l[i+1],l[i]   # No need to use temp. Can directly swap in this way
		i=i+2



==========================================================


Scope of variables:

Where can we use a particular variable

Broadly two scopes: local variable, global varaible

local variable: declared inside a func
global variable: declared outside func


In python this will be valid:

def f3():
	print(a)

a=10
f3()


Observe a is defined below the fn, but in python this ok and func f3 will be able to access a. Only thing is 
a should have been defined before the f3 is called.


This wont work though:


def f3():
	print(c)

a=10
f3()
c=13


Observe c is defined after fn call

/////////////////////////////////////////////////////////
**************
You can access any global variable defined before fn value


//////////////////////////////////////////////////////////////

*****************

Lets see what happens here

a4=13
def f4():
	a4=12
	print(a4)

print(a4)
f4()
print(a4)

Over here the outpuit is :

13
12 
13

No change in global value when we update it from a function.
Accessing a global variable is ok from within a fn but when we try to update it, a local variable
is created instead and its reference is stored
********************************************************


/////////////////////////////////////////

===========================================================

To actually change and use the global varaible we need to use global keyword

a4=13

def f4():
    global a4
    a4=12
    print(a4)
    
print(a4)
f4()
print(a4)


Output:

13
12
12


////////////////////////////////////////////////////////

Default arguements:


Functions can have optional arguements

If the optional arg isn't provided, then also we are ok

Default arguement rules are same as Cpp for python as well


def sum(a,b,c=0):
	return (a+b+c)

In this fn we can give 2 or 3 args.
If 2args are given c is assumed as 0


====================================================

def sum(a=0,b,c):
	pass

This is not be valid just like Cpp

======================================================

def sum(a,b=2,c=0)
	pass

sum(2,3)


Here b takes 3 and c takes the default value 0


To specifically give 3 to c, call fn like this

sum(a,c=3)
Here c will take 3 now

======================================================


Tuples:

Simmilar to lists but immutable


In python by default many things behave like a tupple

c,d=1,2    c gets 1 and d gets assigned 2
e=1,2 Python assumes e to be tupple
 
Tuple is the default Data Structure if you want to put multiple things in 
one place

/////////////////////////

Accessing elements in Tuples:
Use indexing
Slicing also works

//////////////////////////

We can use indexing in Tuples, strings, lists as they are ordered in nature.
Wheras dictionaries and sets are not ordered in nature

//////////////////////////////////

Difference b/w tupples and Lists

Tupples are immutable whereas Lists are mutable. Create , Add, Remove in the list, works for Lists and not tupples


/////////////////////////////////////////////

========================================================
Tuple Functions:

1) We can use for loops simmilar to lists
2) We can check membership using in keyword
3) Check length of tuple using len()
4) Cannot update a tuple but can concat them to create a new tuple

c=a+b
Output: (1, 2, 3, 4, 5, 6)

This different than doing:
d=(a,b)

Over here we end up creating nesting

((1, 2, 3, 4), (5, 6))  # a tuple of tuples


5) Can use multiplication to repeat as well
e=4*a 
Output: (1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4)


6) min(), max() also valid
But they will only work if all the enteries within the Tuple are comparable.
Won't work if you have strings and ints within a tuple and they are not comparable

7) List to tuple

# Converting list to a tuple
l=[1,2,3,4,5]
tu=tuple(l)

==========================================================

Varaible Length Input and Output:

Earlier we were creating a sum function which was taking 2 or 3 values up to users choice.
Now we want to implement a fn which can take as many inputs and return as many outputs
We can achieve this by using Tuples

def sum(a,b, *more):



Over here more is optional and is actually a tupple:

We can see this by:

def sum(a,b, *more):
    print(a)
    print(b)
    print(more)

sum(2,3)

Output:
2
3
()


All the optional values will now populate the Tuple.

Lets pass more than 2 args and see

sum(2,3,4,5)

Output:
2
3
(4, 5)


To get sum for all we can do something like:

def sum(a,b, *more):
    
    res=a+b
    for i in more:
        res+=i
    print(res)

sum(2,3,4,5)

Output: 14

///////////////////////////////////////////


Varaiable length output
def sum_diff(a,b):  # returning sum and diff both
    return a+b, a-b # Python automatically creates this Tuple


c=sum_diff(5,1)
print(c)

Output: (6,4)

Python automatically created a Tuple for us
/////////////////////////

Can also take output in 2 different varaibles like this:

d,e=sum_diff(5,1)
print(d,e)

Output: 6 4

# If we are using this approach of unpacking, we need to give same args to unpack as that we are receiving from the function


=======================================================


Introduction to dictionaries:

Associate arrays


In lists we access elements using their indexes.
In Dictionaries we can store the data in such a way that indexes can also get different values.
Indexes are called keys in dictionaries


a={}  // Empty dictionary


a={"the":2,"a":5,100:9} // Having key value pairs while defining


For an index suppose 1000 to be a part of list, we need to have index+1(here 1001) sized list 

In dictionary that is not required.

/////////////////////////////////////
Other ways of Dictionary Creation

1) b=a.copy()  # Just copies the old dictionary to new dictionary

2) Using dict class. Need to pass a list of pairs
x=dict([("the",1),("is",2),("yellow",3)])

Output:
{'the': 1, 'is': 2, 'yellow': 3}


3) using fromKeys method of dict class. Here we can pass simple list and the elements of list will be treated as keys and values will be set to None by default 
d=dict.fromKeyws(["abc",32,2])

Ex:
d=dict.fromkeys([1,2,3,44])
Output:
{1: None, 2: None, 3: None, 44: None}

if you dont want none by default we can pass that as an arguement to the fromKeys fn

d=dict.fromkeys([1,2,3,44],10)

/////////////////////

Dictionaries are mutable


//////////////////////////////////////////////////


============================================================

Accessing Elements of a dictionary:

a={1:2, 3:4,list:[1,23],"dict":(1,2)}


We access data using keys

a[1] gets us 2 as 1 is one of the keys and 2 is its key


.get(key) : fn that gets us the value corresponding to key

///////////////////////////////////
Difference with indexing

[key]: will give us an error if key is not present in dictionary
.get(key): wont give us error even if key is part of the dictionary
Ex:
a.get(5) #  Even though 5 isn't a part of dict we wont get any error
Output: None


In case key is present they behave simmilarly

If you want the default value in case of key absence you can pass another arguement in .get() methor

a.get(5,0)

Now we will get 0 instead of None if key is absent in dictionary


///////////////////////////////////////////////////////////////////////////
Get all the keys:
dict.keys()

Get all the values:
dict.values()


All items as pairs:

dict.items()

We get a list of pairs

//////////////////////////////////////////

How to use for loop:

for i in a:  # This iterates over the keys by default
	print(i,a[i])

To iterate over values:

for i in a.values():
	print(i,a[i])


//////////////////////////////////////


Check if key exists in dict:

use in keyword

"list" in a

Returns True if "list" is part of a dictionary


//////////////////////////////////////////

=============================================


Adding or Removing Data in Dictionary:

Adding
Can do like Cpp

a['tuple']=(1,2,4)

tuple key added having values (1,2,4)


Update fn:

Suppose we have another dict say b. Some keys are common in a and b

b={3:5, 'the':4, 2:100}


a.update(b)

This will update a in such a way that uncommon keys of b are added to a and common keys are updated according
to b. So the common keys will take the value of that in b.


Removing keys:

Use pop()

pop(key): Takes the key to be removed

If key is not given or is absent from dict we get an error


Use del:
Can use del on a particular key like we were doing in lists

use clear:
.clear(): clears all the keys in one go


======================================================

Print all words with frequency k

Use dictionary to store the frequency of all the words


=============================================================


Sets Intro:
unordered_set of Cpp

 Like sets of maths.
Collection of elements like list but the elements are unique in sets
Also there is no ordering in sets



a={"apple","abc",3}

Same as dictionary but no key value pairs

/////////////////////////

in keyword is valid

"apple" in a will return true

/////////////////////////

Can use for loop like:

for i in a:
	print i

Order won't be mantained while we perform this op


//////////////////////////////////////
Length fn:  Use len(a)

/////////////////////////////////


Adding into set:

use .add(element) method

a.add(3)  
Will add 3 to set


///////////////////////////////////

.update(set2) method

Suppose we have

b={"abc", "ghi"}

a.update(b):
Everything in b that was not in a is added to a

Sort of union of 2 sets

//////////////////////////////////////

Deleting from set

.remove(element): Removes element from the set
Will give an error if element is absent in a


.discard(element): Removes eleemnt from set
Will not give an error if element is absent in a


.pop(): Removes elements randomly from the set


clear and del also there. Wont be able to use del on a single element rather only on complete set

///////////////////////////////////////////


================================================

Functions in sets:

a={1,2,3,4}
b={3,4,5,6}

Intersection:
a.intersection(b)
Returns the intersection of the two sets

Union:
a.union(b)
Output: {1, 2, 3, 4, 5, 6}


Difference:
a.difference(b)# Everything in a that is not in b
Output: {1,2}
b.difference(a)#   Everything in b that is not in a 
Output: {5,6}


a.symmetric_difference(b)  # Union -Intersection

Output: {1, 2, 5, 6}
////////////////////////////////////////////////////


The above fns just get us the value but don't update the sets in themselves

We can use other fns to do that

Intersection_update:
a.intersection_update(b)

Changes a to the intersection of a and b


Simmilarly with 
difference_update and symmetric_difference_update

So only 3 fns there which facilitate this update functionality




////////////////////////////////////////////////

.issubset()

a.issubset(b)
Checks if all elements of a are in b


.issuperset()

a.issuperset(b)

Checks if all elements of b are in a

.isdisjoint():
Check if intersection of two sets is null or not

a.isdisjoint(b)

 

///////////////////////////////////////////////////////////////

Sum of unique nos in a list:

Just convert the list to a set , and sum the elements of the set


///////////////////////////////////////////////////////////


To create an emplty set use set class

a=set()
# Gives empty set

a=()
# This will give us empty dict


///////////////////////////////////////


**************

If you want to pop from dictionary while iterating over dictionary:

for i in d.copy():
	d.pop(i)


If you directly pop while iterating over the main dictionary it will be a runtime error as size of dict will keep changing while
iterating over dict


=================================================


Lists cannot be keys in a dictionary.
We can make lists as Tuples and tuples can be keys of a dictionary



=================================================================


OOPS:

Programming paradigm dealing with objects

Objects: Any Real world entity having attributes and behaviour(methods)

Ex: Pen (object)
Attributes: height, Color, Weight, Width
Behaviour: Writing, refilling etc

Class: Blueprint of an object
Doesn't hold data rather defines structure of object


===================================================

Object: Properties-> Attributes
	Behaviour-> Methods

Everything in python is object

Creating a class:
class Student():
    pass



Creating object:
s1=Student()

Also checking the type of s1:
type(s1)
Output:
__main__.Student

/////////////////////////////


Different Attributes:
1) Instance attributes:
object attributes. Bound to a specific object. Value different for each object.

2) Class attributes:
Bound to a class rather than object. Value same for all objects.

////////////////////////////////////////////////



Instance Attributes:

In python there is a difference than Java and Cpp
In Python object can have different instance attributes

For ex:   s1 can have instance attributes: Name, Age
	s2 can have instance attributes : Roll No
	s3 can have instance attributes: Age


We don't need to add instance attributes within the class in python

Creating the instance attribute in Python:

class Student():
    pass

s1=Student()
s2=Student()
s3=Student()

s1.name="Mohit"  # In Cpp and Java, we access the already define instance attribute in this way. But in Python this is the way of creating instance attributes
s1.age=20

In Python for every object we have a dictionary which tracks the instance attributes of an object.
We can access that dictionary by doing: objectName.__dict__ 

Example:
s1.__dict__
Output: {'name': 'Mohit', 'age': 20}


============================================================

Accessing the instance attributes:

We can access attributes using . operator
Example: s1.name

If we try to access instance variable which is not aan instance variable for 
particular object we will get an error


==========================================================


Python inbuilt fns for objects:

hasaatr(s1,'name') : Checks if s1 object has 'name' attribute

setattr: sets attribute for an object  : Fn footprint:  setattr(obj, attr, val)
delattr: deletes an attribute: footprint: delattr(obj,attr)
getattr: gets the value of attribute:  footprint:  getattr(obj, attr,def_value)  ; If attribute is not present, we get the default value


getattr(s2,'name',9)
Returns 9

==============================================================


Class Attributes:

Can create class attributes on the fly, dynmaically just like instance varaibles

Just use class_name.attribute_name

For ex:
Student.totalStudents=10

Can use __dict__ for Classes as well
So 

Student.__dict__ gives us:

mappingproxy({'__module__': '__main__',
              '__dict__': <attribute '__dict__' of 'Student' objects>,
              '__weakref__': <attribute '__weakref__' of 'Student' objects>,
              '__doc__': None,
              'totalStudents': 10})



There are some pre defined attributes there for every class but in the end we can see our class attribute as well

We can add class attributes within class as well.
Unlike Java and Cpp, any attribute that we define within a class becomes class attribute


class Student():
    ## Class attributes
    totalStudents=10
    classTeacherName="Konal"


If we use __dict__ on Student we see:
mappingproxy({'__module__': '__main__',
              'totalStudents': 10,
              'classTeacherName': 'Konal',
              '__dict__': <attribute '__dict__' of 'Student' objects>,
              '__weakref__': <attribute '__weakref__' of 'Student' objects>,
              '__doc__': None})

We can see class attributes being a part of __dict__ for Class Student


Can we access class attributes using objects/instances of our classes:
Yes

s1.totalStudents
Returns us 10


So in a nutshell:
When we do :  object_name.__dict__: We get dictionary containing only instance attributes for particular object
When we do: class_name.__dict__: We get dictionary containing the class attributes




Updating class attributes: Simply use . operatior, to access the attribute use the class itself. 
If we try to update using Objects rather than Class we get a problem like we got with global and local variables



So lets see the differences:

Updation via Class itslef:
Student.totalStudents=30
print(Student.totalStudents)
print(s1.totalStudents)

output:
30
30


Updation via Object 

s1.totalStudents=40
print(Student.totalStudents)
print(s1.totalStudents)

output:
30
40


Also:
Now checking instance varaible dictionary:
print(s1.__dict__)

Output:
{'totalStudents': 40}

We can see that we can access Class members with Objects just like we can access global variables from within a fn.
But if try to update it using object, an instance member gets created instead and no change is reflected within the Class attribute itself.

======================================================


Methods:

1) Instance Methods: Object methods, Just like instance attributes. Methods bound to the object.
Ex updateName()   (as name is an instance attribute and may not be present in other object of same class)

2) Class Methods: Class methods, just like class attributes. Method bound to class.


3) Static Methods: Will discuss later



Any method that we add within class body becomes Instance method. 


*********** What happens in Python when we call instance methods:

When we call instance method: 
Say s1.printHello() ,    this is interpretted as Student.printHello(s1) by python
Object gets passed on its own. More precisely the reference of the object because of the way data is stored in python. For ex a=10. A memory location stores 10 and its reference is stored in a. Simmilarly over here as well.

 So whenever we have instance methods we will have to receive this object. Normally we receive it like this:


class Student():
    ## Class attributes
    totalStudents=10
    classTeacherName="Konal"
    
    def printHello(self):
        print("Hello")

Observe that we have a self arguement in Instance method printHello. This self will contain the reference of object. 
self can serve the same functionality as this in Cpp


# Can we access instance methods using Class name:
Student.printHello()
We get an error:
printHello() missing 1 required positional argument: 'self'

self is the reference of the object that is passed. If we call the fn using Class that refrence will be absent, hence this error

If we want to call this using class, we will pass an object .

For ex:
Student.printHello(s1)

Now s1 gets passed



///////////////////////////


Suppose we have an object s1 having instance variable name

We want to print name from fn

We can have fn like:

class Student():

def printName(self):
        print(name)


This will not work. This is because within class, an attribute is treated as class attribute. name will
be treated as class variable or local varaible of fn itself. But it is an instance varaible. We can use self to signify that this is an instance attribute




class Student():

def printName(self):
        print(self.name)


Now this will work


================================================

Instance Methods:

Creating new instance attributes with methods:


Instance method:
def addName(self,name):
        self.name=name

Calling this method with the object:
s1.addName("Rachinder")


Checking the dictionary:
s1.__dict__
Output:
{'name': 'Rachinder'}

We can see we added instance attribute with the help of a method

We can change this attribute using this method as well by calling this fn again
//////////////////////////////////////////////////////////////////


If we want to print a class attribute from within a fn:

def test(self):
	print(totalStudents)


Firstly we will have to use an object to call this method as this is an instance method.
Also totalStudents will be treated as a local varaible rather than class attribute.
To treat it as a class attribute we should use className.attributeName


def test(self):
	print(Student.totalStudents)


Now this will be treated as Class attribute.
We can update and create class attributes from within the class as well.


//////////////////////////////////////////////////////////////////////////////////


Constructors:
First method that is called after object is initialized.
Used for initializing the properties of an object

s1=Student()

As soon as this statement is executed, constructor gets called

A default constructor is present in all the class but we can define our own constructors as well.


def __init__(self):
        pass

Default constructor that is present in the class
As soon as we create our constructor, the default constructor gets deleted.


Constructors are a very good way to initialize our objects. This is because it is the first fn that is called when object is created
We can initialize our objects from within constructors and we wont have to worry about order of fn calling(sometimes we may call a fn which use some instance attribute which is yet not created).

So the best way to initialize instance varaibles is like this:


class Student():
    ## Class attributes
    totalStudents=10
    classTeacherName="Konal"
    
    def __init__(self,name,age,rollNo):
        self.name=name
        self.age=age
        self.rollNo=rollNo
        
    
    
    #Instance methods
    def printHello(self):
        print("Hello")
        
    def printHello2(self, string):
        print(string)
        
    def printName(self):
        print(self.name)
        
    def addName(self,name):
        self.name=name


Object creation:
s1= Student("Rachinder",20,1)

Checking out the s1 dictionary:
s1.__dict__

Output:
{'name': 'Rachinder', 'age': 20, 'rollNo': 1}
//////////////////////////////////////////////////////

Can have default args in constructor
Function overloading is not allowed in Python so we wont be able to overload our constructor as well

Instead we can actually pass multiple arguements to any method using this:

def fun(a,b, *args):


As we have already seen this args will be a tuple and adjust itself accoring to the no of arguements we pass.
With this we can pass variable arguements and avoid making multiple constructors.

////////////////////////////////////////////////////////


Access Modifiers:
1) private
2) public
3) protected

private: can access within the class only
public: accessible anywhere
protected: inside the class or its immediate child


By default everything is public in python.
In order to make a property private we have to use 2 leading __

To make anything protected we need to use 1 leading _

Making name attribute as private:



class Student():
    ## Class attributes
    totalStudents=10
    classTeacherName="Konal"
    
    def __init__(self,name,age,rollNo):
        self.__name=name    # name attribute is private
        self.age=age
        self.rollNo=rollNo
        
    
    
    #Instance methods
    def printHello(self):
        print("Hello")
        
    def printHello2(self, string):
        print(string)
        
    def printName(self):
        print(self.__name)
        
    def addName(self,name):
        self.__name=name


Observe that we have to use __ everytime we use name attribute which is private

So __name is used as attribute instead of just name


 We wont be able to use name outside the class:

s1.name will give us error.
We can use __dict__ to see what are the attributes for the object.
name would have been changed to _Student__name. We can actually use this to access private attribute.
Although this is not ideal but this option exists.


====================================================


Benifits of private access modifier:

1) abstraction: If we have an attribute or property which is irrelevant for end user, we should make that property private
2) security: If we have a very critical piece of data and we don't want it to be accessible to any outside authority so we can make it private.


/////////////////////////////////////////////

How to use private properties of class:

Getters and Setters



def getName(self):
        return(self.__name)

#Method which allows us to get access f private property out of the class
# See we have to __ before name.

What is the benifit of Private Modifier if we are able to access via Getters and Setters??
Can have validations inside class . We already discussed it in Cpp. Same reasons persist here as well.

==============================

***************

In reality nothing is private in Python. 
Python follows name mangling.
If a property is set as private it name gets changed to _ClassName__AttributeName as we have already seen in
the object.__dict__.

We can accesss in this way as well as done in the Notebook as well.


//////////////////////////////////////////////////////////


======================================================


Class Methods and Static Methods:

If we want to make class Attribute as private.
Suppose total students.
We won't be able to access outside the class easily.

 
We want to implement pulic  getter to achieve this 


def getTotalStudents(self):
        return Student.__totalStudents

To call this fn:
Student.getTotalStudents(s1)   # We will have to pass s1 as self is required


But this is not very ideal way of calling.
We should not require s1 to get total students

So we need to create o=it as class method
We will have to use decorators.(seen in Flask )


Just add @classmethod decorator to the fn you want to make class method

Now the method looks like:

@classmethod
def getTotalStudents(cls):
     return Student.__totalStudents

We can call it like this now:
Student.getTotalStudents()
Now we dont pass self rather we pass cls signifying class

We can also  use Class methods to create Factory methods.
eXPLORE THEM AND also explore static methods for python.



=======================================s===========================

Working with Files:

Focus will be on :

.txt files
.csv files


Open and Read text file

We dont need any external library
open fn can be used:



open(fileName, access_Mode)

filename takes the file path. Can be relative or complete. This is compulsory
access_Mode :  Options:  'r' (read)  ->default,   'w' (write) (if we use this all the data present in the file gets deleted)
'a' (append) we can retain our content  and new info added at the end.

We will focus on read mode as of now


This fn returns us  a file object


Example:

file_obj=open('notes.txt')

If this file is absent we get an error

//////////////////// 

To read content from this file:

We can use read method of Python


x=file_obj.read()   # To read the complete file as string



*************

Sometimes we might have to specify the encoding of the input file other wise we may get some error

file_obj=open('notes.txt','r', encoding="utf-8")

Observe in the open function we added the encoding as well



***********************

///////////////////////////////////////////


Sometimes we may not want to read entire data of the file rather some bytes.
read(n) : read function accepts an arguement where we can specify how many bytes are to be read.

 
x=file_obj.read(100)

Output: 'Symbol table: At compile time, the table which formed which maps the memory locations with variable'
Only first 100 bytes read 

/////////////////////////////////////


readLine(): With this func we can read one line at a time

readLine stores the state signifying till what point the content has been reteived and with next fn call readLine() will
retreive further data.

///////////////////////////////////////

readline(n): Also accepts arg signifying how many bytes are to be read


Overhere how n is perceived:  
min(n,current_line):  If n exceeds the size of current line , we will still print current line only.
////////////////////////////////////


Read Line by line:


Requirement is we want to read the entire content but line by line

1) use readLine() fn for as many lines present in our file
2) We have another fn called readLines() which returns us a list of strimgs, where each entry is one line from the file

Using readLines:

myLines= file_obj.readlines()

And mylines becomes a reference to a list containing all the lines of the file



type(myLines)
Output: list

We can operate on list after taht easily

///////////////////////////////////////////

When we are done with working with our file, it is our responsibility to close this conenction as well.

To do this:  file_obj.close()

File gets closed after this and all the content in the buffer gets erased.

After closing we wont be able to use any fn like read or read lines as this link is now closed
//////////////////////////////////////////////


We might forget to add the close op. 
To counter this we can use this syntax
*****************
with open(Filepatyh, access mode) as file_obj:
	file_data=file_obj.readLines()
***************

Now our code will be enclosed within this block and connection closed automatically once we are done.

Also we can still access the file_data list outside the block as well
/////////////////////////////////////////////////


CSV files:

Common Seperated File.

Have downloaded the sample dataset from Coding Ninjas

Syntax:

with open('year2017.csv') as File_obj:
    file_data=File_obj.readlines()


We get a list of strings.
Each string is one row in the csv

First 5 rows something like this:
['Year,Month,Day,Country,Region,city,latitude,longitude,AttackType,Killed,Wounded,Target,Group,Target_type,Weapon_type,casualities\n',
 '2017,1,2,Afghanistan,South Asia,Takhta Pul,31.320556,65.961111,Hostage Taking (Kidnapping),0.0,0.0,Construction Workers,Taliban,Business,Firearms,0.0\n',
 '2017,1,3,Sudan,Sub-Saharan Africa,Fantaga,12.921007000000001,24.318324,Armed Assault,2.0,0.0,"Civilians: Haroun Yousif, Hamid Ibrahim",Unknown,Private Citizens & Property,Firearms,2.0\n',
 '2017,1,1,Democratic Republic of the Congo,Sub-Saharan Africa,Saboko,1.452372,29.875162,Armed Assault,7.0,0.0,Village,Allied Democratic Forces (ADF),Private Citizens & Property,Melee,7.0\n',
 '2017,1,1,Democratic Republic of the Congo,Sub-Saharan Africa,Bialee,1.4523700000000002,29.875186,Armed Assault,7.0,0.0,Village,Allied Democratic Forces (ADF),Private Citizens & Property,Melee,7.0\n']

First string contains the headers


With this way we wont be able to seperate column values by default

We have another module in python called csv which we can use to achieve the same.


We can use reader fn in csv module:

import csv
with open('year2017.csv') as File_obj:
    file_data=csv.reader(File_obj)



We wont be able to directly see the content of the object that is created.
But we can iterate over it

Code:

with open('year2017.csv') as File_obj:
    file_data=csv.reader(File_obj)
    
    for row in file_data:
        print(row)

Output:


['Year', 'Month', 'Day', 'Country', 'Region', 'city', 'latitude', 'longitude', 'AttackType', 'Killed', 'Wounded', 'Target', 'Group', 'Target_type', 'Weapon_type', 'casualities']
['2017', '1', '2', 'Afghanistan', 'South Asia', 'Takhta Pul', '31.320556', '65.961111', 'Hostage Taking (Kidnapping)', '0.0', '0.0', 'Construction Workers', 'Taliban', 'Business', 'Firearms', '0.0']
['2017', '1', '3', 'Sudan', 'Sub-Saharan Africa', 'Fantaga', '12.921007000000001', '24.318324', 'Armed Assault', '2.0', '0.0', 'Civilians: Haroun Yousif, Hamid Ibrahim', 'Unknown', 'Private Citizens & Property', 'Firearms', '2.0']



/////////////////////////////////////////////////

=================================================================

csv.reader() uses , as a seperator by default
We might have a case where our csv dataset is of the form

data1 | data2| data3|.............

Here the pipe symbol  (   |  )  is used as a seperator.
This will be a problem for csv.reader() fn.

We can passs another arguement in the csv.reader() fn 
So the fn call now looks:

file_data= csv.reader(File_obj, delimiter='  | ')




Now | will be used a delimeter/ seperator instead.

We should also perform strip fn of string manipulation on all the cells as space might create a problem.

'   1'  ,  we might face a difficulty converting this to int.
We can use another arg     skipinitialspace

Default value is False. If we configure it to True, the initial spaces are skipped.   

==========================================================


Also for now we are getting an itretator where each individual element is a list.
We can convert the iteartor to a list and we will have one big 2d list.
We were not able to perform iteration over the iterator outside the with block.
But we can use this 2d list easily outside the with block as well

Code:
with open('year2017.csv') as File_obj:
    file_data=list(csv.reader(File_obj, delimiter=',', skipinitialspace=True))
        

    
    for row in file_data:
        print(row)

==============================================================================

Total People killed

with open("year2017.csv") as FileObj:
    file_data=list(csv.reader(FileObj))


Find out total people killed in 2017??

with open("year2017.csv") as FileObj:
    file_data=list(csv.reader(FileObj,skipinitialspace=True)) # skip initial space makes sure that we remove additional space
killed=[]
for row in file_data[1:]:
    val=row[9]
    if val=='':    # Skipping the missing values
        continue
    killed.append(float(row[9]))

killed= [int(k) for k in killed]  # Converting to int
total=sum(killed)
print(total)

========================================================


Dict Reader:
Instead of reading our data as a list we can read it as a dictionary as well.

Function:  csv.DictReader()
Keys: Col.head ( the first row)
Value


import csv

with open("year2017.csv") as FileObj:
    fileData=list(csv.DictReader(FileObj))

Lets see the first row:
fileData[0]

Output:
{'Year': '2017',
 'Month': '1',
 'Day': '2',
 'Country': 'Afghanistan',
 'Region': 'South Asia',
 'city': 'Takhta Pul',
 'latitude': '31.320556',
 'longitude': '65.961111',
 'AttackType': 'Hostage Taking (Kidnapping)',
 'Killed': '0.0',
 'Wounded': '0.0',
 'Target': 'Construction Workers',
 'Group': 'Taliban',
 'Target_type': 'Business',
 'Weapon_type': 'Firearms',
 'casualities': '0.0'}


By defaults keys are picked from 0 th row
We can change it as well

import csv

with open("year2017.csv") as FileObj:
    fileData=list(csv.DictReader(FileObj, fieldname=('a','b','c')))




a , b, c will be taken as field now. 

Now , we wont have to fetch indexes for different fields. Can use key value pairs



=============================================================================

Country wise killed:


Create a dictionary where key is Country name and Value is total killed

Done in the notebook


==================================================

import csv
with open("year2017.csv") as FileObj:
    fileData= list(csv.DictReader(FileObj))
countryCasuality={}
for row in fileData:
    country=row["Country"]
    casuality=row["casualities"]
    if casuality=='':
        continue
    if country in countryCasuality:
        countryCasuality[country]+=int(float(casuality))
    else:
        countryCasuality[country]=int(float(casuality))
        
for country in countryCasuality:
    print(country,countryCasuality[country])



===================================================================

numpy:

Python's linear algebra library
Numpy stands for numerical Python

Numpy gives us n d arrays.  n can belong to set 1 to infinity

Why needed if we already have list??

Diff with Lists

Memory requirememnt:  Numpy array takes less memory compared to Lists
Time execution: Numpy work faster than normal lists
Convenience to use: Easier manipulation and ops as compared to normal list
Functionality: Lot of function already implemented in optimized way

Prefer  numpy if we have to perform ops on large data



Some functions:

time.time(): fn which tells us how much time has passed till current epoch.

t1=time.time()
some code
t2=time.time()


t2-t1 can tell us how much time was required to execute the code.


====================================


To get size of one element in list:

sys.getsizeof(element)



===========================================


Why is Numpy fast??

We have two arrays.
We want to create a new array by performing element by element addition.

We might have to run a loop, to loop over the two arrays and do c[i]=a[i]+b[i]

In numpy we have something called vectorization.
We dont have to operate on each element one by one.
Rather the addition can happen in fairly larger chunks compared to a single element
 With vectorization, we can perform mathematical ops on chunks of data rather than doing the ops element by element


We can simply do :     c=a+b

We don't need to take care of vectorization, it is handled natively by numpy itself.
This is because these functions are written in a very optimized manner in C or Fortran

With numpy we can have both: speed and easy of op
==========================================================================

Create numpy array1

First we have to import this library

import numpy as np

np is the alias for numpy. Anytime we want to use any fn within numpy we can use this alias to easily call numpy library


np.array(array_like_object, ) : Fn to create numpy array
array_like_object: anything resembling an array like a list or range



Ex:
a=[1,2,3,4]
b=np.array(a)

b is numpy array



**************

numpy array has a restriction. It can only contain homogeneous data.
For ex:
a=[1,2,3,4,'a',4.5]
The data in a is hetrogeneous
b=np.array(a)
print(b)

array(['1', '2', '3', '4', 'a', '4.5'], dtype='<U32')

If we observe, we'll find that all elements are being treated as strings
By default all elements are treated as strings.
We can change this by passing another arguement called d-type which will determine the datatype within our numpy array

b=np.array(a, dtype=int)

Initially we will get an error, as our array has a string 'a' which cannot be converted to int. But if remove 'a' , we will get int numpy array

We can change dtype to the desired data type



Other fns to create numpy arrays:

np.ones(no of elements): Creates an array of 1s. The no of 1s will be equal to no of element passed into the function
np.zeros(no of elements): Creates an array of 0s. By defaul datatype is float. We can change the data type according to our wish using dtype arguement
np.full(): 
np.empty()


We can create 2d arrays as well with np.ones and np.zeroes


f=np.ones((2,3))
print(f)
array([[1., 1., 1.],
       [1., 1., 1.]])



np.full(dimension_of_array, fill_value): Returns us the np.array with the mentioned fill value

Ex:
g=np.full((4,5),10)
print(g)

array([[10, 10, 10, 10, 10],
       [10, 10, 10, 10, 10],
       [10, 10, 10, 10, 10],
       [10, 10, 10, 10, 10]])

Here d-type by default is 0


np.empty():  Return a new array of given shape and type, without initializing entries.
We get random integers instead.


/////////////////////////////////////////////////////


=================================================================

np.arange()
np.linspace()

Both methods nearly same in functionality.
Using these methods we can get evenly spaced values within a given interval

np.arange(start,stop,step): start is optional. If not given by default start is assumed as 0. Stop is mandatory.
Step is option. By default the value taken is 1. Start is inclusive but stop is exclusive.


Example:
b=np.arange(10)   # start is assumed as 0 and step is assumed as 1
print(b)
Output:
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

Example 2: 
c=np.arange(2,10,2)
print(c)

Output:
array([2, 4, 6, 8])

=============================================

linspace(start,stop, num)

start and stop same as arange.
num here stands for number of smaples to generate.
By default the value is 50
Step value calculated on its own by the function

Here start and stop both are compulsory
Also start and stop are inclusive in this function

We have the an arguements called endpoint which is boolean in nature. It is True by default.
This is used to configure whether you want the stop value to be included in the distribution or not.

Ex:
e=np.linspace(2,10,num=5,endpoint=False)
print(e)

Output:
array([2. , 3.6, 5.2, 6.8, 8.4])
We can observe now that 10 (the stop) is not part of the distribution



Diff b/w arange and linspace

With arange we have control over end step size whereas with linspace we have control over no of samples


===============================================================================

Few other functions:


np.eye(N,M=None,K=0): Return a 2-D array with ones on the diagonal and zeros elsewhere. 
N: no of rows in output
M: no of columns in output. Optional
K: index of diagonal. 0(default) refers to main diagonal

Ex:
e=np.eye(3,3)
Output:
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])

Ex2: 
e=np.eye(3,3,-1)
Output:
array([[0., 0., 0.],
       [1., 0., 0.],
       [0., 1., 0.]])

np.identity(n):  
The identity array is a square array with ones on
the main diagonal.  Here both rows and columns are same.

We can get only square matrices with identity but we can get rectangular array with eye

=============================

np.random()
Random no generator

rand within random

np.random.rand(): 

Create an array of the given shape and populate it with
    random samples from a uniform distribution
    over ``[0, 1)``.

Parameter: 
d0, d1, ..., dn : int, optional
       The dimensions of the returned array, must be non-negative.
        If no argument is given a single Python float is returned.



Ex:  
f=np.random.rand()
print(f)

Output:
0.7891554718584611

Ex2:
f=np.random.rand(5)
Output:
array([0.77644223, 0.44255455, 0.10089424, 0.84709744, 0.3004456 ])

Ex3: 
f=np.random.rand(2,2)
Output:
array([[0.44245637, 0.79592433],
       [0.85645423, 0.41739804]])



Ex: We know we will get distribution from 0 to 1, 1 exclusive.
To expand the distribution say 0 to 10 , do something like:

f=np.random.rand(10)*10   # Multiplies every number in distribution by 10

Output:
array([1.68016031, 9.41037998, 6.37856344, 0.61573014, 1.83098738,
       7.32999957, 8.18522092, 8.0898521 , 7.62413182, 2.13329789])



Another variation:
np.random.randint(low,high=None,)

return random integers from `low` (inclusive) to `high` (exclusive).
Parameters:
low: Required
high: None, by default= low+1
Size= dimension of the distribution desired.


Ex:

g=np.random.randint(2, size=(10,10))

Output:
array([[0, 1, 0, 0, 0, 1, 1, 1, 0, 1],
       [1, 0, 1, 1, 0, 1, 0, 0, 0, 1],
       [0, 1, 1, 1, 0, 0, 1, 0, 0, 0],
       [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
       [1, 1, 1, 1, 0, 1, 0, 1, 1, 0],
       [0, 0, 1, 1, 1, 1, 1, 0, 0, 1],
       [0, 0, 1, 1, 1, 1, 1, 0, 0, 1],
       [0, 0, 1, 1, 0, 1, 1, 1, 0, 0],
       [0, 1, 0, 0, 1, 1, 1, 0, 0, 1],
       [1, 1, 0, 1, 0, 1, 0, 1, 1, 1]])

=======================================================================


Slicing and Indexing

numpy array is a collection of references or pointers which point to four different attributes

data: reference to first byte of array
shape: reference to shape of array
dtype: reference to the data type of the array
strides:no of bytes to be skipped to get to next element

We can reference these attributes for an array with the help of that array object

Ex:

a=np.random.rand(3,5)
print(a.data)
print(a.shape)
print(a.dtype)
print(a.strides)

Output:
<memory at 0x000002629C842110>
(3, 5)
float64
(40, 8)  # 8 bytes to be skipped to get to next element, 40 bytes to be skipped to get to next row

========================

Accessing element from array

Can use indexing and slicing easily

in 1d array both work the same.
A little different than 2d array

Indexing is same in both. We also have an additional option in numpy array. Suppose you are accessing 2,3 from arr1
You can do it like:  arr1[2][3]  or arr1[2,3]

What about slicing??

In lists only option of indexing is [][]  whereas we can do [,] as well in addition to [][] in case of 2d arrays of numpy

Ex
a=
1 2 3
4 2 6
7 8 9

Say we want to print the last column:  3,6,9

If we use slicing on [][]:

a[0:3][2]:  Here first a[0:3] is executed

We will get    [[1,2,3],[4,5,6],[7,8,9]]
After that a[0:3][2] is executed and we end up getting:   [7,8,9] 
In this way slicing won't work in [][]. So list wont be able to help us at all and if we do this for numpy we will fill there as well


But with Numpy we have the option of [,] as well for indexing and slicing

Over here if we write:  a[0:3,2]  , then a[0:3] wont be executed first.
Rather the complete statement will be executed together and both x and y will have same weightage.
That's why we can achieve this with this kind of indexing in numpy.

For list only option is 2d array

===============================================================


Mathematical ops:


import numpy as np
li=[1,2,3,4,5]
a= np.random.randint(1,20,5)
b=np.random.randint(1,20,5)

print(a)
print(b)


Op1:
We want to add 1 to every eleemnt of li and a


For li only option is to use a  loop

# For li
for i in range(len(li)):
    li[i]=li[i]+1

For a we can simply do, a=a+1

# For a i.e. a numpy array
a=a+1

The reason is vectorization. The numpy array is considered as a vector.
When we add a and 1 , its like adding two vectors with 1 being converted to a size same as a

=====================================================

Element by element addition:
We can simply do  c=a+b, where c is a new numpy array


Element by Element subtraction:
Same as addition


Multiplication:
Same as above. Multiplication will be element by element.


Division:
Same as above.

Floor Division:
Same as above

Exponentiation: Same as above

if  a=[a,b,c,d],  b=[i,j,k,l]
a**b= [a**i,   b**j, c**k, d**l]


==================================

sum():  a.sum()

.max():
.min():
.mean(): 

.argmin(): Index for the minimum element
.argmax(): Index for maximum element


Relational and logical operators are valid

a>b
Returns an array giving us the element by element comparison

a==b:
Element by element by comparison



Logical:

Functions:


np.logical_and(a,b):  logical and b/w a and b
np.logical_or(a,b): logical or b/w a and b
np.logical_not(a): logical not of a


All the above fns return element by element comparisons and manipulations



===================================================================================


Boolean Indexing:

Technique which we can apply on our data to extract the desired data

import numpy as np

a=np.array(['t','b','r','d','e','f','z','a'])
b=np.random.randint(1,20,8)

print(a)
print(b)



print(b>10)

Output:  [ True  True False  True False False False False]


This tells if element is greater than 10.

Suppose we want to extract all elements greater than 10 in the above array

bool_arr=b>10
new_arr=b[bool_arr]
print(new_arr)

Output:

[16 17 17]



How it works:

Mask is applied on b and we get only those values which have a True value according to this mask



Could be done in one go as well:

new_arr=b[b>10]


======================================

Multiple condns:

new_arr=b[(b>10) & (b<18) ]

new_arr

 

=======================================


We can even assign values using slicing


c[:3]=1

Will assign the first 3 values of c as 1.
This can't be done in normal lists


Another example:


c[c>15]=100
For all the values that are greater than 15, make their value as 100



===============================================


np.where():

We can use this to get the indexes where our condn is satisfied

For ex: Getting max value  index


x=np.where(b==max(b))



Ex2:

indices for values whose value is equal to 100

ind=np.where(b==100)

We can even further do

a[ind]=5
Will replace every 100 with 5
That is further adds a mask


================================================

For creating a mask , if we have to use and , or condns use the logical_and, logical_or fns


=====================================================


Boolean idexing works because say we are working with numpy array a.
If we do a>30 , this actually means we intend to do this comparison for all the eleemnts of the array

==================================================




Boolean masking in 2d arrays:

import numpy as np
a=np.random.randint(1,30,(5,6))
a




get values which are greater than 20
mask=a>20
print(a[mask])

2d arrays just behave like 1d arrays



Updating array using mask:
mask=a>20
a[mask]=100

We can do accessing and updating in 2d arrays just like we did in 1d arrays

We should take care about dimensions.

If a is of size x*y then mask should also be of size x*y, if this is not the case we get an error





What if we want to work on specific column of 2d array??

For column 3, any value which is equal to 100 should be changed to 99


mask1=a[:,3]==100

Also to perform updation using boolean indexing we now need to pasas the column no as well on which this mask should have its  effect

a[mask1,3]=99
print(a)

Observe that we have passed the col no as well 


Suppose we had mask as F,F,T,T,F

Why do we do:
a[mask1,3]

The boolean array gets the rows for us. Observe the 2nd and 3rd bools are True only, so 2nd and 3rd row is selected by default

The second paramter of of a[mask1,3] i.e 3 tells us which columns will be impacted. Over here only 3rd column is impacted.



===========================================================


 
numpy broadcasting:

So far we have performed ops on arrays where sizes are equal.
Ex: a->5,1    and b->5,1

What if sizes of the two arrays are not equal.
Broadcasting comes into this

if x->2,3
y->3,2

if we do x-y, we will definetly get an error


Dimension compatibility:

		Array 1			Array2

equal     		both are compatible

one of them	x->3,3			Y->3,   OR y-> (1,3)  this is a 1d array
is 1					Then also this is compatible as even if one of the dimension are queal and other dimension for other is 1 , both arrays are compatible



So rules:  If both arrays have same dimensions.
If both arrays have 1 common dimension and other dimension for one of the arrays is 1.This is done via brodcasting



Suppose we had 2 arrays:     
x:
7 7 5
9 4 2
5 7 5

y: 
1 6 7


What happens when we try to do:  x+y is:

y gets broadcasted and converted to:

1 6 7
1 6 7
1 6 7

to match the dimension sof x.
This broacsting can happen in any dirn if required

This is handled natively, internally by numpy

How to make arrays compatible:

===================================================

To make the two matrices compatible we can:

Use transpose, if that works
Reshape the array, if that works   (doesn't change the data)
Resize the array, if that works   (change the data according to the new and old size)


========================================

Some example:

array 1:   3,1 

Array 2:  3,3

Array 1 broadcasted to 3,3 


Ex2:

Array1:  1,3

Array2:  3,1

both arrays broadcasted to 3,3
Special case


Ex 3:

Array 1: 1,3
Array2:  1,1

Array 2 broadcasted to 1,3



Array with lesser shape is broadcasted to size of larger array



==================================================================
Example:

We will use year2017.csv dataset.
Add new column wounded + killed


import csv
import numpy as np



killed=[]
wounded=[]
country=[]


with open("year2017.csv") as FileObj:
    data=(csv.DictReader(FileObj, skipinitialspace=True))
    for row in data:
        killed.append(row['Killed'])
        country.append(row['Country'])
        wounded.append(row['Wounded'])


# Creating numpy arrays
np_killed=np.array(killed)   
np_wounded=np.array(wounded)
np_country=np.array(country)

# removing the missing values
np_wounded[np_wounded== '']='0.0'
np_killed[np_killed== '']='0.0'

# Converting to float and then int

np_killed=np.array(np_killed,dtype=float)
np_killed=np.array(np_killed, dtype=int)

np_wounded=np.array(np_wounded,dtype=float)
np_wounded=np.array(np_wounded, dtype=int)

# Create column killed+wounded
killed_wounded=np_wounded+np_killed


# Only those countries where country is India

indices=np.where(np_country=='India')
killed_wounded[indices]


==============================================================



Pandas:

Library like Numpy

Great way to manipulate data

Pandas behaves like an Excel file

Importing Pandas:

import pandas as pd


Loading dataset:

Many fn: 
pd.read_csv() : To read csv

Many fns present with us


Can get data from internet as well by using urls
Path/url will be arguemnt of reda csv fn

 
dataframe: 2D table with rows and columns

===================================================


Creatingf a copy of dataframe:


Suppose we have dataframe iris and we want to make copy.


df=iris

Both variables will have reference to same variable hence this won't do us any good.

/////////////////////////
Copy():

df=iris.copy()   # Creates a copy of iris and stores in df

/////////////////////////////
head(n) fn:  Returns n rows. By default n is 5.

/////////////////////////////////

 shape():  df.shape()  Returns the shape of data frame

//////////////////////////////////////////
***************************
Just like we use help fn we can also use '?' 
Simply type the property you are interested in , add a ? after that and execute the cell


***********************************
/////////////////////////////////////////////////////


df.dtypes: Return the datatypes that are present in dataframe

//////////////////////////////////////////////


Change col headers:

Current headers:
df.columns


Adding new columns:

df.columns=['col1','col2','col3','col4'  ........]

Populate the list accordingly


==========================================


Describe() fn: 
very useful
describes the data for us
df.describe()

Paramters we get:

count: valid values(excludes NAN) for each col
mean:
std dev
min:
max:
spread:  25,50 and 75 percentiles  ,  if we arrange the values in increasing order, 25percentile will mean 25% values are less than this value. Simmilarly for 50 and 75. 50th percentile is called median
max: 


/////////////////////////////////////////////



To access a particular column:  df.column_name
For ex:
df.sepal_length

We can also access it like:

df["sepal_length"]



////////////////////////////////////////


Look at the null entries:

df.isnull()

Returns a boolean series signifying which entry is Null

We can do df.isnull().sum() to try to make some sense of this:

df.isnull().sum(): Does sum column wise

sepal_length    0
sepal_width     0
petal_length    0
petal_width     0
species         0
dtype: int64


//////////////////////////////////////////////
Slicing:

To access some data in b/w:

.iloc[]  : Helps us accessing the data.
We can use indexing and slicing with this

Ex:
df.iloc[2:17]

Ex:
df.iloc[2:17,1:3]

Indexing and slicing same as numpy array


//////////////////////////////////////////////////////////

======================================================

Data Manipulation1

We have column headers and labels for rows:

Trying to remove some data:
Trying to remove some rows:  We can remove cols as well, just change axis to 1

drop() fn:

df.drop(row no): drops the mentioned row. Doesn't do inplace, rather makes a copy
To make it inplace, we have a paramter called inplace which can be toggled to True

df.drop(0,inplace=True)
Drops the 0th label inplace
0 doesn't means 0th row rather means 0th label

If you do inplace and dropped 0th row. If you try to drop 0th row again you will get an error as it is dropped now from df itself

Removing column:
a=df.drop("petal_length", axis=1)
 
///////////////////////////////////

As we accessed cols earlier , we can access labels(rows) as well.
We can use  df.index
/////////////////////////////

We can drop by position as well by using df.index.
df.index[0] always mean the first row in the data frame

# dropping by index
df.drop(df.index[0])

////////////////////////////////////////

Remove multiple rows in one go:
Pass an array


df.drop([0,1],inplace=True)
This will drop 1st and 2nd row in one go

//////////////////////////////////////


We can use boolean masking here as well

df["sepal_length"]>3

Will return you a boolean series



To get data regarding one flower type:
Use Boolean masking

df[df["species"]=="Iris-setosa"]



***********************
We can then start describe() on individual class itself

With this we can get great insights.
We can know what is the mean of particular attribute for this flower.
The spread etc and other things as well.

/////////////////////////////////////


How to add a row??
iloc works by posn

df.iloc[0]: 0th posn row

We have another fn:
.loc[]  : This is label based


To add a row we can straight away use loc
df.loc['Rachinder']=[1,2,3,4,"Iris-setosa"]


With this we will have a label having Rachinder as label

////////////////////////////////////////////////////////////////////////

We should not use and, or, not etc for chaining condns in python.
We can use bitwise operators

Ex:

Find the data of flower “Iris-virginica” type where petal-length > 1.5?


df[(df["variety"]=="Virginica") & (df["petal.length"]>1.5)].count().variety


Observe & in chaining condns


////////////////////////////////////////////////////////



Manipulating Data in Data Frame:

Resetting the labels when some labels have been deleted/dropped


resest_index(): Fn to reset the indices

issue: A new column gets created called index which is carrying previous indices


Pass another parameter called drop and toggle it to True. It won't another column called 'index' to the table

df.reset_index(drop=True)

////////////////////////////////////////////////////////////


Add and delete column:


Delete Column:

Can use drop(). Will have to change axis to 1, meaning that we are looking to drop the columns
By default the axis is 0 which means we want to drop the rows rather than columns.


Ex:
df.drop("sepal.length", axis=1)
Drops the sepal.length column


Another way:
Can simply call del on column to be deleted

del df("sepal.length")


/////////////////////////////////

Add a column:

Simply we can do , what we were doing with numpy

Vectorization works here as well

c1-c2 does element by element subtraction 

Which is diff of petal_length and petal_width

Ex:

df["diff_pl_pw"]=df["petal_length"]-df["petal_width"]



////////////////////////////////////////////////////


Handling Nan Values:

2 options: 

1) Remove the data points
2) Fill the values with some data 


dropna()
fillna()


To introduce nan in our data set

numpy has an attribute nan


import numpy as np
df.iloc[1:3, 0:4]=np.nan


will add nan in these cells


Dropping Data:

df.dropna(): Will simply drop the data which is nan
After dropping we should reset the index as well
df.reset_index(drop=True, inplace=True)


Filling data:

df.fillna()

We have some options with which we might want to fill nan entries


Fill with the mean
Fill with some other value
Fill with most occuring entry
Fill with the mean of same class itself





For now, we will just fill with mean of column where nan is,


Ex:

df["sw"].fillna(df["sw"].mean(), inplace=True)

Ex:
Filling with particular class mean only:

df["pl"].fillna(df[df["species"]=="Iris-setosa"]["pl"].mean(), inplace=True)




===========================================================================

Handling strings:


We want data in columns to be ints and numerics and not strings

For ex , we have a gender column in our dataset

We can change string to numeric

df["Gender"]

Use apply()
With apply we can apply some fn to our specified series or data


def f(s):
    if s=="Male":
        return(1)
    else:
        return(0)

df["sex"]=df.Gender.apply(f)   # This creates another series. We are simply adding that series as column in data frame
 
Observe we are applying f to every field in df["Gender"]. We are able to apply on every field due to 
vectorization. Then we add another column called sex which either contains 0s or 1s.


After this we can simply drop the extra column

Code:
del df["Gender"]


/////////////////////////////////////////////////

***********

df.value_counts

Return a Series containing counts of unique rows in the DataFrame.

value_counts is a series  method
*************************


nlargest(): Returns the values in sorted order descending 

*********************************************


///////////////////////////////////////

Plotting

Matplotlib


We will focus on 2 graphs:
Scatterplot, Basic points, we will need two arrays.  For each point we will have x,y coordinates. x coordinates will be in one array and y coordinates in other array.
Line Graph

# Importing:
import matplotlib.pyplot as plt


Arrays to plot:
x=[1,2,3]
y=[2,4,6]


Scatter Plotting:
plt.scatter(x,y,c='Orange')  # Plotting the graph 
plt.show() # Showing the graph on screen after it has been plotted



////////////////////////


Plotting the points so that they are connected via some lines:

x=[1,2,3]
y=[2,4,6]

plt.plot(x,y,color='orange',marker='o')
plt.show()



***********

We can use both scatter and plot simulataneously so that dots are also  visible on the plotted graph


************************

 

//////////////////////////////

To decide color of the plot  in plot fn, we can pass a string, the first letter of string determines the color of line.
We have prefixed colors with us

From the second character and so on, it decides how your line should look like and what kind of line it should be.

Ex:   'r-'   means red line
'ro'  means red circles
'b': blue line
'bo': does't create a line, rather creates circles.
'b--': Blue dashed line
'b+': No line, just blue pluses
'b^': blue triangles
'bp': blue pentagons
'b-*': line which is a combination of dash dot


We can numpy array in plot func as well


Plotting Cubes:
# Easy to cube in numpy array

import numpy as np
#no=[1,2,3,4,5,6]

np_no=np.arange(0,5,0.1)
cubes=np_no**3

plt.plot(np_no,cubes,"r-")
plt.show()


///////////////////////////////////////////////////////////////////


What happens if we give one array , instead of 2

a=[3,4,5]
plt.plot(a)
plt.show()


It will assume a to be y.
It will take x as range(len(y))
Basically it will see , that there are 3 nos, so x must be 0,1,2

/////////////////////////////////////////////////////////////////////



plt.grid(): Creates a grid on the graph


///////////////////////////////////////////////////////////////

=====================================
Customizing Graphs:

in .plot() we can pass color specifically as well
We can pass marker as well
Both get plotted together

py.plot(x,y, color='red', marker='o')


//////////////////////////////////////////////

We can add the linewidth as well

plt.plot(np_no,cubes,color='red',marker='p', linewidth=4)


////////////////////////



Adding, labels, Tile and legend

plt.ylabel(label) : Add the y label 
plt.xlabel(label): Add the x labels 
plt.title(title): Adds the title

plt.legend()  : We can add the label arguement when we call plot a particular distribution.
That label will help us adding the legend
The plt.legend() will pick these labels on its own

Changing the axis:
plt.axis():  Requires a list with 4 entries:   start x, end y, start y, end y



plt.grid(): Adds a grid to the graph

//////////////////////////////////////////////////////////


How to add random text at some place??

plt.text(x,y, text): Adds arguement text at coorinates x, y


////////////////////////////////////////////////


Ex: All the features discussed earlier

import numpy as np
#no=[1,2,3,4,5,6]

np_no=np.arange(-5,5,0.2)
cubes=np_no**3
squares=np_no**2

plt.plot(np_no,cubes,color='red',marker='p', linewidth=1, label='cubes')
plt.plot(np_no,squares,color='blue',marker='p', linewidth=1 , label='squares')
plt.ylabel('x**3 and x**2')
plt.xlabel('Numbers')
plt.title("Cube and squares  graph")
plt.legend()
plt.axis([0,10,0,100])
plt.text(4,16,"Random",animated=True, c='k',family='fantasy',size='large')
plt.grid()
plt.show()

//////////////////////////////////////////


==================================================

Bubble charts:

We can have bubbles and the bbble size can signify something.

Very useful when we want to output multiple things in one go

It is just a scatter plot where we can pass a size arguement. The size arguement can depend on some third kind of arguement

import matplotlib.pyplot as plt
import numpy as np
year=np.arange(2012,2018,1)
salary=np.arange(12,18,1)
population=np.linspace(100,500,num=6)
plt.scatter(year,salary, s=population)
plt.show()


Observer here that 's' , the size arguement depends on population

This is because each point has a size associated with it.

We can pass a size array and depending on the array the scatter dots take size.

Over here what we have effectively done is passed the size array as popuation and now the size will be proportional to population

Ex:
plt.scatter(year,salary, s=population, c=['red','yellow','green','orange','pink','blue'])
plt.show()



Over here , for c we have passed an array, but for s , we have just passed the population array



/////////////////////////////////////////////////////////////


Alpha:

How opaque a particular field is.

Alpha is same as CSS.


//////////////////////////////////////////////////


For colors we can even pass a sequence of numbers

plt.scatter(year,salary, s=population, c=year, alpha=0.4)
plt.title("Bubble chart")
plt.grid()
plt.show()


Over year , we have simply passed the year array  as colors and scatter will take colors on its own



////////////////////////////////////////////////

Edgecolor:
We can also change the edge color of the bubble.
By default it is same as the color of bubble.


Ex:

plt.scatter(year,salary, s=population, c=salary, alpha=0.8, edgecolors='black')
plt.title("Bubble chart")
plt.grid()
plt.show()

/////////////////////////////////////////


We can make bubbles of different shapes by using marker args as we have already discussed earlier


Ex:
plt.scatter(year,salary, s=population, c=salary, alpha=0.8, edgecolors='black' , marker='p')
plt.title("Bubble chart")
plt.grid()
plt.show()


Making the markers as Pentagons


/////////////////////////////////////////////////////

Some text:

plt.text()

Ex:

plt.scatter(year,salary, s=population, c=salary, alpha=0.8, edgecolors='black' , marker='p')
plt.text(year[0]+0.1,salary[0]+0.1,population[0])
plt.title("Bubble chart")
plt.grid()
plt.show()


Observe we have added 0.1 to x and y coordinate in plt.text()  . This is because text might overlap with the figure and
this helps us to shift the text by some distance

/////////////////////////////////////////////////////////


plt.figure(figsize()): To add different properties for the figure like figure size etc


////////////////////////////////////////////////////////


Pie Charts:

plt.pie(x): Plots a pie chart, x is a 1d array carrying the different distributions of pi chart

plt.axis("equal"): Makes both axis equal

////////////////////////////////////////

Changing colors=
pass an argument


figure=plt.figure(figsize=(4,4))
sizes=[3,4,6,2]
plt.pie(sizes,shadow=True,labels=["lbl1","lbl2","lbl3","lbl4"],colors=["red","blue","green","orange"])
plt.legend()
plt.axis("equal")
plt.show()


colors should be an array having same size as that of 1d array given for plotting
Over here sizes has been plotted as pie chart.
len(sizes)==len(colors)



Passing labels:
Just like colors, we can pass an arguement for labels= 1d array. The size of labels should once again be equal 
to the array that was given for plotting




//////////////////////////////////////////

Adding percentages in wedge of Pi chart

need to pass a parameter called autopct

Adds percentages

plt.pie(sizes,shadow=True,labels=["lbl1","lbl2","lbl3","lbl4"],colors=["red","blue","green","orange"],autopct="%.1f%%")




Over here percentages will be added and rounded to first decimal place
autopct="%.1f%%"

% means start
.1f mean print till first decimal

%% signify % is a special char and add % after this
We can pass a fn here as well and return text accordingly



///////////////////////////////////////////


Highlight 1 particular class:

Explode the particular class

1d array which tells by how much do we want to explode each class

plt.pie(sizes,shadow=True,labels=["lbl1","lbl2","lbl3","lbl4"],colors=["red","blue","green","orange"],autopct="%.1f", explode=[0.1,0,0,0])

explode=[0.1,0,0,0]


/////////////////////////////////////


counterclock: By default our graph is in anti clockwise dirn.
We can toggle this parameter to false and graph is plotted in clockwise dirn.



startangle: Angle at which 0deg starts

//////////////////////////////////////////



===============================================
Histogram

Great way to look at distribution of data.
Great way to know the frequency of data 


plt.hist(x):
Plots a histogram. Takes input , 1d array x. Depending on the frequency of elements the individual bar is plotted.

it clubs various frequencies together by default. Can use edge color arguuemnet
plt.hist(a, edgecolor='black')

We have edges now colored black


xticks: The nos that are shown on the x axis of graph.

So plt.xticks(x): Plots these nos on x axis so that we can have greater clarity

Ex:
ticks=np.arange(23)
plt.xticks(ticks)
/////////////////////////////////////////////////////////////


Bins:
No of towers plotted on Histogram.
By default no of towers are 10
We can change it by passing an arguement called bins, which increases the no of bins present

plt.hist(a, edgecolor="black", bins=20)
Now we will have 20 bins, so we can have a very deeper look at the data

Size/ Width of each bin can be calculated by:  (maxNo-minno)/no_of_bins
Over here: (20-1)/11:  Nearly 2 units

In each bin , if the bin spans:  [a,b)   then a is included and b is not.
Except the last bin, in last bin both a and b are included, in last [a,b]


We can have our bins distributed in specific way by passing a list to bins instead of a single number

plt.hist(a, edgecolor='black',bins=[1,3,4,5,7,9,15,23])


The bins here are:   [1,3), [3,4),[4,5),[5,7),[7,9),[9,15),[15,23]

//////////////////////////////////////////////


Bar Grpah:

We have some values and we want to show some x values

Ex:

figure=plt.figure()
plt.bar(year,salary)
plt.show()



More precisely:

plt.bar(x values , heights)


Works well for limited values.
Weird for huge ammount of values


Can change width of bars.

plt.bar(year,salary, width=0.5)

Default width is 0.8

/////////////////////////////////////////

Rotation of xticks:

We can rotate the xticks by some angle and change how text looks on the xaxis of graph

plt.xticks(rotation=40)

Rotates the text on x axis by 40 degree

////////////////////////////////////////////////////


======================================================


Which graph to use??

Depends on type of data we have


		Line			Bar			Scatter

For all three we will have data in form of two arrays say x   and y

Bar grpahs work well when we have  some few categories and we want to see how one category 	differs from other. Really good to look at trends.
But only works for few data. Ex : Time vs Stock price for small data


line graph will work very well when we have huge data and where bar graph fails



Scatter works very well when we want to see corelation b/2 two variables




//////////////////////////////////////


Say country has released its budget and we want to see what %age of budget goes to which sector.

Pie charts work best here.
Whenever we want to show %ages or which section gets which %age

Bubble charts work great when we want to show 2 variables but we want to show a third variable as well when sombody is visualizing these 2 variables.
We wont need to make graph 3d


////////////////////////////////////////////

Histograms are perfect to show what is the frequency of data points in a certain range
Whenever we want to see frequency visualizations.



//////////////////////////////////////////////


===================================================================


*************************
dataframe.sort_index(inplace=True): Sorts the index inplace
*********************

=======================================================


series.replace(to_replace=None,
    value=None,
    inplace=False,)


Replace values given in `to_replace` with `value`.

Values of the Series are replaced with other values dynamically.


///////////////////////////////////////////////////////////////////////////////


pd.to_numeric(arg, errors='raise', downcast=None)

Convert argument to a numeric type. The default return dtype is `float64` or `int64`
depending on the data supplied.






//////////////////////////////////////////////

=====================================================

SQL basics:


Data: Collection or set of values in form of text, words, numbers, videos etc


Basic ops should be easy


retreival

update:   add, delete, modify

/////////////////////////////////////////

Data Storage:

Traditional way: on papers
File Systems
Databases


//////////////////////////////////

File Systems:
Storing data as files.

Drawbacks:

1) Data redundancy(Same information present in many locations)
2) Incosistency(Different data for same entity at different locations)
3) No concurrent access(Multiple users can)
4) No relationship b/w multiple files
5) No backup and recovery
etc


////////////////////////////


Databases: collection of inter related data which helps in effecient retreival, insertion and deletion of data

DBMS: application which mantains or manage databases


Types of DBMS:

1) Hieracrchial DBMS
2) Network DBMS
3) Relational DBMS
4) Object oriented DBMS



/////////////////////////////////////////////////


===================================================


Relational Database:

Everything storedin relations/tables

One row can be termed as tuple, one column can be termed as tuple

Onr table represents one entity

Entity: Can be anything real world. For ex: table.

Entity:

Student

Attributes:

id
name
contact
address

///////////
*****
Entity: Just like class of OOPS
Tuple: Object in OOPS
Attributes:Instance variables in OOPS
**********

///////////////////////////


RDBMS: Software system which is used to mantain relational databases

///////////////////////////



Primary Key: Minimal set of columns and attributes that uniquely identifies row in a table.

Key features: Must contain unique values for each row of data,  cannot contain NULL values


///////////////////////////////////////////////

Foreign key:  Foreign keys are columns that point to or matches the primary key columns in other tables.

The foreign keys can be used to cross referebce tables. Foreign keys do not need to have unique values in the referencing relation.

 
  
===============================================================


SQL:
Structured Query Language

Language to interact with relational database.

An SQL query is how you access the data. Using an SQL query is how you access the data. Using an SQL query , we 
can create and delete, or modify tables, as well as select, insert and delete data from existing tables.



Example of popular RDBMS:

1) SQLite(Open source)
2) MySQL(Open source)
3) PostgresSQL(Open source)
4) Oracle DB(owned by Oracle)
5) SQL Server(owned by Microsoft)


================================================================

To work with SQL open SQL Command Line client from startup menu and enter the password

======================================================



Create :

Basic commands:


Show all databases: show databases;

Create database: create database database_name;

Use database: use database_name;

List all tables inside a database: show tables;

/////////////////////////////////////////////
SQL commands are case insensitive

Clear the screen: system cls

////////////////////////////////////////


Create new table:



create table table-name(
column_name1 datatype1,
column_name2 datatype2,
column_name3 datatype3
);


create table Student(ID INT, NAME VARCHAR(100), AGE INT);


//////////////////////////////////////////


Create table with some default constraints:

1) Primary key
2) Not null
3) With some default value


create table Faculty(id int primary key,name varchar(100) , course varchar(100) not null , salary int default 1000 );



/////////////////////////////////

Checking out the structure of table:

describe table_name;


/////////////////////////////////////////////


NULL means absent value

///////////////////////////////////////


Adding data into table:

insert into table_name values(data1,data2,.........)



insert into table_name(column1, column2,.......) values (data1,data2,data3,............)
Ex:    insert into student(id, name, age) values(1,'Rachinder', 23);

	Not providing all the values:  insert into student(id,name) values(2,'ramesh');


	Can shuffle the order of insertion as well:   insert into student(name, age, id) values('suresh', 34,3);


///////////////////////////////////////////////////////


Fetching all the data from table:


select * from Student;  




////////////////////////////////////////////////////////////

=======================================================


update table:

add column in already existing table, update data type of a column, delete a column etc.


Command: alter

We want to add new column in student table

alter table table_name add(Address varchar(100));



Example:   alter table student add(Address varchar(100) default 'New Delhi');


///////////////////////////////////////////////////
Adding multiple columns in one go:

alter table student add(Email varchar(100), ContactNo v



///////////////////////////////////////////////////////////



New column with default value

 alter table student add(Marks int default 75);

////////////////////////////////////////////////////////////

Alter- Modify Column:

change data type of existing column:
alter table table_name modify column_name datatype;
Example:  alter table student modify contactno int;


Rename a column:

alter table table_name change old_column_name new_column_name datatype;




Delete a column:
alter table table_name drop column column_name;


alter table student drop column test;



Deleting multiple columns:

alter table student drop column Email , drop column contactno;


////////////////////////////////////////////////////////////

====================================================================

Retreive data:

select : Allows us retrieve specific information as per our requirements from a relational database.
Returns a set of records from one or more databases.


Different variations:

1) Select *
2) Select one column
3) Select multiple columns


Fetch name column from student relation:

 select name from faculty;


////////////////////////////////////////////////////////////////////

Loading data into our dbms from some file:

source filepath/filename.sql


Can use this link: https://www.mysqltutorial.org/how-to-load-sample-database-into-mysql-database-server.aspx
/////////////////////////////////////////////////


Restrict result set:

Limit: Restrict number of rows that are returned
	Example:  select * from faculties limit 1;


Distinct: Ignores duplicates

Example:
select distinct name from payments;

 ///////////////////////////////////////////////////////////////////



Filter results:


where:  We can specify a selection criteria to select the required records from the table.
Looks like an if condn in programming language.

Different ops supported

Relational ops (<,>, = , <= etc)
Logical Ops (And, OR)
Is Null and is not Null



Example:  select * from orders where status='In Process';

Example 2:    select * from orders where orderDate='2003-01-06';


Example 3: select * from orders where comments is null;  // Here we cannot write something like:  coments=null

///////////////////////////////////////////////////////////////////////////////


Aggregate functions:

Perform calculation on a set of values and return a single value

different functions:   
count:    Count(*),   Count(column_name)    , Count(distinct column_name)

Count(*) considers null values.
Others count functions don't consider not null values.



Example: select count(*) from orders;    // Will return number of rows

Example 2: select count(distinct status) from orders;

Avg:

Example: select avg(amount) from payments;
Example 2: select avg(amount) from payments where amount>2000;

Max:

Min:

Sum:



If we apply aggregate fns like avg , max, min etc on non numeric columns the answer is always 0.



/////////////////////////////////////////////////////


=========================================================

update and delete:

update row of a table:

update table_name set column_name='column_value';
update table_name set column_name='column_value' where condition;

Example: 

 update faculty set salary=10000;


Delete:

delete from table_name where condition;

Example: delete from faculty where id=2;


Truncate :

truncate table_name;

Used to delete all the rows from table. Just like delete without where


Drop:

drop table table_name
Delete table completely. Truncate just deletes the rows. Drop destroys the table completely.


==================================================================================



SQL advanced:

Group by: used to group rows that have same values.


Summarizes data from database

Groupby clause returns one row for each group.


Example:    select country, customername from customers group by country;
	Here as there are multiple customers for one country, one customername will be fetched. That customer will be decided on first cum first serve.


Example2:   select country,count(country) from customers group by country;

Returns us the count of customers from each country.


Example3: select state,country,count(country) from customers group by country order by count(country);
How do we decide which state will be printed.
Whatever state that comes first which will have a particular country gets picked.

Ex:  Australia

States : Victoria, Queensland, NSW etc
Suppose victoria  come first in the datatable for australia.

Victoria gets picked as state. 

/////////////////////////////////////////////////////////////////////////////





================================================================================



Group by 2:

Combining group by and where


select country,count(*) from customers where state is not null group by country;


Lets write query for:

Sum of credit limit for each country:
select country,sum(creditlimit) from customers group by country;





Group by on multiple columns:

country, state


Query:  select country, state, count(*) from customers group by country, state;

Two subqueries will be executed here.
First groups will be created on country.

For example groups created on Country.

Then for each country different groups will be created.

Over here:
For ex Australia:

Australia    NSW                   2 
Australia    Queensland            1 
Australia     Victoria              2 

Observe over here all the states get printed once.

=================================================================================


Having, order by:

where keyword cannot be used with group functions.
We have to use having clause in select statement to specify filter conditions for grouped results.
If the group by clause is omitted, having clause behaves like where clause.


**********
select state, count(*) from customers where state is not null group by state;


This still works because we can't use where on that column which respresnts value for each group. Overhere 
count(*) denotes the value for each group

If we had done something like:

select state, count(*) from customers where count(*) > 5 group by state;

We get an error:
ERROR 1111 (HY000): Invalid use of group function


We use having here:

select state, count(*) from customers group by state having count(*) > 5;

having comes after group by clause in the query


/////////////////////////////////////////////////

order by:
sorts the reselt set in ascending or descending order
Sorts in ascending order by default
To sort in descending order use DESC keyword.


//////////////////////////////////////////////

IN:


Just like python

Multiple ors can be avoided with this.

shorthand for multiple or conditions.
in operator allows you to determine if a specified value matches any value from a list or from a subquery

select column1, column2 from table_name where (expr|column_1) IN ('value1','value2',.......);

The values in teh list must be seperated by a comma(,)


Can be clubbed with multiple filter conditions as well.
Can add limit constraint.

////////////////////////////////////////////
Not can be combined with in , just like we do in python


////////////////////////////////////////////////////



===============================================================


select * from products where productline not in("Motorcycles","classic cars", 'vinatge cars');


Rewrite without using not in, just use where.

Negation rule for logical ops:

not(A or B)=  not A and not B,  where A and B are some consitions.

So Answer: 
    select * from products where productline<> "Motorcycles" and productLine<>"classic cars" and productLine <> "vinatge cars";



1002, 1056



////////////////////////////////////////////////////////////////////////////////

Between:


shorthand for <= and >=

We can use between clause to replace a combination of "greater than  equal and less than equal" conditions.


Select column_name(s) FROM table_name WHERE column_name BETWEEN value1 AND value2;




values can be numbers, texts or dates

Example: Doing with string:

select productname, productline, quantityinstock, buyprice from products where productname between "A" and "D";

Over here comparison is done with respect to dictionary order.


We can use not to negate our results here as well.

//////////////////////////////////////////////////////////////////////////////////////////////////////////


Like:

Filter results using some patterns

Like is used in a where clause to search for a specified pattern in a column.

There are two wildcards used in conjunction with like operator:

% : The percent sign represents zero, one or multiple characters.  Can represent 0 to n characters
_: The underscore represents a single character.



Syntax:  Select column1, column2 from table_name where column LIKE pattern

Example:  select customernumber, customername from customers where customername like "C%";

Example2:  select customernumber, customername from customers where customername like "C%co.";
Starts with C and ends with co.



Examples:

S%:  Starts with 'S'
%S%: Has S in any position

_SS%: S in 2nd and 3rd position.

S_%_%; Starts with S and has minimum 3 characters

%S: End with S.

_S%P: S in second position and ends with P,

S___P: 5 digits , starts with S and end swith P.




We can club like with not to negate the condition here as well.


Escape characters: To use % and _ as part of string as well.

For example if you have a name like %qr. 
we can write something     :  like  "%\%%"

\ serves as wildcard and tells sql to treat next character as part of string only.

////////////////////////////////////////////////////////////////////////////


Joins:

We can get data from multiple tables with joins


With join, we can query data from two (or multiple) tables based on a related column which is present in both the tables.

While performing join we need to specify the shared column and the condition on which we want to join tables.

You can use Joins in the select, update and delete statements to join multiple tables.

At least one column should be present in both tables.

Types of joins:

Inner Join or Simple Join
Left Outer Join or Left Join
Right Outer Join or Right Join
Full Outer Join or Full Join



/////////////////////////////////////////////////////////////////////////



Inner Join:

Easiest and simplest join

Intersection b/w two tables

This will only return rows when there is atleast one row in both tables that match the specified condition.

Syntax:


Select table1.col1, table2.col2,....
From table_name1
INNER JOIN table_name2
ON
table1.column_ 




Example:

select t1.ID, t1.name, t2.id, t2.name from t1 inner join t2 on t1.name=t2.id;
We can have * as well in select , all the column will get selected.

How it works:
t1.name column is picked
For each value in t1.name , simmilar value in t2.id is searched.
For each tuple where t1.name equals t2.id we get a tuple in resultant datable.

///////////////////////////////////////////////////////////////


We can add filters as well to filter out the results.


Example: select * from t1 inner join t2 on t1.name=t2.id where t1.name='A';





////////////////////////////////////////////////////

If there are ambiguity in columns of the two tables then we will have to refer the columns with their tables.
IF there is no ambiguity we can directly use the columns.

//////////////////////////////////////////////////////////


When we are writting inner join query, we can chose to skip inner and by default join is taken as inner.



====================================================


Use classicmodels database:

Tick the correct order numbers of 'Atelier graphique' (customer name) from orders and customer table

Query:
 select orderNumber from orders inner join customers on orders.customernumber=customers.customernumber where customers.customername='Atelier graphique';


==========================================================================


Inner join with 3 tables:


Syntax:



Select table1.col1, table2.col2,...........

from table1 
inner join table_2
on 
table1.col_nam=table2.col_name
INNER Join table_3
ON 
table_2.col_name=table_3.col_name



Example:

select * from class inner join teacher on class.teacherid=teacher.teacherid inner join subject on teacher.subjectid=subject.id;


How this works??
First the first two tables are joined, a temp data table is created. The temp data table is then joined with the third data table.


In this way we can join n tables as well.


 

///////////////////////////////////////////////////////////////////


Get sum of priceeach from orderdetails table for customer name ’Atelier graphique’ from customers table matching orderNumber from orders table


A litlle different query:

select customers.customername, sum(priceeach) from orderdetails inner join orders on orderdetails.ordernumber=orders.ordernumber inner join customers where orders.customernumber=customers.customernumber group by customers.customername order by customers.customername;



//////////////////////////////////////////////////////////



Left join and right join:


Left join: Returns all the rows from left table specified and only those rows from other table where join
condition is matched.


Syntax: 

select table1.col1, table2.col2,....
From table_1
left join table_2

on table1.column_name=table2.column_name


Example:

 select * from t1 left join t2 on t1.id=t2.id;


How it works:


t1.id column is picked, it is matched with t2.id.

For those tuples, where we get a match, we get a value from t2
For those tuples , where we are unable to match any column,null is returned from t2


Same is the case with right join.


Example:
 select * from t1 right join t2 on t1.id=t2.id;



////////////////////////////////////////////////////////////////////////////////


===========================================================
Indexing:


Suppose we want to continuosly refer a particular tupple in a data table many times.
We can use where clause to do this.
But every time we execute this query , linear search is performed. So time complexity is not very good.
That's where indexes come into play.

Just like we have in arrays, using indexes in arrays we can get data in O(1) time.


Indexing is a way to optimize our search queries on large databases. With the help of indexing, we can 
avoid scanning the whole table to obtain the results we are looking for.


In more formal terms, indexes in database is a kind of data structure that improves the speed of operations in a table.


///////////////////////////////////////////////////

=======================================================================================


We chose that column to create index on which we have to perform search ops very frequently.


To see if an index is already there on the table:


show indexes from table_name;


Example:  show indexes from faculty;

Output:

+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
| Table   | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression |
+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
| faculty |          0 | PRIMARY  |            1 | id          | A         |           1 |     NULL |   NULL |      | BTREE      |         |               | YES     | NULL       |
+---------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+



A BTree is used over here as an index.



The index over here is chosen is by default



*******************

In SQL if any key is marked as Primary Key or Unique key, that column by default is taken as the index column


***************************


////////////////////////////////////////////
Primary key+Null= unique key



//////////////////////////////////////////////////

Data is sorted on the basis of index if key is primary.

Even if we enter a new data point inside data table, the posn of insertion is according to the sorted collection.



////////////////////////////////////////////////////////////


Data is not sorted on the basis of index if key is unique.
Here data is inserted at the end only. Insertion Order is followed here rather than some sorting.

///////////////////////////////////////////////////////////


Default indexing:

Clustured: By default applied if we mark a key as primary

Non-Clustured: By default applied if we mark a key as unique.



Reason why primary key resultant index gets us sorted data wheras unique key does not??

Cluster indexing impacts the order of arangement of the table and arranges on the basis of column on which index is applied.

 For non clustered indexing: Over here a new data structure is created which carries the column on which index is created
(say id)  and references(addresses) to these ids in the original table.

So when we retrieve data from this, this data structure helps in fetching the data quicklly.


For ex: 
Original table:

ID      
1
5
3
2


In case of clustering index:
Table itself becomes:   
ID
1
2 
3
5


In case of non clustering index:

A new data structure created


Id    Reference
1	Address of row having id 1
2  	Address of row having id 2

ans so on
////////////////////////////////////////////////////////////

==============================================================


Difference in execution b/w indexing and non indexing


********************
Mul Key: Multiple key.  This is applied on that column on which index has been applied but which
is not unique.   It allows multiple tuples to have same data.

Example:

+------------------------+---------------+------+-----+---------+-------+
| Field                  | Type          | Null | Key | Default | Extra |
+------------------------+---------------+------+-----+---------+-------+
| customerNumber         | int           | NO   | PRI | NULL    |       |
| customerName           | varchar(50)   | NO   |     | NULL    |       |
| contactLastName        | varchar(50)   | NO   |     | NULL    |       |
| contactFirstName       | varchar(50)   | NO   |     | NULL    |       |
| phone                  | varchar(50)   | NO   |     | NULL    |       |
| addressLine1           | varchar(50)   | NO   |     | NULL    |       |
| addressLine2           | varchar(50)   | YES  |     | NULL    |       |
| city                   | varchar(50)   | NO   |     | NULL    |       |
| state                  | varchar(50)   | YES  |     | NULL    |       |
| postalCode             | varchar(15)   | YES  |     | NULL    |       |
| country                | varchar(50)   | NO   |     | NULL    |       |
| salesRepEmployeeNumber | int           | YES  | MUL | NULL    |       |
| creditLimit            | decimal(10,2) | YES  |     | NULL    |       |
+------------------------+---------------+------+-----+---------+-------+
13 rows in set (0.00 sec)


salesRepEmployeeNumber here has MUL key. This means this column/key/ attribute is not unique but still
indexing is applied.




We can confirm this as well

show indexes from customers;

+-----------+------------+------------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
| Table     | Non_unique | Key_name               | Seq_in_index | Column_name            | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression |
+-----------+------------+------------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
| customers |          0 | PRIMARY                |            1 | customerNumber         | A         |         122 |     NULL |   NULL |      | BTREE      |         |               | YES     | NULL       |
| customers |          1 | salesRepEmployeeNumber |            1 | salesRepEmployeeNumber | A         |          16 |     NULL |   NULL | YES  | BTREE      |         |               | YES     | NULL       |
+-----------+------------+------------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+



*********************************************




To check how many rows have been searched  while executing a query, we can use explain.

explain query


For example:

Suppose base query:  select * from customers where customernumber=480;

Using explain: explain select * from customers where customernumber=480;

Output:

+----+-------------+-----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
| id | select_type | table     | partitions | type  | possible_keys | key     | key_len | ref   | rows | filtered | Extra |
+----+-------------+-----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | customers | NULL       | const | PRIMARY       | PRIMARY | 4       | const |    1 |   100.00 | NULL  |
+----+-------------+-----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+

Observe that, we have in rows parameter only 1. That means only 1 row has been actually checked.




Let's see no of rows compared for non-index rows:

 explain select * from customers where customername='Royal belge';


Output:

+----+-------------+-----------+------------+------+---------------+------+---------+------+------+----------+-------------+
| id | select_type | table     | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+----+-------------+-----------+------------+------+---------------+------+---------+------+------+----------+-------------+
|  1 | SIMPLE      | customers | NULL       | ALL  | NULL          | NULL | NULL    | NULL |  122 |    10.00 | Using where |
+----+-------------+-----------+------------+------+---------------+------+---------+------+------+----------+-------------+


Over here 122 rows have been scanned.



Actually this table contain 122 rows itself.
select count(*) from customers;

+----------+
| count(*) |
+----------+
|      122 |
+----------+


So all rows have been compared as normally in output we get all the rows where this filter is satisfied.
So that's why to achieve this all rows had to be compared.



///////////////////////////////////////////////////////////////////////////////////////////////////


Comparisons in MUL key index

Example:
 select * from customers where salesrepemployeenumber=1504;


Output :

9 rows satisfy this filter.  (Haven't added the rows themselves here.)


Using explain:
 explain select * from customers where salesrepemployeenumber=1504;
+----+-------------+-----------+------------+------+------------------------+------------------------+---------+-------+------+----------+-------+
| id | select_type | table     | partitions | type | possible_keys          | key                    | key_len | ref   | rows | filtered | Extra |
+----+-------------+-----------+------------+------+------------------------+------------------------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | customers | NULL       | ref  | salesRepEmployeeNumber | salesRepEmployeeNumber | 5       | const |    9 |   100.00 | NULL  |
+----+-------------+-----------+------------+------+------------------------+------------------------+---------+-------+------+----------+-------+

Observe that 9 rows were only compared.


///////////////////////////////////////////////////////////////////////////////////////////////////

======================================================================


Adding and removing indexes:

Will use cutomers table in classicmodels databases


Syntax:


create index index_name on table_name(col_name)

Example:   create index postal_code on customers(postalcode);

+-----------+------------+------------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
| Table     | Non_unique | Key_name               | Seq_in_index | Column_name            | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression |
+-----------+------------+------------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
| customers |          0 | PRIMARY                |            1 | customerNumber         | A         |         122 |     NULL |   NULL |      | BTREE      |         |               | YES     | NULL       |
| customers |          1 | salesRepEmployeeNumber |            1 | salesRepEmployeeNumber | A         |          16 |     NULL |   NULL | YES  | BTREE      |         |               | YES     | NULL       |
| customers |          1 | postal_code            |            1 | postalCode             | A         |          95 |     NULL |   NULL | YES  | BTREE      |         |               | YES     | NULL       |
+-----------+------------+------------------------+--------------+------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+


/////////////////////////////////////////////////////////////////

Creating index while creating the table:

create table test (id int primary key, name varchar(100) , address varchar(100), index name_index (name));



///////////////////////////////////////////////////////////


Remove indexing:


drop index index_name on table_name;



Example:drop index postal_code on customers;


//////////////////////////////////////////////



Why don't we have indexes on all the columns:

Indexes only good when we have to perfrom lot of search queries.

Indexing is bad when we have to insert/delete very frequently.
Indexing slows these ops down.

If its non clustured indexing When anything is added, updates need to happen in the data structures storing the references as well.
Simmilar is the case with droping indexes as well.

Normally BTrees are Data Structures implementing indexing


//////////////////////////////////////////////////////


===================================================



SQLite:
relational dbms based on sql language.
Self containes, serverless and zero-configuration
very fast and light weight, the entire database stored in a single disk file.
The lite in SQL lite means light weight in trms of setup, database administration and required resource.
Used in lot of applications as internal storage data.



Libraries:

mysqlalchemy

sqllite3



We will use sqllite3 

After Python 2.5 this library comes by default

=======================================================================

Connect with  db:

Import sqlite3
Connect with database
Create cursor object
Execute commands


=========================================================================

We work on imdb.sqlite.

We can download the database directly from coding ninjas in the lecture itself.


Importing the library:

import sqlite3


Connect methos:

sqlite3.connect(path of database):
Opens a connection to the SQLite database file *database*.

Example:
db=sqlite3.connect('IMDB.sqlite')




///////////////////////////

What if we pass path for which file doesn't exists??

We won't get any error.

It will create a file on its own.


////////////////////////////////////

We can use DB browser applications to see our sqlite databases.

We can open imdb.sqlite from db browser and see the data.



///////////////////////////////////////////

===================================================

Cursor object:

db.cursor()


Returns a cursor for the connection.

We can use cursor object to execute queries on the sqlite db.


/////////////////////////////////////////////////////


Creating new table in database

We already created a new db.


db=sqlite3.connect('school.sqlite')



Creating table inside this db:

sql_query='create table student(rollno int primary key, name text, age int)'

cur.execute(sql_query)


In sqlite, instead of varchar we use text as datatype



/////////////////////////////////////////////////////////


Whatever manipulations we do in the database won't be reflected until we commit the changes.

So we need to commit the changes as well.

db.commi(): Commits the current transaction


===========================================================


Close the connection:

db.close(): Closes the current connection. After this no command will be executed.
It is a good practice to close the connection.

===============================================================


Passing parameters in query:


We can pass in the query with some parameters.



rollNo=104
Name="Nikhil"
Age=34

Suppose we want to insert these into our table.

We can pass this using paramters.

cur.execute('Insert into student values(?,?,?)',(rollNo, Name, Age))

Explanation:
The ? serve as placeholders. 

(rollNo, Name, Age): Its a tuple where all the paramters have been passed.

/////////////////////////////////////////////////////
We can use Format function as well
Be mindful of the quotation marks


rollNo=105
Name="Nikhila"
Age=35


sql_query='''insert into student values({},"{}",{})'''.format(rollNo,Name,Age)
cur.execute(sql_query)
db.commit()



////////////////////////////////////////////////////////


executemany():

We can execute many queries in one go with this function


We pass the arguements as list of tuples:

Example:

values=[(106,"Nidhi",20),(108,"Manisha",21),(107,"Ankush",22)]

cur.executemany('Insert into student values(?,?,?)',values)
db.commit()

 
As an arguement we just need to pass the list of tupples name.

//////////////////////////////////////////////////////////////////


We can update and delete as well using Python


Example:
cur.execute('Update student set age=30')


//////////////////////////////////////////////////////////////////////

Passing parameters with update:


roll=103

cur.execute('Update student set age=43 where rollNo=?',(roll,))



We need to pass a tuple as argument.

Also observe that (roll) alone won't create a tuple. We need to add a comma after that as well.

(roll, ) is a tupple


///////////////////////////////////////////////////////////////

=============================================================

Fetch data:

Extract each and every row:

cur.execute('Select * from student')


We can treat cur as an iterable and iterate over it


Example:

for row in cur:
    print(row)



Different methods:
fetchone()
fetchmany()
fetchall()




fetchone: fetches one row with the help of cursor object.

cur.execute('Select * from student')
cur.fetchone()


fetchmany(no of results):  Fethes no of results rows.
The iterator keeps updating with each fetch

cur.fetchmany(3)

Will fetch row 2,3,4 because row 1 was already fetched using fetch one.
Now the iterator sits on row 4
If no value is passed to fetchmany(), by default 1 row is fetched.


cur.fetchall(): Fetches all from current iterator on cur


///////////////////////////////////////////////////////////////////////////



==============================================================================


SQLite with pandas:


Function:
pd.read_sql_query(QUERY, Connector object):

Read SQL query into a DataFrame.

Returns a DataFrame corresponding to the result set of the query
string. Optionally provide an `index_col` parameter to use one of the
columns as the index, otherwise default integer index will be used.




Example:


df=pd.read_sql_query('select * from student', db)


Advantages of this method:


1) No need to use cursor methods
2) No need to call explicit fetch methods
3) We get a dataframe, which is very good for datamanipulation, using boolean masking

=================================

Creating datatable using pandas:


Adds datatable in to the dataset
data_frame_name.to_sql(Datatable_name, connector object)





==========================================================



Adding another column:

data=pd.read_sql_query('Select * from Faculty',db)
data["Address"]='test'
data.to_sql("Faculty",db,if_exists='replace')



if_exists parameter:  what to do if table already exists.

By default the behaviour is raise error.

We can change the behaviour to "replace" which replaces the existing table.

////////////////////////////////////////////////////////////////////////////


Ques: IMDB Movie:   Need to perform inner join in the query


//////////////////////////////////////////////////////////////////


pd.Join(left, right, how, on=None, left_on=None, right_on=None)

Merge DataFrame or named Series objects with a database-style join.

left,right: Dataframe
how : {'left', 'right', 'outer', 'inner', 'cross'}, default 'inner'
    Type of merge to be performed.

//////////////////////////////////////////////////////////////////


========================================================


APIs:


Application Programming Interface


Conventional data collection methods:

1) Using databases
2) Loading data from files like csv.


Sometimes these techniques fails.

Some scenarios where they can fail:

1) We only need part of data
2) Data is very dynamic i.e. it changes very frequently.



In these scenarios , it doesn't make sense to store this data on local storage.
We should get data on the fly.

We can use APIs for such a scenario.

///////////////////////////////////////////////

What is an API:



User   -> Request          Waiter          -> Request          Kitchen
            <- Response		          <-  Response   


Waiter is the medium which conveys our message to Kitchen
API is a means of communication b/w two applications. It is an application which achieves this.


			Request(By API)
Your application  --------------------------------------------------------> Database(3rd party)

	             <------------------------------------------------------------
			Response (By API)



API is just like a black box through which we make our request to some database and through this box itself we get a reply

For example we interact with a travel website.
We don't need to interact with many websites, the travel website itself communicates with other websites 
with the help of APIs of other websites.


///////////////////////////////////////////////////////////////////////////////////////////


==================================================================================


Examples of APIs:


Can we get data from all applications using APIs??
No, we can get data only for those websites for which an API exists
Generally every big website has an API


Example of well know APIs:

Google Maps API: We can inbuild maps into our application. Example uber and ola. Whatsapp use it.
Facebook: Our users can login using api
Weather API: Gets temperature. Calls whether API multiple times in a day.
Youtube API: You tube live stream in our website.
Twitter API: Can fetch data from twitter


Why do companies allow their websites  to be accessible via API??
Indirectly they increase their user base.



Elements of an API:


1) Access: Who all are allowed to access API call.
2) Request:
	Methods: Which methods do we call for API??
	Parameters: The additional parameters that we pass
3) Response: The response that we get.

///////////////////////////////////////////////////


===================================================


HTTP basics:

In order to get data using API, we need to make an API request call to retreive data from any website or its server.


Like we do while vsisiting any website, we enter a URL and then hit enter.

URL is reference to a specific web page.

URL consists of 2 parts: 

Protocol identifier:  http or https
Host name: codingninjas.in, amazon.in



//////////////////////////////////////////////////////


HTTP:
Hyper text transfer protocol

Prototcols are nothing but set of rules which defines how to format and transmit the request to server and how 
to receive response from servers.

A network protocol defines rules and conventions for communication between network devices

Communication b/w client and server on internet happens using HTTP


Information is exchanged in the form of hypertext documents.

A web bropwser may be one of the client, and an application on a computer that hosts a website may be the server.



Methods of HTTP:

GET:    read method(We can read the data using this method)
POST:   write method(We can update our database using this method)


While using API, we also need to make a request to the server.
This request, response will be done using HTTP using some library.

///////////////////////////////////////////////////////////////////


============================================================================

Different HTTP Libraries in Python:

1) httplib
2) urllib
3) requests


We will use requests library

 
requests:  HTTP library written in Python.
We can send HTTP requests using our Python Code and we caan access response also


/////////////////////////////////////////


Importing the library:

import requests



///////////////////////////////////////////


Making an api call:

get() method:

requests.get(url, params=None, **kwargs)

Sends a GET request.



Example:
response=requests.get('https://codingninjas.in/api/v3')

response is a Response object


//////////////////////////////////////////////


Response object attributes:

status_code:  checks if the call was successful or not

	200: Successful                  Any answer that starts with 2 means success. Something like this:  2xxx
	404: Fail			Any answer that starts with 4 means failure. Something like: 4xxxx

Example:  print(response.status_code)


encoding: Encoding of data that we got in response from server.

Example:    print(response.encoding)

Answer:  'utf-8'
Encoding is used when try to access the content of response.

url:    url to which we made our api call
Example:  print(response.url)

text: The data that we fetched from api call. It uses the encoding that we see above.

Example:  print(response.text)

headers: Header info of the response that we get from our api call.
Type is python dictionary


Accesing individual properties using headers dictionary:

Example:   print(response.headers['Date'])

Answer: 'Fri, 27 Aug 2021 20:49:36 GMT'

//////////////////////////////////////////

Header keys in JSON response after API call, are case insensitive 

///////////////////////////////////////////////



================================================================


JSON File format


JSON: Java Script Object Notation.

File format used in interchange of data


The most comman formats fof information retreived, in APIs are JSON and XML.

Light weight, text based, human readable file format. Can be edited using a text editor.


2 parts: 
Key
Value




//////////////////////////////////////////////////////////////


JSON Viewer: Viewer to pretify JSON content in web browser


////////////////////////////////////////////////////////

JSON Data types:

Primitive data types: number(ints and floats), string, boolean, null(emplty value. We can leave the value as nulll as well and it will signify null value)

Complex data type: Array(  "elements":[10,12,13,14]  ), object(Collection of different kind of values)

Object example:

{
"student":{
"name":"Mohit",
"ID": 101,
"addr":"xyz"
}
}

The value for key student is an object as it is a collection of different values

We can create an array of objects as well.



/////////////////////////////////////////////////////////////////////


===============================================================



JSON in python

response.text decodes the response based on response.encoding


JSON to Python:

Object		dict
number		int, float
boolean(true, false)	   boolean(True, False)
string		 str
Array		list, tuple
null		None


JSON data is a string

Library: We will use json library of python.


Method: json.loads(s) will be used

Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance
containing a JSON document) to a Python object.



Example:
json_data='{"roll no":1001}'

python_data=json.loads(json_data)




//////////////////////////////////////


==========================================


Explore JSON data:


Lets explore the response from coding ninjas api


response=requests.get('https://codingninjas.in/api/v3/courses')
python_data=json.loads(response.text)


python_data is a dictionary.

all_courses=python_data['data']['courses']
# Printing the titles
for course in all_courses:
    print(course['title'])


///////////////////////////////////////////


request_object.json():  does the same job as json.loads(). We can use this as well.

Example:
response.json()


It fails if content is not of json type


/////////////////////////////////////////////////////////


==========================================================



Passing parameters:

With every URL there are many endpoints associated.

Sometimes we have to pass parameters to API calls. For ex: sometime APIs need authorization before servimng 
us data.
In those cases we have to pass parameters. Probable parameters that can be passed can be found in the documentation of that API


method:
requests.get('url', params={})

params is a key value pair



Example:
a= requests.get('https://api.openaq.org/v1/cities',params={"country":"AE"})


Every api has its own paramters. We can check the documentation to see available paramters.


///////////////////////////////////////////////////////////////////////////////////

===============================================================================

Parameters 2

Sometimes api requests fail because by default we have to enter some parameters
For example:

a= requests.get('https://www.metaweather.com/api/location/search/')
print(a.status_code)

Result:

403


Over here this failed because we have to pass   Either query or lattlong. Our of the two atleast one needs to be present.



a= requests.get('https://www.metaweather.com/api/location/search/', params={'query':'Delhi'})
print(a.status_code)


Now this works fine.



///////////////////////////////////////////

a= requests.get('https://api.nasa.gov/planetary/apod',params={"api_key":"DEMO_KEY"})

Over here we had to pass the api key.
Api key is used to validate the user.

With API key you can know the source of request and if anything malicious happens we can stop the request.

////////////////////////////////////////////////////////
Here Demo_Key is a default api key

We can create our own key as well, just by signing up.

===============================================


POST requests:


Different requests:

get, post, put, delete, patch

We discuss post here.

post is kind of write request


Any website won't allow us to simply make changes on its website by some random person. 
So we use a dummy website.

httpbin.org/#/HTTP_Methods




Example:
response=requests.post('http://httpbin.org/post')


Method:

.post(url, data=None, json=None, **kwargs)
Sends a POST request.
param url: URL for the new :class:`Request` object.
:param data: (optional) Dictionary, list of tuples, bytes, or file-like
    object to send in the body of the :class:`Request`.



Changing using POST

data parameter is responsible for sending the data.

response=requests.post('http://httpbin.org/post',data={'key1':'val1','Key2':'val2'})



Passing multiple values for 1 key

# Pass as a list
response=requests.post('http://httpbin.org/post',data={'key1':['val1','val3'],'Key2':'val2'})


# Pass as list of tuple

response=requests.post('http://httpbin.org/post',data=[('key1','val1'),('key1','val2'),('key3','val3')])
================================================================



Authorization and Authentication:


Authorization: What you can do

Authentication: Who you are


Some times for API calling we need to authenticate ourselves

////////////////////////////////////////
Simplest one:


HTTP basic authentication:

requests.get(' url', auth=HTTPBasicauth('user','password')  )
or
We can use shorthand

requests.get(url, auth=('user','password'))


Libraries:

from requests.auth import HTTPBasicAuth



Example:
a=requests.get('https://api.github.com/user')
a.status_code

We get code as 401.



We need to authenticate ourselves for this api call

As of 13th November 2020, Github has deprecated basic authentication via username and password.
Attempting to access the Github API using this method will give a 401 error.
Now, a Personal Access Token (PAT) has to be created and used in lieu of the password. Everything
else will remain the same.
Here are the details regarding the deprecation: https://developer.github.com/changes/2020-02-14-
deprecating-password-auth/
Steps to generate a PAT are given here:
https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personalaccess-token


So over here:

a=requests.get('https://api.github.com/user', auth=HTTPBasicAuth('Rachinder3', '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'))


print(a.status_code)

Answer:  200





We have three ways to authenticate ourselves:

BasicAuth:  We just saw it. 
HTTPDigestAuth: Study on your own.
Oauth: Will see later.



/////////////////////////////////////////


Creating repository on github with post:


json.dumps():  Converts a python object to json file

////////////////////////////////////////////////////

GITHUB BASE API: 

'https://api.github.com/'




=====================================================


Reddit:

Here to authenticate ourselves we have to follow different steps.
Reddit is made of subreddit
subreddits: A community regarding some subject. Example Atletico.

Every subreddit's name starts with r/


up arrow and down arrow.

comments

Can upvote and downvote comments also.
Upvotes and downvotes on comments is called Karma


Reddit also uses many abbreviations, like TIL , OP etc. Here OP is not overpowered rather it means original poster.
//////////////////////////////////////////////

Get request to reddit api

Reddit asks us to follow its API rules.


We have to authenticate ourselves via OAuth2


==========================================================


Oauth 2   Introduction:

Oauth stands for open authorization.
We don't need to give anyone our user id and password with this.


For example some cars now come with a valet key.
There are many restrictions on this key and nobody can misuse your authentication in this way.
Oauth is just like this valet key.



Oauth us a token based authentication and authorization system.

It allows access to an end user's account and private information to be used by third-party services withouth exposing
the user's password.
It acts as an intermediary on behalf of end user.

==================================================================


OAuth roles  and processes:

Roles:

Resource owner:  Person/ Party who owns the data.

Client:  Party who wants to get the access via OAuth

Resource server: server which saves the information, private to the user. 

Authorization server:  grants permision to client that, client can access the data of resource owner after taking permision.


Generally Resource server and Authorization server are same


///////////////////////

Ex:
Suppose you want to play with fb friends in PUBG

Pubg makes us an authorizatiom request.
We give authorization grant.
Pubg sends this grant to Fb
FB's authorization server, on the basis of this grant gives access token to PUBG. This grant contains what can be accessed via this like name, email, friends etc. The token is granted keeping that in mind.
PUBG passes this access token to FB's resource server.
PUBG gets the resource returned by FB's resource server.



/////////////////////////////////////////


================================================


Reddit Get access token:


Over here:



Client:						Resource Owner
Me						Reddit user(Me )
			
								Auth Server
						Reddit:		Resource Server

We only want public data hence we will also play the role of Resource owner. On behalf our own account we will get public data.

redirect ai: Where you want top redirect your user after granting authorization.


Guide:  https://github.com/reddit-archive/reddit/wiki/OAuth2


Client id: RFjilM4eVRDLI1ehy389qA
Secret key: XOe14dInOKU3uZKTgELSjrS2Eh0o4g

Obtained from reddit for later use.


We need to make a get request to user to get authorization.

Need to pass many paramters

They can be seen at guide location


state: Random string that we pass and we get back if authorization was granted


Access Token retreival:
We receive authorization code, when resource owner grants request and is redirected to another page.
The code is present in the redirected page itself.



After this make a post request.
The guideline is given in Reddit API Oauth documentation.
Also given in "Reddit Access Token" exercise


grant_type:  standard value 'authorization_code'. Currently we are using Oauth2.
 Oauth2 defines 4 processes with which we can authenticate ourselves.


authorization grant(currently we are working with this)
implicit grant
password type


In post request we need to enter the authorization code that we get when user accepts access.
In that remove the trailing #_

data1={'grant_type':'authorization_code','code':'YqhTtGpkugPKXLV4uAICrWp8S9loaA','redirect_uri':'https://www.google.com/'}

r=requests.post('https://www.reddit.com/api/v1/access_token',data=data1, 
                auth=HTTPBasicAuth('aDyRIfZ0F74pQKDvCHiNeA','INDDR5OmXrmdEgyRCNvT71td9lVWgA'), headers={'User-Agent':'Test'})

Headers is a parameter which we can pass in post and get reuqestss
Here in headers we pass an arguement called User-Agent where we can pass any string.

The User-Agent request header is a characteristic string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent.



Getting data after post request:


You may now make API requests to reddit's servers on behalf of that user, by including the following header in your HTTP requests:

Authorization: bearer TOKEN


h={'User-Agent': 'Test', 'Authorization': 'bearer 410454940303-mcohaq8dju3MEVS_opFZhhDmuD6zHA'}

r=requests.get('https://oauth.reddit.com/best', headers=h)
data=r.json()
print(data)

 In headers we pass the same User-Agent.  Also we add Authorization parameter and replace 'Token' with the access code that was generated.

Also we can go to reddit api base documentation to see which endpoint to hit. Over here we have hit the endpoint returning 
best posts for us.
They are available under listings.

Before making any request we should make sure to have the right scope. It is available next to the endpoint itself.


/////////////////////////////////////////////////////////////



Reddit few more applications:



If we only want limited posts.

We can use params and use limit to restrict the number of responses that we get.
This limit is mentioned in the reddit api documentation.

/////////////////////////////////////////////////////



subscribe endpoints: subscribes a particular endpoint. This is a post request as with this we make some changes.
scope should be subscribe

/////////////////////////////////////////


Github:

Oauth documentation:
https://docs.github.com/en/developers/apps/building-oauth-apps/authorizing-oauth-apps


We have to create oauth app first by going to setting and then developer options

OAUTH:

Client ID:
2e36a9a5f12afefa793b

Secret:
964719c1da33b329c51b0f687387dc45435b5d14



//////////////////////////////////////////////////////////////



Web scrapping:

Getting data from websites

getting data from local systems:  CSV files, sql etc
Getting data from online sources: APIs , Web scraping




Web scraping come in handy when we want to get data from a website which doesn't provide apis


Web scraping: technique for extracting data from internet automatically using our script that simulates human web surfing.
Helps us extract large volumes of data.

Scraping rules:
1) Check website's terms and condns before you scrape it.
2) Do not spam the website by making a lot of requests to a specific page.
3) Update your code from time to time.

Libraries used:

1) Beautifulsoup
2) Selenium
3) Scrapy


Process:

1) Find url you want to scrape
2) Send an HTML request to that URL and get HTML response
3) Parse the HTML content
4) Inspect the web page and find the data that we want to extract
5) Extract required data and store it in the desired format.


===================================================

HTML tour:


we put url in web browser
web browser sends get request to web server
web server responds back with all the data.
this data is rendered and shown to us.


Web server responds with 4 types of data:

1) HTML
2) CSS
3) JavaScript
4) Media Content



HTML:


markup langugae not a programming language.

tags
html, head(config of web page), body(main content)

body tags:   p, h1 to h6,  

anchor tag: adds hyper link

///////////////////////////////////////////////
Relative tags:

child
parent
siblings


html encompasses head and body tags


html tag is parent, head and body tags are children, head and body become siblings of one another.



class and id: uniquely identify elements



//////////////////////////////////////////////////////////////////////////


Beautiful Soup Introduction: 

Load HTML:   use requests.get(url). We get html back. Save it in some variable say 'response'.
Parse HTML: HTML content is in string. Cannot use simple data manipulation. This is because HTML has very deep nesting.
		Thats why we parse the data. We use beautiful soup for this.



To store a string in multiple lines we can add / when one line finishes.
Ex:
'<!DOCTYPE html>\
<html>\
<head>\
<title> Testing Web Page </title>\
</head>\
<body>\'




importing library:

from bs4 import BeautifulSoup


Beautiful soup: A data structure representing a parsed HTML or XML document.

BeautifulSoup(
    markup='',
    features=None,
    builder=None,
    parse_only=None,
    from_encoding=None,
    exclude_encodings=None,
    element_classes=None,
    **kwargs,
)



Example: 
data=BeautifulSoup(html, 'html.parser')


'html.parser' is the parser that is being used.
We have many options for this parser. 
Can be found in documntation.


data here is a beautiful soup object

////////////////////////////////////////////////


data.prettify:

Pretty print this PageElement as a string.
Instance member function of Beautiful Soup object

Example:

print(data.prettify())



////////////////////////////

How to extract data from beautiful soup object

Just do:
data.tag_name

Here data is Beautiful soup object.


Example: We want to access title:

Example:

data.title

output:
<title> Testing Web Page </title>


///////////////////////////////

Accessing tags via classes and ids:



///////////////////////////////////////////

Methods and Attributes:

prettify()
parse.tag
	page.tag.name
	page.tag.string
	page.tags.attrs
		Using get()
		Access like dictionary
get_text()




Example 1:

data.title.name
Output:  'title'


Example2: 
data.title.string


Output: ' Testing Web Page '



Example 3:
data.p.attrs
We can get the attributes(like class and id) of a tag

Output:  {'id': 'first_para'}

Example 4:

Accessing attr dict

data.p['id']

Output:
'first_para'




Example 4:

data.get_text()
" Testing Web Page  Web Scraping Let's start learning Web ScrapingYou can read more about BeautifulSoup from  here  Coding Ninjas "

gets all the data  without any tag.


////////////////////////////////////////

find(): gets the first occurence of a tag
find_all(): find all the elements with a tag
find by class: finds by class
find by id


Example1 :  
data.find('p')
Output:
<p id="first_para">Let's start learning <b>Web Scraping</b></p>


Example2:
data.find_all('p')
Output:

[<p id="first_para">Let's start learning <b>Web Scraping</b></p>,
 <p class="abc" id="second_para">You can read more about BeautifulSoup from <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/"> here </a></p>,
 <p class="abc"><a href="https://codingninjas.in/"> Coding Ninjas </a></p>]


///////////////////////////////////////////////////////////////////


==================================================================


navigating parse Tree:


1) Searching Parse Tree
2) Going up
3) Going down
4) Going sideways
5) Going back and forth


////////////////////////////////////////////////


Searching Parse Tree:

find_all()
1) A string
2) A list
3) True
4) Using id
5) Using class
6) Using CSS selector


Examples:

Ex1: Passing a string:  
data.find_all(['p','a'])
Output:
[<p>Coding Ninjas Website</p>,
 <a href="https://www.codingninjas.in/">Link to Coding Ninjas Website</a>,
 <p id="template_p">This is a template paragraph tag</p>,
 <a href="https://www.facebook.com/codingninjas/">This is the link of our Facebook Page</a>]

Over here, we get all the tags with p and a


Example 2:  passing boolean

data.find_all(True)


This simply returns all the tags that are present in html.

Example 3: Search with id:
data.find_all(id='template_p')

Result: gets element with that id.


Example 4: Search with class:

data.find(class='')


Example 5:

Using CSS selector
Here we need to use   data.select(selector)
We dont use find function here.



///////////////////////////////////



Going down:

Navigating using tag name
	We can use nested tag names also

.string

.strings and stripped_strings

.contents and .children

.descendants

/////////////////////////////////////////

Example:

Suppose we get head tag in this way:

data.head

Output: <head><title>Learning Beautiful Soup</title></head>

Now, we can get title via nested tagging

data.head.title

Output: <title>Learning Beautiful Soup</title>





What is the need for nesting tags? We can get title by simply, data.title

Suppose we have this sort of structure:

<h1>
	<p>
	</p>
</h1>

<h2>
	<p>
	</p>
</h2>

Here if we do data.p, there will be ambuity b.w h1.p and h2.p


To avoid this we can use nesting of tags


///////////////////////////////////////////////////////////////////


data.title: Gets the title in Beautiful Soup object

data.title.string: string is the child of title and prints the data within the title.


//////////////////////////////////////////////////////



Printing only string b/w all p tags:

for p in data.find_all('p'):
    print(p.string)


///////////////////////////////////////////


.string gives us the string if there is only one child


if we have 

<p>

hdljsh

<a>,hdlswhd

</p>

Here we have 2 children. Here .string fails


If we have multiple children, .string returns None as there is ambiguity


///////////////////////////////

For multiple children:

We can use .strings()

li=data.find_all('p')
for i in li:
	print(list(i.strings))


i.strings gives us a generator for each child.
We can convert it to list

Ex:
for p in data.find_all('p'):
    print(list(p.strings))


Output:
['Coding Ninjas Website']
['This is a template paragraph tag']

For each tag we get a list of children.


//////////////////////////////////////////////////////////////////////////


.stripped_strings(): Returns the stripped strings from generators. We dont have to  use strip ourselves



/////////////////////////



.contents:


data.html: Returns us the complete tag.

Suppose we want to know all the children of this tag


data.html.content:

We get a list of all the children tags.

Example:

data.html.contents

[<head><title>Learning Beautiful Soup</title></head>,
 <body><h1> About Us </h1><div class="first_div"><p>Coding Ninjas Website</p><a href="https://www.codingninjas.in/">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id="template_p">This is a template paragraph tag</p><a href="https://www.facebook.com/codingninjas/">This is the link of our Facebook Page</a></body>]


Over here we get two tags as html tag has 2 children  , head and body.

///////////////////////////////////

.children: Here instead of a list, we get an iterator over which we can iterate to get the children tags


Ex:  
for i in data.html.children:
    print(i)
    print()

Output:
for i in data.html.children:
    print(i)
    print()
for i in data.html.children:
    print(i)
    print()
<head><title>Learning Beautiful Soup</title></head>

<body><h1> About Us </h1><div class="first_div"><p>Coding Ninjas Website</p><a href="https://www.codingninjas.in/">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id="template_p">This is a template paragraph tag</p><a href="https://www.facebook.com/codingninjas/">This is the link of our Facebook Page</a></body>



///////////////////////////////////////////////////////////////////

contents vs children:

contents: gives us the list

children: gives us the iteartor

////////////////////////////////////////////////////////////////


.descendants():

Returns all the descendants as generator object, not just the children

Ex:

[<head><title>Learning Beautiful Soup</title></head>,
 <title>Learning Beautiful Soup</title>,
 'Learning Beautiful Soup',
 <body><h1> About Us </h1><div class="first_div"><p>Coding Ninjas Website</p><a href="https://www.codingninjas.in/">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div><p id="template_p">This is a template paragraph tag</p><a href="https://www.facebook.com/codingninjas/">This is the link of our Facebook Page</a></body>,
 <h1> About Us </h1>,
 ' About Us ',
 <div class="first_div"><p>Coding Ninjas Website</p><a href="https://www.codingninjas.in/">Link to Coding Ninjas Website</a><ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul></div>,
 <p>Coding Ninjas Website</p>,
 'Coding Ninjas Website',
 <a href="https://www.codingninjas.in/">Link to Coding Ninjas Website</a>,
 'Link to Coding Ninjas Website',
 <ul><li>This</li><li>is</li><li>an</li><li>unordered</li><li>list.</li></ul>,
 <li>This</li>,
 'This',
 <li>is</li>,
 'is',
 <li>an</li>,
 'an',
 <li>unordered</li>,
 'unordered',
 <li>list.</li>,
 'list.',
 <p id="template_p">This is a template paragraph tag</p>,
 'This is a template paragraph tag',
 <a href="https://www.facebook.com/codingninjas/">This is the link of our Facebook Page</a>,
 'This is the link of our Facebook Page']



///////////////////////////////////////

Can visualize this DOM as a tree

These methods get us the different tags from one tag

//////////////////////////////////////////




Going up:

.parent():



.parents()

//////////////////////////
Going sideways:


.next_sibling and .previous_sibling


.next_siblings and .previous_siblings


///////////////////////////////



Going back and forth:

.next_element and .previous_element


.next_elements  and .prev_elements



///////////////////////////////////////////////

================================================

.name(): Prints the name of tag


data.html.name:

Prints html



=======================================================


We can sort of have a boolean mask here as well

data.find_all(id=True)

Return those tags where id is True meaning id exists


========================================================


First Website scrapping:



Can search 'first webpage'

1) Load the HTML  ,  we make a get request
2) Parse the HTML, use Beautiful soup
3) 


Loading the HTML:
response=requests.get('http://info.cern.ch/hypertext/WWW/TheProject.html')

Printing response.headers:

response.headers
{'Date': 'Tue, 31 Aug 2021 19:11:23 GMT', 'Server': 'Apache', 'Last-Modified': 'Thu, 03 Dec 1992 08:37:20 GMT', 'ETag': '"8a9-291e721905000"', 'Accept-Ranges': 'bytes', 'Content-Length': '2217', 'Connection': 'close', 'Content-Type': 'text/html'}


We can see in content-Type, the type is 'text/HTML'

/////////////////////////////////////////////// 
To get access of HTML:

Simply do response.text

//////////////////////////////////////


We can use inspect element as well to see HTML code.

We can use inspect elemenet and 'select an element to indicate it' options. 
With the arrow icon we indicate the data in the browser, that we are interested in extracting. Then simply check which tags are
responsible  for this particular data.

///////////////////////////////////////////


Get Complete text:


Can use get_text()


Ex:

print(data.get_text())


The World Wide Web project


World Wide WebThe WorldWideWeb (W3) is a wide-area
hypermedia information retrieval
initiative aiming to give universal
access to a large universe of documents.
Everything there is online about
W3 is linked directly or indirectly
to this document, including an executive
summary of the project, Mailing lists
, Policy , November's  W3  news ,
Frequently Asked Questions .

What's out there?
 Pointers to the
world's online information, subjects
, W3 servers, etc.
Help
 on the browser you are using
Software Products
 A list of W3 project
components and their current state.
(e.g. Line Mode ,X11 Viola ,  NeXTStep
, Servers , Tools , Mail robot ,
Library )



////////////////////////////////////////////////////////////


Extract each and every hyperlink



Can use :

1) data.a: Gets the first tag with anchor element
2) data.a['href']:  gets us the href for data.a

3) data.find_all('a'):   Gets all the anchor tags

4) data.find_all(href=True): Gets all the tags where href has value. href is an attribute

///////////////////////////////////////////////////

Extracting Hyperlinks in some section:

Use indicate arrow in inspect element

Check which part / tag does the hbyperlinks fall


Use data.tag
Then call find_all on this to get all anchor elements



Ex:
data.dl.find_all('a')


Output:
Extracts hyperlinks starting from dl tag.


///////////////////////////////////////////////////////////////



Hyperlinks only in dt tags:

li3= data.dl.find_all('dt')

////////////////////////////////////////////////////////////


========================================



Books to scrape:

https://books.toscrape.com/


response=requests.get('https://books.toscrape.com/')
data=BeautifulSoup(response.text)

///////////////////////


Extracting heading:

It is the first hyperlink.

We got to know this via using inspect element

data.a



Can also use tag nesting

data.header.a

This means a tag inside header
/////////////////////////////////////

Extracting details of first book:

Use inspect elemnt and the arrow button

****************

When we are using class in a find() function of beauutifulsoup object, we should do it like:

data.find(class_= 'product_pod')

class has an underscore. Effectively, we write  class_

This differentiates it with keyword class

///////////////////////////////
Printing the title of first book:

b.h3.a['title']

b is the BS object, h3 is the tag, a is the tag and title is the attribute of this tag.


/////////////////////////////////////



getting hyper link to this book:

b.h3.a['href']



The url we get is relative.

To make it complete we can append with base url.
We can see the actual url and try to observe what is the base url 

'https://books.toscrape.com/'+b.h3.a['href']

////////////////////////////////////

In the above problem , it was not practical to use find_all('a') as there are so many hyperlinks on this page.

We needed to do tag nesting. Thats what we did


////////////////////////////////////

Getting list of all 20 books

All the books present under product_pod



books= data.find_all(class_='product_pod')
With this we get a list where each elemnt of a list contains details about one book.



base_url='https://books.toscrape.com/'
for i in books:
    print(base_url+i.h3.a['href'])

This gets us the urls for all the books



/////////////////////////////////////////



Link of all the webpages:

We need to visit each and every page

Currently we are loading just 1st page.
For loading we make a get request. Currently we making get request only to the 1st page.
We need to make the get request to all 50 pages.

/////////////////////////////////////

Inspecting how to go to next page.

Visite next page and see how base url changes.




Page 2 url:  https://books.toscrape.com/catalogue/page-2.html


Only 2 changes.
 

Also we can see with the next button in the web page. Inspect it and see, how it changes.


////////////////////////////////////////////////////


******************
If the urls are not in systematic order, we take the help of next button's href

******************
We will go via this way only so that we know what to do in unsystematic paging scheme.

/////////////////////////////////////////////


Ques, get the url of all the 50 next pages:

Code:
all_urls=['https://books.toscrape.com/catalogue/page-1.html']

current_page='https://books.toscrape.com/catalogue/page-1.html'

base_url='https://books.toscrape.com/catalogue/'


response= requests.get(current_page)

while response.status_code==200:
    data= BeautifulSoup(response.text, 'html.parser')
    next_page=data.find(class_='next')
    # Checking if next page is none. If it is, that we means we areo
    # on the last page and we can simply break
    if next_page==None:
        break

    next_page_url=base_url+next_page.a['href']
    print(next_page_url)
    all_urls.append(next_page_url)
    response= requests.get(next_page_url)


In the lasst page, next becomes None.
We simply break the loop to avoid errors


We can use exception handling as well.
If an exception arises due to this we can simply break the loop

////////////////////////////////////



Now we have a url dictionary. 
To get details of all the books, simple iterate over the array and make these requests




///////////////////////////////////////////////


Saving informations in csv:

For each book 
We will extract Title, Link, Price, No of copies.

For 'number of copies' we will have to visit each page.


Steps:

First we will do for 1 book only

1) Extract the URL of first book

2) Load the HTML of first book.

.string returns None if there are more than 1 child
To get the details of all the children we have to use .contents


3) Extract the desired data

4) Correct the data

We can use regular expressions for this.

Python has an inbiuilt library for regular expressions called re

Library:
 re

Methods:

re.search(pattern, string): returns a match object if pattern is matched.

Example:

For quantity:
re.search('\d+',qty).group()

Explanation:  

/d  means extract  number b/w 0-9
+ means extract all nos in continuation

group() method returns the part of the string which matched the patter


For price:
re.search('[\d.]+', price).group()

[]: matches a collection of nummer

[/d.]:  matches either a number or a dot

+ means extract more than 1


We can use pandas and df.to_csv method to convert a dataframe to csv

To create a data frame we create a 2d list



book_details=[]

book_details.append([title,b1_url,qty,price])

import pandas as pd
df=pd.DataFrame(book_details, columns=['title','Link','Price','Quantity'])
df.to_csv('Books.csv',index=False)


index=False helps us to ignore the first index column and it is not a part of csv

/////////////////////////////////////////////////////

=======================================================



Selenium:

1) With Beautiful Soup we can only extract static content. Here static content means ui activities like clicking on buttons,
submitting forms etc.

2) Sometimes, data we want to extract is hidden behind JavaScript objects, objects that need to be clicked on to 
reveal the hidden data.

Selenium helps us with this.



Selenium is basically a Web browser automation took, which simulates a user surfing the Internet.

It allows:

1) Clicking buttons
2) Entering information in forms
3) Searching for specific information on web pages



Selenium has to be installed explicitly

Installation:

1) Selenium package
2) Web Driver
3) Supported browsers are: Chrome, Firefox, Internet Explorer, Safari, Opera, PhantomJS (invisible)

If you don't want to download chrome driver, alternative method is given in CN.


Basic steps:


1) Importing library:

from selenium import webdriver


2) Initiate a web browser session:

webdriver.Chrome(executable_path)

Controls the ChromeDriver and allows you to drive the browser.

You will need to download the ChromeDriver executable from
http://chromedriver.storage.googleapis.com/index.html

We can also downlad the driver as well


So two options:  1) Download the driver   2) Don't download
Driver is just an executable file. 
We can simple store it.

Example:

driver=webdriver.Chrome(executable_path='chromedriver')

# I have stored the driver in the base directory itself


As soon we execute the webDriver.chrome() function, a browsing session is opened for us and a chrome window opens.


3) Visiting coding ninjas website


.get() method: Loads a web page in the current browsing session.
Different from the get method of requests library.



/////////////////////////////////////


driver.title

Gets the title of web page



/////////////////////////////////


driver.back() : clicks the back button of browser


driver.forward(): clicks the forward button



driver.save_screenshot(filename): takes the screen shot and save it
return True if screenshot is captured



/////////////////////////////////

driver.page_source: gets the html of the page


///////////////////////////////////



find_element_by_link_text: finds an element by link text

We can store the output in a variable.
Then we do variable.click() to actually click that button


Ex:
driver.get('https://google.com')
button=driver.find_elements_by_link_text('Images')
button[0].click()


/////////////////////////////////////////

find_element_by_partial_link_text:
This can be used when we don't know the exact text


driver.find_elements_by_partial_link_text(link_text)

Finds elements by a partial match of their link text.


//////////////////////////////////////////


What if we use this on some text which is not present:

An exception is raised.



//////////////////////////////////////////

=============================================================


Browser interaction:


So far the methods that we have seen:

1) driver.get(url)

2) driver.back()

3) driver.forward()

4) driver.title()

5) driver.page_source()


Here driver object is created.

Method 2 and 3 work on the history that is mantained by the browser.



Some more properties:

1) maximize_window(): Self explanatory name. By default window is not maximized

2) driver.current_url:  Fetches the url of current page

3) driver.refresh(): self explanatory

4) driver.get(driver.current_url):   driver.current_url fetches the current url itself. basically this code is same as driver.refresh()

5) driver.close(): close only our current session. Basically the window on which the driver is currently focussing on

6) driver.quit(): closes each and every window.


========================================================================


Locate element 1:

Methods:


1)  find_element_by_link_text(): already seen
2) find_element_by_partial_link_text(): already seen
3) find_element_by_id(): uses id to locate
4) find_element_by_class_name(): uses class to locate
5) find_element_by_name(): uses name attribute to locate
6) find_element_by_tag_name()
7) find_element_by_xpath()
8) find_element_by_css_selector()



Ex 1:  
c1=driver.find_element_by_partial_link_text('usic')
c1.click()


Ex2: 
i1=driver.find_element_by_class_name('image_container')
i1.click()






All these methods only return the first matched entry


How to get all elements satisfying instead of 1:

////////////////////////////////////////////////////////////////////////

just use  elements instead of element:


For example:


instead of 
find_elemenet_by_link_text  
use
find_elements_by_link_text


Ex: 
i2=driver.find_elements_by_class_name('image_container')


///////////////////////////////////////////////
Locate element to get all elements:


1)  find_elements_by_link_text(): already seen
2) find_elements_by_partial_link_text(): already seen
3) find_elements_by_id(): uses id to locate
4) find_elements_by_class_name(): uses class to locate
5) find_elements_by_name(): Uses name attribute to locate
6) find_elements_by_tag_name()
7) find_elements_by_xpath()
8) find_elements_by_css_selector()


====================================================================================



Checking the types of elements found by selenium:

type(driver.find_element_by_name('viewport'))


Output:
selenium.webdriver.remote.webelement.WebElement


All the methods that we discussed to get elements return this type of value.

////////////////////////////////
What are the properties and methods that can be applied over these type of data types:

If these properties are clickable, the browser clicks on it.
If its not clickable , nothing happens



Methods and properties:

click():     already seen

clear(): clear the content of our element Only works if the elment that we have selected is user editable. Otherwise we get an error.
Ex: Tetx boxes where we can get the data

get_attribute(): Find out the value of some attribute. If attribute is present, we get the value. If its absent we get None, we don't get error

is_displayed(): if some element is displayed on screen or not.
Ex: Before clicking any button we should make sure if a button is visible to us or not. If it is, then only we should perform click

is_enabled(): We can check if some value is enabled or not.

is_selected(): Radio buttons and check boxes.

send_keys(*value): With this, we can go and type data into the browser

submit(): we can submit the form 

text: Gets the text of a tag.
Ex:
btn=driver.find_element_by_tag_name('h1')
btn.text

Output:   'Music'.
Here the HTML structure was:

<h1> Music </h1>

tag_name: finds out the tag name
Example: continuing the previous example:

btn.tag_name
Output: h1


==========================================================


Find all jobs():


We will work on a website. It has many links of differnt websites.
We need to click on the links.
When we click on any link, the url doesn't change.
Over here the urls are java script urls. We can't work with these java script urls directly using beautiful soupkanview


The link provided by Coding ninjas isn't working.
Link provided: http://kanview.ks.gov/PayRates/PayRates_Agency.aspx


Lets use Books.toscrape itself.

//////////////////////////////////////////


Lets refer the video itself a website isn't accessible




With the help of ids, getting urls of all the links:

Currently only extracting for two

for i in range(2):
	diver.find_element_by_id('MainContent_uxLeve12_JobTitles_uxJobTitleBtn_'+ str(i))
	link.click()
	

	#Now to get data from here we simply use beautiful soup.
	#Beautiful soup here is being used to extract the data
	# To get html content of web page, we can use driver.page_source(). We don't need to use requests.get	

	data=BeautifulSoup(driver.page_source(),'lxml')
	emp_table=data.table  # extracting table directly via tag name
	
	#To convert data table to pandas we can simply use  pd.read_html function of pandas

	#pd.read_html(io):  Read HTML tables into a ``list`` of ``DataFrame`` objects. io can either be str, path object or file-like object
	# We will simply type cast the extracted html(tanble) to string
	data= pd.read_html(str(emp_table), header=0)
	# We pass the pareser as well. By default the parser is lxml
	# Here data is actually a list of data frames
	driver.back()

================================================================



Extracting all job titles using Exception:


We use try, catch. This is just like we have been doing so far

The find element methods throw an exception , when the element is not present on the page.


Library to be imported:
from selenium.common.exceptions import NoSuchElementException



try and except: except is just like catch in other languages



Code:
from selenium.common.exceptions import NoSuchElementException

dataFrameList=[]

i=0
while True:
	try:	
		diver.find_element_by_id('MainContent_uxLeve12_JobTitles_uxJobTitleBtn_'+ str(i))
		link.click()
	

		#Now to get data from here we simply use beautiful soup.
		#Beautiful soup here is being used to extract the data
		# To get html content of web page, we can use driver.page_source(). We don't need to use requests.get	

		data=BeautifulSoup(driver.page_source())
		emp_table=data.table  # extracting table directly via tag name
	
		#To convert data table to pandas we can simply use  pd.read_html function of pandas

		#pd.read_html(io):  Read HTML tables into a ``list`` of ``DataFrame`` objects. io can either be str, path object or file-like object
		# We will simply type cast the extracted html(tanble) to string
		data= pd.read_html(str(emp_table), header=0)
		# We pass the pareser as well. By default the parser is lxml
		# Here data is actually a list of data frames
		driver.back()
		i+=1
	except NoSuchElementException:
		break

Here, only when NoSuchElementException is caught, then only the loop will break.

This is exactly same as we have done in the previous chapter, Data Scrapping with BeautifulSoup



pd.concat(): Concatenate pandas objects along a particular axis with optional set logic
along the other axes. 
	
	
//////////////////////////////////////////////////


===========================================================

Locate element 2:


find_element_by_xpath()

find_element_by_css_selector()


//////////////////////


xpath:  xml path
html is a kind of xml
We can create a path, with which we can reach the desired nodes.



Using XPath:
Path:   Absolute, Relative

Absolute: We have to create path from scratch

Example: Using books.toscrape

Xpath for Title:   /html/head/title

driver.find_element_by_xpath('/html/head/title')


Example 2: 

breadcrumb=driver.find_element_by_xpath('/html/body/div/div/ul')
breadcrumb.get_attribute('outerHTML')


Using absolute xpath is very messy. We should ideally go for relative xpath

Relative:   
Discussed down



////////////////////////////////////////////////////////////////


driverObject.get_attribute('outer_HTML'): We get the data inside the tag, including the tag

driver_Object.get_attribute('inner_HTML'): We get the data inside the tag, excluding the tag


 //////////////////////////////////////////////////



Relative XPath:

Selecting Nodes:  

nodename:     Selects all nodes with the name  "nodename"
/ :  selects from the root node
//: selects nodes in the document from the current node that match the selection no matter where they are

@: selects attributes


Syntax:

// tag[@attribute='value']

Example:


// div[@id='navbar']   :    finds some div, with id == navbar

// div[@id='navbar']/div/li    :  it goes even further than upper path. Here we started from div with id=navbar and didn't start from root

// div[@class='index']//div:  find out any div which is present in the base div with class index

// *[@id='navbar']:   looks for all the tags with id == navbar. * means everything

// *[@id='navbar']/div/div/a:  

/bookstore/*

//* : finds everything

//title[@*]: Find everything within tile having some attribute



Example:
driver.find_element_by_xpath('//ul[@class="breadcrumb"]')
Output:

<selenium.webdriver.remote.webelement.WebElement (session="bae2c92d1384c3548b0a630f1104198c", element="1cc57246-d8d2-4cba-b63c-6d38174c2710")>



If there are multiple elements satifying this, we get the first match only.
To get all the elements, we have to use    find_elements_by_xpath

///////////////////////////////////////


/ vs //:


/: look immediately after the parent

//: look everywhere inside the parent.


/////////////////////////////////////////////


Partial match:

Sometimes , we can have multiple classes for an attribute.

There if we try to match a single element, we won't get a match.

Hence, we go for partial match


Syntax:

// tag[contains(@attribute, 'value')]:   Helpful when there are multiple classes

// tag[starts-with(@attribute,'value')]


Example:
driver.find_element_by_xpath('//div[contains(@class,"h1")]')

///////////////////////////////////


Predicates:

Allows us to get specific tags



/div/book[1]:  suppose we have many books.     We get the 1st book from here.
/div/book[last()]:  We get the last book tag
/div/book[last()-1]: We get the second last book tag]
/div/book[position()<3]: All the book tags having posn less than 3


In predicates 1 based indexing is being used
 
Examples:
driver.find_element_by_xpath('//ul[starts-with(@class, "nav")]/li/ul/li[1]')


Example 2:

driver.find_element_by_xpath('//ul[starts-with(@class, "nav")]/li/ul/li[last()]')


///////////////////////////////////////////////

Using xpath is the least prefferable method, because of low performance

Simmilar is the case with css_selector

Types of inputs to be given for css selector fun


CSS selector:

1) Absolute Path
2) Relative Path
3) Using Class
4) Using id

Absolute path:    html head title

We dont add '/' here

Relative Path:   div li



Here like we have in css, class is represented by . and id by #


Example:

driver.find_element_by_css_selector('html head title')


Example 2   Relative path:

driver.find_element_by_css_selector('div.page_inner')

Gets a div with class == page inner


Instead of space we can add '>>' as well.
/////////////////////////////////////////////////////

When to preffer CSS  and when to prefer xpath

Ideally both should be avoided.
If we have non other option, with css we can only move in forward direction
With xpath , we can move in both dirns with the help of its parent.



=============================================


Type into fields:


Steps:

1) Importing libraries

2) Locating the text box:
search_box=driver.find_element_by_name('q')


3) Sending the key:

We need to use the method:   send_keys()

search_box.send_keys('Selenium')

4) Submitting: 2 options
a) Search the submit button and click
b) If the submit button's type is 'submit', we can simply use driver.submit() method

search_box.submit()


//////////////////////////////////////////


Ques. Use google maps to get address of coding ninjas


///////////////////////////////////////////////////////


To move up and down from a particular tag use xpath or css selector


//////////////////////////////////

.get_attribute('innerHTML')  works like .string in BeautifulSoup 


==============================

Sending hotkeys:


Library to be imported:
from selenium.webdriver.common.keys import Keys


Sending hotkey in a particular element:

Suppose search_btn is the web element

search_btn.send_keys(Keys.ENTER)


==============================================================


Advanced Selenium

Details of all the flights from Delhi to Mumbai on some specific day.

Steps:
1) Visit a website. Say we visit paytm
2) Click on flights button
3) Enter details
4) Click search/submit
5) Extractn all the data

For UI synchronization, we might want to wait some time.

For ex after clicking search, some time may be required for this search as the page might not be loaded yet.

Why??

1) Website is slow
2) Multiple elemnts loading at different times.


////////////////////////////////
Waits in Selenium:

1) Implicit Wait: With this, we can tell our driver to wait certain ammount of time, before it searches for desired elemnt
For ex: don't directly look for an elemnt, rather wait 10 seconds

Code: driver.implicitly_wait(time): time in seconds for which the driver should wait

Ex:
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
driver=webdriver.Chrome(executable_path='chromedriver')
driver.get('https://dell.com/en-in')
search_btn=driver.find_element_by_id('mh-search-input')
search_btn.send_keys('Gaming laptops')
search_btn.send_keys(Keys.ENTER)

# adding wait
driver.implicitly_wait(5)

print(driver.find_element_by_css_selector('h3.ps-title a').get_attribute('innerHTML'))
driver.close()


////////////////////////////////////////
Implicit wait can slow down working of a script


Implicit wait is applied globally on  a driver object.

implicit wait remains active until driver object isn't destroyed.

meaning if we try to locate other element as well, the driver object waits 10 seconds
///////////////////////////////////

2) Explicit Wait:  

Situation where implit wait fails
a) Wait time is different for different situation. If we go for implicit wait, our code sits same ammount of time on all ops


Explicit wait sorts this out.
With explicit we specify a condn. Till this condn the webdriver waits till this condn is met before raising any exception.

Instead of saying, wait for 10 seconds, we can say wait for this element to load before performing operation or raising any exception.


In addition to condn , we also need to add an upper limit for the wait

How this works:

We explicitly specify a condn. If condn is fulfilled in 2 secs, then the operation is performed. If till upper_limit, 
condn isn't fulfilled, then exception is raised.



Code: 

Importing the libraries:

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
froms selenium.webdriver.common.by import By

Different condns:

presence_of_element_located(ui_locator)          :            if one element present on the screen or not
presence_of_all_elements_located(ui_locator)
visibility_of_element_located(ui_locator)
element_to_be_clickable(ui_locator)



Steps to work:

1) Create a wait object:

wait=WebDriver(driver,10)

# Here driver is the selenium web driver obj and 10 is the upper limit of timeout.

2) use .until method. This method will wait till a condn isn't met

3) EC.condn,   select the desired condn till which, this explicit wait is supposed to wait. 

4) (By.CSS_SELECTOR,'h3.ps-title a'):   we pass a condn. Over here the condn is , we are locating a tag by css selector and it equals teh given tag. The complete condn is passes in as a tupple


Ex:

# adding explicit wait
wait= WebDriverWait(driver, 10)
lap1=wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'h3.ps-title a')))


Over here the upper limit is 10 seconds


/////////////////////////////////////////////////////////////////////////////



Radio buttons and check boxes:

Website to be used:
https://demoqa.com/automation-practice-form

Dealing with radio buttons:

1) locate the radio button
2) find the value for which we want to make selection

Example:

driver.find_elements_by_xpath('//div[starts-with(@class,"custom-control")]/label')[2].click()



Method:

is_selected(): with this method , we can check if a particular option is selected or not.

btn.is_selected()

////////////////////////////////////////////



Check boxes:

We can select multiple values here but in radio buttons , we can only select a single value.

If we click over check two times, the value is deselected. This does not happen in radio buttons.

****************

With get attribute buuton we can access any attribute of the tag we are on and notb just inner text

Ex:
hobbies=driver.find_elements_by_xpath('//label[@class="custom-control-label"]')
for h in hobbies:
    print(h.get_attribute('for'))

Observe here , how we are using get_attribute


We can use is_selected here as well
For ex: if we want to clear our previous selection.

///////////////////////////////////////////

===================================================================


Drop down elements:

Steps:

1) Locate the element
2) Locate the option as drop down can have many options.
3) Make selection

////////////////////////////////////////////////

Locate the element: Simply do inspect element and get the desired element

Select the option: We have a special class for this called select.


First we have to import this class.

Importing :
from selenium.webdriver.support.select import Select

We can create a class by calling the constructor and in the 

Select(drp_down)


/////////////////////////////////////////////////////////////////////////////


We can have many methods and properties which we can call on this select class.

Methods:
1) options
2) first_selected_option
3) select_by_index(index)
4) select_by_value(value)
5) select_by_visible_text(text)
6) all_selected_options
7) deselect_all()
8) deselect_by_index(index)
9) deselect_by_value(value)
10) deselect_by_visible_text(text)


Example:

# Getting the drop down elementt
drp_down=  driver.get_element_by_tag_name('select')
s= Select(drp_dwn)


# using the options method

s.options


# using the first selected option
s.first_selected_option



# all selected option

s.all_selected_options
///////////////////////////////////////////////////

# making our selection

3 methods available to us:
	select-index(attribute),  value(attribute value),   visible text(text visible to use)

if index attribute is not present, by default the index starts from 0 , for second option its 1 and so on.




s.select_by_index(1)


s.select_by_select_by_visible_text('Australia')
///////////////////////////////////////////

Simmilar to select, we also have deselect options


////////////////////////////////////////////

The drop down that supports multiple selection:

for these we can call any select function many number of times and make multiple selections.

======================================================


Scroll webpage:

In beautiful soup we didn't care for scrolling.
this is because data is already present on the page. The scrolling doesn't change the data on a page.

But for some websites like Facebook.com the data changes when we scroll.


///////////////////////////////////////


To scroll a page, we need to take help of JavaScript code.
Selenium by default comes with support to execute JavaScript code.
No need to even import any library.


Method:  driver.execute_script()


/////////////////////////////////////////////////////////

In javascript, if we want to visit a website:

window.location="https://www.google.com";

We can pass this as an arguement to execute_script function

///////////////////////////////////////////////////////////////


Locating an element by javascript code:

in selenium we have, driver.get_element_by_id.
Simmilarly in JavaScript as well we have :     document.getElementById(' ');    to locate an element




Nothing will be printed by default, in JavaScript we have to return the element

Ex:
driver.execute_script('return document.getElementById("react-root");')

Here return is there to return the value obtained.



//////////////////////////////////

scrolling by JS:

In JS we have two methods for scrolling:

window.scrollBy(x,y):    Here x and y means no of pixels to move in x direction and no of pixels to move in y direction. x and y can be +ve or -ve

scrollTo(x-coord, y-coord): Scrolls to a particular set of coordinates 




****************************

Suppose we want to scroll down a page 5 times. In that case we can take the help of a loop.


we can make our code sleep for some time using  time library and its sleep function



Ex:

import time
for i in range(5):
    driver.execute_script('window.scrollBy(0,1000);')
    time.sleep(3)


****************************************************


////////////////////////////////////////////////////////////////


To move up we can pass -ve arguement in scroll by function


////////////////////////////////////////////////////////////



=========================================================


scrollInto View():   function helps in scrolling until a particular element is in fully visible


=====================================================================


Infinite Scrolling:

windows.scrollTo(xposn, yposn): move to xposn and yposn


Problem: Stop scrolling when new content stops appearing


This kind of scrolling is called infinite scrolling



/////////////////////////////////


Another property in JavaScript:

document.body.scrollHeight:  Returns the complete height of the page at a moment.


We cannot directly pass Python variables into javascript.
We can use arguements array
arguements array hold the value of parsed arguements.

Ex:
current_height=driver.execute_script('return document.body.scrollHeight')
driver.execute_script('window.scrollTo(0, arguments[0]);', current_height)


This is how we pass python varaible to javascript.

//////////////////////////////////////



Infinite scrolling:

current_height=driver.execute_script('return document.body.scrollHeight')
while True:
    print(current_height)
    driver.execute_script('window.scrollTo(0, arguments[0])', current_height)
    #print('not executed')
    time.sleep(3)
    new_height=driver.execute_script('return document.body.scrollHeight')
    if new_height==current_height:
        break
    else:
        current_height=new_height
    


Once height stops changing , we break the loop
/////////////////////////////////////////////


We can use infinite scrolling to extract all the data on a page, say get all the tweets from a particular twitter handle.

First we can scroll to bottom of the page and then extract all the data.
 
==========================================



Shift window focus:

When a new window is opened after performing some op and we want to perform our actions on the new tab.

////////////////////////////////
Open new tab using javascript:
window.open(url),   a new tab opens on the url.


Ex:
driver.execute_script('window.open("https://twitter.com")')


/////////////////////////////////////////////////

driver.get('https://codingninjas.in')

driver.title

Output:
'Coding Ninjas – Learn coding online at India’s best coding institute'

driver.execute_script('window.open("https://twitter.com")')
driver.title

Output:
'Coding Ninjas – Learn coding online at India’s best coding institute'

*****
Focus is still on previous page

//////////////////////////////////////////////////////////



To switch focus we have some methods which we can very well use:


driver.window_handles:  all the windows or tabs which are open on current driver session

driver.current_window_handle: gets the current hanle


driver.switch_to.window(handle to switch focus to) : switches to the desired window handle.


///////////////////////////////////////////////////////////

============================================================================
Handle pop ups:


Different pop ups:

1) Simple Alert
2) Confirmation Alert
3) Prompt Alert



Methods and properties:

alert_obj= driver.switch_to.alert
alert_obj.accept(): accepting the popup(clicking ok)
alert_obj.dismiss():  declining the popup(clicking cancel)
alrert.obj.send_keys() :   sending in the data
alert.text:  get the message displayed in code. property not a method



///////////////////////////////////////////////////////
To open local web page with selenium: 
do :

driver.get('file:/'+url of the file)

//////////////////////////////////////////////////

Example: working with popups:


driver.get('https://chercher.tech/practice/practice-pop-ups-selenium-webdriver')
driver.find_element_by_name('confirmation').click()
alert_obj=driver.switch_to.alert
alert_obj.accept()
===============================================
//////////////////////////////////////////////


Instagram followers scrolling:
https://stackoverflow.com/questions/49664982/how-to-press-page-down-key-in-selenium-to-scroll-down-a-webpage-through-java


/////////////////////////////////////////////////



====================================


Data Visualization?

Representing data in form of charts or grqaphs or maps.

Refers to techniques used to communicate insights from data through visual representation.

It means presenting data in a visual manner

Art of presenting data in a manner that any non technical person can understand it.

Allows data scientists to converse with their end users.



When we plot a chart, its not that we plot complete data. Rather we look to plot the implortant data.

///////////////////////////////////////////////////////////

Why we need Data Visualization??
To communicate our findings with non technical people we need data visualization

To analyze large ammounts of data we need data visualization.

The ammount of data that humans gain through vision is far greater than any other organ.

Visualizahtion can help us deal with more complex information and enhance memory.

By using visual elements like charts, graphs and maps, data visualization tools provide an accessible way to see and 
understand trends, outliersand patterns in data.



///////////////////////////////////////////////////////////

==============================================================================

Importance of data visualization:


Helps in quickly understanding the data.
Simplifies large datasets in a way that people can easily interpret.
Shows trends that may not be apparent in first glance.
Identify areas that need attenntion or improvement.
Offers new insights to your data.
Quickly identify outliers.


Example: 

Anscombe's Quartet:

Can use this link:
https://towardsdatascience.com/importance-of-data-visualization-anscombes-quartet-way-a325148b9fd2

Anscombe’s quartet comprises four data sets that have nearly identical simple 
descriptive statistics, yet have very different distributions and appear very different when graphed.


Here , only data visualizations can convey to us, the difference b/w the 4 data sets.

Hence, all the important features in the dataset must be visualised before implementing any machine learning algorithm on them which will help to make a good fit model.


=====================================================

Types of Data Visualizations:

1) Exploratory Analysis   (Find insights)  (Step 1)
2) Explanatory Analysis   (Present insights)  (Step 2)

Exploratory Analysis:			Explanatory Analysis:
1) Done during Data Analysis			1)  Done after we have found an insight
to find insights		

2) Appropriate when we have a		2) Appropriate when you already know what the data has 		 
whole bunch of data and are not sure what 	   to say and we are just trying to explain it to someone else
it is

3) Visualizations built for these purposes do not		3) Visualizations done for these purposes need to be surrounded
need to be perfect. You are simply looking for patterns	by a story that points to an insight that answers the question
 


==================================================================


Different ways of data visualization:


1) Python, R: 

a) Ability to generate complex and attractive statistical graphics in order to gain insights and explore our data.
b) Equipped to handle millions of data points.
c) Most popular languages for data analysis.


2) Excel: Offers large number of charts and graphing tool to create quick visualizations.


3) Tableau:
a) Master of data visualization tools abd huge customer base
b) A Big Data visualization tool generally used in many corporates
c) Drag and drop and very easy to use software
d) Quickly create interactive visualizations out of large data.



Power BI:
a) Visualization tool developed by Microsoft
b) Allows to connect to different data sources.


Google Charts:
a) Google Charts run on HTML 5 and SVG(Scalable Vector Chartsss)
b) Powerful, easy to use and an interactive data visualization tool for browsers and mobile devices.
c) aims at Android, iOS and total cross-browser compatibility, including older Internet Explorer versions
d) Rich gallery of charts and very easy for you to customize as per the needs.

 
QlickView:
Check out the slides


Infogram:
Check out the slides


======================================================================



Tableau:
Can help anyone see and understand the data.

Buisiness Intelligence software that allows anyone to easily connect to data, then visualize and create interactive. sharable dashboards

Users can create dashboards which depict teh trends, variations and density of data in the form of graphs and charts

The software allows data blending and real-time collaboration, whcih makes it very unique


////////////////////////////////////////


Feautures of Tableau:

1) Ease of use   (because of drag and drop)
2) No need of any technical knowledge  
3) Real-time analysis
4) Data Connection to different data sources  (we can connect with many data sources)
5) Interactive Dashboards
6) Self reliant (can be easily installed. Not dependent on other installations)



/////////////////////////////////


What is the file extension of Tableau.

.twb (tableau workbook) , .twbx   (tableau packaged workbook)  , .tds  (tableau data source)


//////////////////////////////////////////////////


==============================================================


Tableau Installation:


Tableau Product family:

Tableau Desktop
Tableau Reader
Tableau Public
Tableau Server
Tableau Online


Tableau Desktop:  Desktop version. Its paid. Doesn't work on Linux, only on Windows and MAC

Tableau Server: Used by organization. Publish dashboards with Tableau Desktop and share them throughout organization 
Its also paid


Tableau Online: hosted version of Tableau Server. Just like Tableau server, only difference is data is stored on server hosted
in the cloud which are mantained by the Tableau Group.

Tableau Reader: 
Free Desktop application and read-only tool
Enables us to open and view visualizations that are built in Tableau Desktop
We can filter, drill down data but we cannot edit or perform any kind of interactions.



Tableau Public:

Free version of Tableau to create visualizations.
But we need to save our workbook or worksheets in Tableau server which can be viewed by anyone.



We will use Tableau public in this course

https://public.tableau.com/en-us/s/download

==============================================================================

Using Tableau:

1) Connect with Data   (connect with Data Source)
2) Build Data Views  (what data we have in files that we have connected with Tableau)(collecting all the data, blending data etc)
3) Create Visualization (Worksheets)  (a visualization is called worksheets)  
4) Create Dashboard   (Collection of worksheets)(where we have connected all the sheets together)



////////////////////////////////////////////////


1) Connect with data:
Many options available. Just select the type of data we want to work on and select the file of that type

Datasets available to use in Tableau Public:

https://public.tableau.com/en-us/s/resources


Multiple sheets are treated as different tables



Data Type representation:

#: represents Numeric values
Calender: represents date
ABC: represents Text

Globe: Location


Tablau automatically picks the values.

//////////////////////////////////////////////////
We can sort the data according to some columns
We can rename our columns

We can perform split function as well on the basis of some seperator on columns


========================================================


Data Joins and unions:

Data on which we want to work, may not be present in a single location.
We may want to get data from different locations and combine them in some way.

We have 2 ways to do this:

1) Data Joins 
2) Union



Data Joins:

Data Sources have one or more columns in common that you can combine together, creating a wider table.

The tables present in data source can be related to each other using the joins such as :
1) Inner Join(default)  (intersection picked)
2) Left Join   (left set and intersection picked)
3) Right Join   (right set and intersection picked)
4) Full Outer Join   (left and right both picked )



Inner join:

Suppose the two columns on which we are performing join iare:

id			id
101			102
102			103
103			110
			109

Here 101 is not present in 2nd column. This row from first table will be discarded
By default all the columns will be taken from both the tables.

Result will be:


102
103

Tupples corresponding to these ids will be picked.

////////////////////////////////////////////////////////


Left Join:

id			id
101			102
102			103
103			110
			109

Here tuple corresponding to 101 is also picked in addition to 102 and 103
For 101 tuple, the rows that are picked from table 2 will have NULL as there was no match for this id


////////////////////////////////////////////////////////////////////////////



In Tableau

By default, in tableau relation ship is selected. We should open the table before creating join.
To open a table, Right click the table after dragging the table into the space where we are supposed to drag the table. 

 


By default inner join is created b/w two tables. We can change the type of join by clicking on the join and changing the type.

We can also define on which column we want to join.


////////////////////////////////////////////////////////



Union:

The data sources have the same columns and will be stacked on top of one another, creating a longer table.

It helps you combine the data, that has been split into little files.

Tableau will automatically append the rows into original, identifying the  

If fields have mismatched names, Tableau will create a new field, simply adding nulls to entries where no data exists 
for that field name



///////////////////////////////////////////////////


In tableau:

For union we need a base table.
On this base table we union a second table. This second table is placed at the foot of base table and in this way, union is 
performed.


Also matching of attributes is done on the names. If two attribute names don't match, even if they represent same attribute,
they will be treated as different attribute.


If two fields are not matched, we can select the two fields and right click. We will have the option of 'merge mismatched fields'



/////////////////////////////

Join vs Union:


Join:
T1   T2

Union:

T1
T2

///////////////////////////////////////////////////////


=======================================================


Tableau Navigation:

Start Page   (page that opens as soon as we open Tableau Public)
Data Source Page
Workspace Area





Start Page:
page that opens as soon as we open Tableau Public
Three sections:

1) Connect  (To connect with different sources)
2) Open Worksheets  (can directly open worksheets from here) (we can pin sheets here)
3) Discover (can access content from here)

////////////////////////////////////


Data Source Page:

1) Left Pane  (where we see the different tables present)
2) Canvas  (where we can drag tables to perform joins and unions) (relationships b/w different tables)
3) Data Grid (where data is present) (preview of data)
4) Metadata Grid (metadata for a table is present) (no of columns, their data types, data source etc)
5) Data Source Lab  (The bottom pane where we can go to different sheets, create new dashboards, new story etc)


//////////////////////////////////////

====================================================


Workspace:

Where we develop our visualizations

Menu Bar: Same as menu bar for other applications.

Analysis menu: With help of analysis we caqn do analysis like trend line , forecast etc

Map View: We can create geographical maps of our data, can create map view of our data

Format menu: Format our visualizations.

Left pannel: All the columns present in dat
Columns divided into two: Dimensions and Measures


Pages and Filters: Will discuss later


Marks card shelf: Helps us enhancing visualization by setting color, size, label, detail, path or shapes

Show me: Intelligent recommendation highlighting which graph works best for our use case.


Rows: x axis of the plot
Columns: y axis of the plot
	

=======================================================

Tableau Data Types:

Datatype reflects kind of value which is present in a column


Tableau supports 7 different data types:

Text values           (Strings)
Date Values          (supports date in many formats   like   dd-mm-yy| mm-dd-yy| dd-mm-yyyy)
Date and Time Values  (when we have data and timestamps together)
Numerical Values	(any number)
Boolean Values (relational only)	(true or false) (if any value cannot be represented in True or False , the value is NULL)
Geographic Values (used with maps)	(longitude, latitude, city name etc)
Cluster Group (used with Fluid Clusters in Data)    (1 column having multiple data types)

Tableau automatically detects the data type for each column, we can even convert column data types.

For each data type uses unique column.

////////////////////////////////////////////////////////////

For Cluster Group how Tableau deals with data type/ which data type picked by Tableau for this Cluster Group:

For Excel data source
It takes the dicision on first 1000 values. 
If majority of data is Text, it treats this column as text. If majority of data is numeric, it teats the data as numeric.

For CSV:
First 1024 rows considered.

These numbers might not be absolutely correct

////////////////////////////////////////////////////////////////////////


For Mixed Data Type, We might have a situation where the data type is picked as text but the column also contains 
numbers and texts. What is done in that case. Let's discuss.

Mixed Data Type:

Column data type taken as :   what happens to other data types
---------------------------------------------------------------------------------------
Text :        Date and Numbers are treated as text
Dates :      Text is treated as NULL. Number is treated as day in numeric order from 1/1/1990.  Example 10 treated as 10/1/1990
Numbers:   Text is treated as NULL. A date is treated as number of days since 1/1/1990
Boolean:   Text, dates and numbers are treated as NULL

//////////////////////////////////////////////////////////////////////////////////////////


We can change the data type of a column (if required). from:

1) Data Source Page   (Right click on column, select change data type option)
2) Left Pannel of Worksheet Area   (Right Click on column, select change data type option)

==================================================================


How does Tableau decides the datatype of column:

Checks the majority datatype of first 1000 rows.

=============================================================



Dimension and Measures:


	Dimension:					Measure:

1) Qualitative data that cannot be measured			Contain numeric, quantitative values that you can measure

2) Fields that cannot be aggregated. It is used for			Fields that can be aggregated or used for mathematical operations. When you use a measure field
       categorizing facts and reveal the details in your data      		Tableau applies an aggregation to that measure (by default)
 
3) Fields are used for row or column headings			Usually used for plotting or giving values to the sizes of markers. It helps you to answer your business - related questions.

4) Tableau treats any field containing qualitative, 			A field containing a numeric value is placed under the measure.
      categorical data as dimension.

5) Ex: Name, date, Category, Country, City			Ex: Discount, Profit, Sales





Date can be categorized as dimension and measure both

We can convert Dimension to measure and vice versa. In Left Pane of Worksheet, left click and for converting dimension
to measure simply click 'convert to measure'.


====================================================


Blue and Green Fields:

Blue and green fields don't represnt Dimensions and Measure respectively.
They mean something else


Blue					Green
Discrete data				Continuous data


Discrete values:  Fixed set of values. Ex no of students
Continuous: Infinite set of values. Ex weight


To remember, remember BD1 of Last Jedi. 

Fields can be converted from continuous to discrete and vice versa by right clicking a field and selecting the appropriate option
This option is not available on all teh fields, available only on those fields which Tableau feels can be converted from one form to another.



=========================================================================


Automatively Generative Fields

Tableau generates these fields as soon as we connect to a data source.



1) Measure Name and Measure Values: Used to express different measures present in our dataset.
Measure name: The names of all the measures in our data
Measure values: Stores the values for each measure. Here value for each measure might be some aggregation on the values of that measure.
We get one value for each measure. By default this aggregation is sum.


2)  Number of records: 
It is a calculated field near the bootom of the measures shelf.
It gives record count based on the dimension and measure added to the data source.

It has '=' sign in front. '=' means a  calculated field. To generate this field some calculation has been done.
Here NULL values are also considered. Basically this tells number of rows.


3) Longitude and Latitude:
Automatically added when geographical dimension fields added to the report.
If your data includes standard geographic fields like country, state, province, city or postal codes(denoted by globe icon),
Tableau will automatically generate the longitude and latitude values for center points of each geographic entity displayed in your visualization

These will not be present if geographic values are not present in data or if tableau haven't picked the desired columns as geographic values.
So to have these fields we might need some fields having globe icon signifying that theya re geographic values.



====================================================================


Tableau Visualizations:


Data set to be used: World Bank CO2


When we create a visualization in Tableau, we have to create a sheet

We have to use column and row shelfs  to create the plots.


When we put a field in row shelf we plot y axis values.
When we put a field in column shelf we are plotting x axis values



When we put discrete(blue) data  in column and row shelf, we get a table.
When we put continuous(green) in column or row shelf, we get a plot
//////////////////////////////////////////////////////////////////////

To get a table instead of plot we can make fields as discrete.
For ex when we add year to shelves it is treated as Continuous. To get years in table format, simply convert
years to Discrete


//////////////////////////////////////////


We can change the aggregation on measures by right clicking on measure , selecting the measure option and selecting the
desired aggregation.

//////////////////////////////////////////////////////////////


===========================================================================


Let's create first chart:

If we are not sure where to place a field, whether to place the field in row shelf or column shelf, we can simply double
click a field. By default Tableau places it, at the best possible location.


By default when we double click a geographical data, it is plotted on world map.

=============================================================================

Types of charts:

Show me options provides us with good selection of graphs that we can build with data.
When we hover over a particular chart, it tells which fields can build this chart.

When we hover over bar chart, the suggestion is that we should work with 0 or more dimensions, one or more measures.

/////////////////////////////////////////////////////////////////////

Reverse is also true. When we are not sure which chart will go best with a particular field. We can single click the field.
Initially all charts are greyed but once a click is performed,  the best charts become colored.

//////////////////////////////////////////////////////
Red borderline is visible on one of the charts. The red border line suggests


//////////////////////////////////////////////


We can select multiple fields in one go by pressing ctrl and select the different fields.
Then select the best chart.


////////////////////////////////////////////////


How tableau choses default charts:


Map view(World map):
Whenever we use a geographical feauture, we will be getting a map view, with latitudes and longitudes.

Any further addition of fields will refine a chart. This is true for all the graphs.


Line View/Chart:
First selecting a dimension and then a measure produces line chart.
Any subsequent additions will refine the chart.


Text table:
When we chose a dimension it gives a text table. Any further addition only refines the chart.


Bar Chart:
When we chose a measure and then dimension we get a bar chart.
Any subsequent addition refines our chart



Scatter plot:
When we add more than 1 measures we get a scatter plot.




===================================================

Adding customization:

Marks shelf: Can be used to do customizations on our charts.


We can drop fields in the marks shelf.
Based on this new field customizations will be done on the plot.



Options in marks shelf:

1) Color: To change the colors of different elements of plots.
2) Size: To change the sizr of different elements of plots.
3) Label: To add labels on the plots
4) Detail: Helps regarding different details of our visualization.
5) Tool tip: Customize what appears to us when we hover over a plot.



Adding colors to fields:
If value is continuos we get different shades of same color
If value is discrete we get different colors.

=========================================================================


Line graphs:

Best when we want to visualize trends in the data.

When we are plotting vs time, we can change if we want to plot vs Years or Months or Quarters or any other way.


To plot 2 line graphs in one chart:
Two options
1)  Click on one of the fields, make it dual axis. Sometimes the two axis migt not be same. We can click on one of the axis and select synchronize axis.
2) Drag one of the fields on one axis, when two parallel lines appear, drop it.


============================================================================


Area charts:

Extension of line charts


split vs custom split:
split: Tableau splits on its own
custom split: we can give our own seperator. Also we can decide in how many columns to split the value to.


Area Chart:
Commonly used to showcase data that depicts a time series relationship

But unlike line charts, it can visually represnt volume also

Area Charts are primarily used when the magnitude of trend is to be communicated rather than individual data values.

To showcase this magnitude, the area b/w the line segments and the axes is highlighted by filling with color.


Only difference b/w line and area chart is that in area chart we aren coloring the entire region.

The difference b/w the two lines in Area Chart is the volume.


Here in Area Chart we are interested in seeing how magnitude is changing rather tahn worrying about invidual data points


Use of Area Chart:
Suitable to:

1) get a sense of volume to your data
2) See part - to - whole relationships b/w groups
3) Analyze trend of magnitude of a quantitative data
4) Compare trend/proportion of each category

Not Suitable when:

1) There are too many categories, as this can be difficult to read
2) There are a lot of overlapping values in your data series.


==========================================================

Bar Chart:


1) Provides visual representation of data accross different categories

2) One axis of chart shows the specific categories being compared and other axis represents a numerical value scale

Bar Charts should be created when number of classes are not many


A variation of bar charts is stacked bar charts


Whn to use it??

1) You want to compare two or more values in the same category
2) You want to compare parts of a whole
3) You don' have too many groups (less than 10 works best)
4) You want to understand how multiple similar data sets relate to each other



Don't use bar chart for the following reasons:
1) There are a large number of categories available, i.e. more than 10
2) You want to visualize continuous data

====================================================================


Histogram:

Data visualization that shows the distribution of data over a continuous interval or certain time period.

Basically a combination of vertical bar chart and line chart.

The continuous variable shown on the X-axis is broken into discrete intervals and the number of data you have in that
interval determines the height of the bar.

It gives an estimate as to where values are concentrated, what the extremes are and if there are any gaps or unusual values 
throughout data set.

They are also useful for giving a rough view of the probability distribution.


///////////////////////////////////////////////////


How to change bin size:

A field Quantity (bin) is automatically created by Tableau.
We can right click on this field and select edit. Then we can change tyhe bin size from the options.

///////////////////////////////////////////////////////////


We can perform customizations as well on the fields.

Like changing colors of bars, adding labels in charts etc.

///////////////////////////////////////////////////////////////



======================================================================


Bar Chart vs Histogram


	Bar Chart						Histogram
Useful for comparing distict values			Useful for comparing distribution of continuous data
of data	


Bar height represents the actual values of		Bar height represents the frequency of items that fall in each bin
items		

Used to plot categorical variables			Used to plot numerical variables. (teh quantitative data on x axis)
(the qualitative data on the x-axis.)


The x-axis in bar chart represents a discrete		The x-axis in histogram represents a continuous variable that has been grouped into multiple bins. 
variable. Each item on the axis is independant of
other item. 

===================================================================



Seaborn:

In the world of Analytics, best way to get insights is by visualizing the data.

We have already used Matplotlib, a 2D plotting library that allows us to plot different graphs and charts

Another complimentary package that is based on this data visualization library is Seaborn, which provides a high- level 
interface to draw statistical graphs.


/////////////////////////////////////////////////////////

Features of Seaborn:

1) Python visualization library for statistical plotting. (statistical plotting: plotting used to represent statistics of distributions)
2) Is based on matplotlib(built on top of matplotlib)
3) Designed to work on NumPy and Pandas data structures.
4) Provides a high-level interface for drawing attractive an informative statistical graphics.
5) Comes equipped with preset styles and color palettes so you can create complex, aesthetically pleasing charts with few lines of code.


////////////////////////////////////////////////////

Key features:

1) Statistical plotting library
2) Beautiful default styles
3) Designed to work very well with Pandas dataframe objects.


=====================================================

Seaborn vs Matplotlib:

Seaborn is built on top of matplotlib, its a complement to matplotlib not a replacement.

In most cases, we'll still use matplotlib for simple plotting

If matplotlib 'tries to make easy things easy and possible', seaborn tries to make well-defined set of hard things easy too.


===============================================

Seaborn resolves two major problems faced by Matplotlib, the problems are:

1) Default Matplotlib parameters
2) Working with dataframes



=================================================================


Seaborn Library:

Importing the library:

import seaborn as sns


Functions:

sns.set(): Helps us to load the default color palette and theme from the seaborn library.

plt.show(): Shows/ Displays the graph. This is same as matplotlib

/////////////////////////////////


Datasets:

load_dataset(): can use this function to get already present datasets in seaborn. This returns a dataframe.

get_dataset_names(): Return datasets already present in seaborn library.


Example:
sns.get_dataset_names()

df= sns.load_dataset('iris')


//////////////////////////////////////////////////////

Data Visualization using Seaborn:

1) Visualizing statistical relationships

2) Plotting categorical data

3) Distribution of dataset

4) Visualizing Linear Relationships.

////////////////////////////////////////////////////////


======================================


Visualizing Statistical Relationships:

The process of understanding relationships between variables of a dataset and how the relationships, in turn,
depend on other variables is known as statistical analysis.



replplot():
This is a figure-level function that makes use of two other axes functions for Visualizing Statistical Relationships
which are:

Scatterplot()
Lineplot()

By default, it plots scatterplot()



Scatterplot:

Scatter plot: 

1) A scatterplot is the most common example of visualizing relationships b/w two variables.
2) It depicts the joint distribution of two variables using a cloud of points, where each point represents an observation in the dataset.
3) This depiction allows the eye to infer a substantial amount of information about whether there is any meaningful relationship b/w them


sns.relplot(x,y, data)


In seaborn we can simply pass x and y as column names, only restriction will be that we have to give the dataframe 
in the data parameter.

Example: 
sns.set()
sns.relplot(x='total_bill', y='tip', data=df)
plt.show()


////////////////////////
kind paramter: We can change the type of graph that we want to plot, only two kinds are possiible


Example:
sns.relplot(x='total_bill', y='tip', data=df, kind='line')
plt.show()


/////////////////////////////////

hue parameter: 
Grouping variable that will produce elements with different colors.

Example:
sns.relplot(x='total_bill', y='tip', data=df, hue='sex')
plt.show()
//////////////////////////

style paramter:


Grouping variable that will produce elements with different styles.


///////////////////////////////


hue and style can be used together as well.


/////////////////////////////

row, col paramters:

Variables that define subsets to plot on different facets.  

Example:

sns.relplot(x='total_bill', y='tip', data=df, style='time', col='smoker')
plt.show()


Here we will get two different plot, one for smoker and one for non smoker.
Here smoker is categorical and there are  two categories in smoker, that's why we get two plots

=================================================

scatterplot as a function is also available in seaborn.



============================================================


Line plot:
line plot is the second type of plot that can be plotted by relplot.


1) Suitable when we want to understand change in one variable as a function of time or some other continuous variable.
2) Depicts the relationship b/w the continuous as well as categorical values in a continuous data point format.


Two ways to plot lineplot:

1) Use relplot
2) Use lineplot function directly

Example1: 

sns.relplot(x='year', y='passengers', data=df, kind='line')
plt.show()

//////////////////////////////////////////////////////////

Using different parameters

sns.relplot(x='year', y='passengers', data=df, kind='line',hue='month')
plt.show()


================================================


Plotting with categorical data:


Visualizing Categorical Data:

Mathematical ops can't be performed on categorical data. 	Example sex of a person.

Categorical Relationship:

Now we'll see the relatioship b/w two variables of which one would be categorical.
There are several ways to visualize a relationship involvng categorical data in seaborn.
catplot(): gives unified approach to plot different kind of categorical plots in seaborn.





Categorical scatterplots:

stripplot() (with kind='strip', the default)
swarmplot() (with kind='swarm')

Categorical distribution plots:
boxplot() (with kind = 'box')
violinplot() (with kind='violin')
boxenplot() (with kind='boxen')

Categorical estimate plots:
pointplot() (with kind='point')
barplot() (with kind='bar')
countplot() (with kind='count')

========================================================



Categorical Scatter Plot:


catplot() fn used to analyze the relationship b/w numeric value and categorical group of values together.


Synatax:

seaborn.catplot(x=value, y=value, data=data)

Categorical data is represnted in the x-axis and values correspond to them represented throuh y-axis(standard notation). We can reverse it.

The default representation of data in catplot() uses scatterplot (stripplot)


/////////////////////////////////////

stripplot():

Scatter plot where one of the variables is categorical.

And for each category value, we see a scatter plot with respect to numeric column.

Paramters:

1) The first parameter is the categorical column, the second parameter in the numeric column, while third parameter is dataset

2) The jitter parameter controls the magnitude of jitter or disables it altogether. Jitter is the deviation from the true value.

 Same customizations exist here as well.

Ex:
sns.catplot(x='sex', y='total_bill',data=df)
plt.show()

////////////////////////////////////////

jitter paramter: randomness
by default jitter is 1.

Ex:
sns.catplot(x='sex', y='total_bill',data=df, jitter=0.23)
plt.show()

We jitter we can tweak how randomly the scatter points are there.

////////////////////////////

swarmplot():  simmilar to stripplot, only difference is that it doesn't allow overlapping of markers.

It adjusts the points along the categorical axis using an algorithm that prevents them from overlapping

It can give a better reprentation of the distribution of observations.

Although it only works well for relatively small datasets. Because sometimes it takes a lot of computation to arrange large datasets such that they are not overlapping.

This plot uses a hidden algo to plot such taht no two values are overlapping unlike strip plot where values can overlap.


////////////////////////////////////


For visualization, swarm plot works the best but for large data it is not easy to rearrange the data so that they don't overlap.
In that case we have no option but to go for strip plot.


//////////////////////////////////////////////////////////////


==================================================


Categorical Distribution Plots:


As the size of dataset grows, categorical scatter plots become limited in the information they can provide about
the distribution of values within each category.

In such cases we can use several approaches for summarizing the distributional information in ways that facilitate easy 
comparisons accross category levels.

We use the distribution plots to get  a sense for how variables are distrinbuted.


//////////////////////////////////////////////////////////////
Boxplot:


	   Q1   Q2 Q3
                         ____		
min |--------|     |     |--------|max    ........  Outliers
                      | __|__|
           whisker               whisker
	        IQR(Inter quartile range)


Q1:  25th percentile
Q2: Median
Q3: 75th percentile

IQR: covers the 25th percentile to 75th percentile.

min and max are easy to understand.

Before min and after max are outliers. 

Inter Quartile range


==============================================

also named as box and whisker plot

Boxplots are used to visualize distributions. This is very useful when we want to compare data b/w 2 groups.

Boxplot is the visual representation of the depicting groups of numerical data through their quartiles.
Boxplot is also used to detect the outliers in the dataset.


/////////////////////////////////////

Boxen plot:

Very simmilar to box plot, except for the fact that it plots different quartile values.

By plotting different quartile values we are able to understand the shape of the distribution particularly in the head end and tail end.

Boxen plot shows us quartiles after 75 th percentile and before 25th percentile as well.


==================================================================


Violin Plot:


Box plot is present inside the violin plot

Violin plot provides an extra visualization in the form of overlapping distribution.
This overlapping distribution is termed as Kernel Density Distribution


The Kernel Density Estimation is a mathematical process of finding an estimate probability density function of a random
variable. The estimation attempts to infer characteristics of population, based on finite data set.


determines density of data points at a particular pount



with violin plot, effectively we get a box plot with all the information and also get a density estimation for our distribution.


The density is mirrored and flipped over and resulting shape is filled in, creating resembling of box plot.

The advantage of violin plot is that it can show nuances in the distribution that aren't perceptible in boxplot.

On the other hand, boxplot more clearly shows the outliers in the data.

Violins are particularly adopted when ammount of data is huge and showing individual observations gets impossible.

=======================================================


Categorical Estimate Plots:

show an estimate of centeral tendency of the values.


Bar Plot:

barplot() operates on a full dataset and applies a function to obtain the estimate(taking the mean by default).
When there are multiple observations in each category, it uses bootstrapping to compute a confidence interval around the estimate, which
is plotted using error bars.

Learn about bootstrapping.

Example:
titanic = sns.load_dataset("titanic")
sns.catplot(x="sex", y="survived", hue="class", kind="bar", data=titanic)

bootstrapping:  it gives us confidence level as we are actually plotting means. This is true for line plot as in line plot we 
get a line and also a shape encomapassing that line. The shape gives us the confidence level.

Suppose we are working on a dataset , The columns are:

Date    Time     Count
9th         9AM       213
9th        10 AM    442
9th         11 AM   1231
10TH
10th


For 9th category
First we will get all values corresponding to 9.
Then we do random smapling with replacement and build a dataset with same size as original datasets.

After that 1000 such datasets are created and mean computed for them.

Then we sort these means and confidence levels are picked in a way that within confidence levels we can encmpass 95% of the values.

Use this link to get better understanding: https://www.youtube.com/watch?v=CMRVEKf9jWA&list=PLtPIclEQf-3cG31dxSMZ8KTcDG7zYng1j&index=10



////////////////////////////////////////////////////////////////////

Count Plot:

Special case for bar plots, when we want to show the number of observations in each category rather than computing 
a statistic for a second variable. Simmilar to histogram over a categorical rather than quantitative variable. 



/////////////////////////////////////////////////////////////////


Point plot:
Another way to visualize same info is offered by pointplot(). This function also encodes the value of estimate
with height on other axis, but rather than showing a full bar, it plots the point estimate and confidence interval.

Additionally pointplot() connects points from same hue category. This makes it easy to see how relationship is changing 
as a function of hue.

Ex:

sns.set_style('dark')
sns.catplot(x='sex', y='total_bill', data=df,kind='point', hue='smoker')


========================================================

Let's take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood) but the Seaborn API is much more convenie

We can plot histograms simmilar to the way we were doing for matplotlib.



Ex:

import numpy as np
data = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)
data = pd.DataFrame(data, columns=['x', 'y'])

for col in 'xy':
    plt.hist(data[col], alpha=0.5, density= True)
/////////////////////////////////////////////

KDE: Instead of getting bar of Histogram we can smooth it out to get KDE instead

Example:
for col in 'xy':
    sns.kdeplot(data[col], shade=True)

/////////////////////////////////////////////


We can combine histogram and KDE with distplot

sns.distplot(data['x'])
sns.distplot(data['y']);

///////////////////////////////////////

Pair plot:

Creates some plots for all the attributes depending on their data type

Example:

sns.pairplot(df, hue='sex')


//////////////////////////////////////////


Jointplot:

Plots 3 things.

First plots scatter plot b/w the two attributes. Also plots distribution for the two attributes by plotting histograms on
the same plot.

sns.jointplot(x='total_bill', y='tip', data=df)

/////////////////////////////////////////////////////

Heatmaps:

We map each value to a color so that we can easily see what is working best.

df.pivot(): reshapes the dataframe according to the given attributes.

df.pivot("month", "year", "passengers")
Here the axes will be month and year and counts will be passengers

=======================================================
Can use notes from Coding ninjas

====================================================================================

Statistics:

Applying some mathematics to get some insights into the data.

Statistics is a collection of tools and methods that we can use to derive meaningful insights from data by performing
mathematical computations on it.

===============================================================================

Data Types in Statistics:

In a dataset we can have many rows and columns. One row represents one data point and one column represents one property

Suppose Data Set is:

Roll No	Name	 Age	Course		Blazer
1	Res	12	hindi		30
2	Ves	14	English		32
3	Les	15	Science		34


Here one row is data point and one column is a property.

With datatype we mean what different type of data we can have in one column.

Data Types:

1) Numerical ( Quantitative): Measurable quantity in terms of numbers
2) Categorical (Qualitative): Some category. 
  Most times categories are represented by letters/strings but they can also be represnted by numbers as well. Here blazer size is numeric but it is categorical data.



Numeric can be of 2 TYPES:

1) Discrete: Count of some values. It is counted rather than measured. Ex: Number of students.

2) Continuous: Measured rather than counted. Can have data b/w any range. Ex: Height of a student, Weight, Temperature etc.
	Continuous can take decimal values.


Categorical can be of 2 types:

1) Nominal: Unordered list. Male/ Female. No order b/w the categories. None of the category shas more weight than other category.
 
2) Ordinal: Ordered List. Size of shirts. S/M/L/XL. There is an order b/w the categories.
To remember: Ordinal Ordered. O and O.

///////////////////////////////////////////////////////////////////////////
Ques True Statements:

Which of the following statements are true?
I. All variables can be classified as quantitative or categorical variables. 
II. Categorical variables can be continuous variables. 
III. Quantitative variables can be discrete variables

Soln: I AND III



All variables can be classified as quantitative or categorical variables. 
Discrete variables are indeed a category of quantitative variables. 
Categorical variables, however, are not numeric. Therefore, they cannot be classified as continuous variables.



/////////////////////////////////////////////////////////////////////


================================================================


Sample and Population


Problem: Suppose a company places an orser of 10k bulbs, it will only accept the order if number of defected bulbs is less 
than 1%.

Two ways to solve this:
1) Check  all the 10k bulbs. Lot of effort reqd
2)  Select a sample from these 10k bulbs, say 500 and perform a check on these 500 bulbs instead. 
	Less effort required in this scenario. For a large dataset we select a small sample from that data and perform 
our analysis on this sample instead. Then we generalize our result. This is done for many real world problems.

The 10k bulb dataset is termed as population.
The  500 bulb sample is termed as sample.


A set of all individuals relevant to a particular statistical question is called a population.

A smaller group selected from the population is called a sample.


sample is basically subset of population selected randomly.


//////////////////////////////////////////////////////


Parameter and statistic:

Parameter is descriptive measure of population and statistic is descriptive measure of sample.

Basically suppose we want to calculate mean of some property for population. We can use sampling to do so.
Mean of population will be called as paramter and Mean of Sample will be called as Statistic.

/////////////////////////////////////////////////////////////


Populaton  -----------------------> Sample     is called sampling

Sample---------------------------------> Inference is called Inference

Sampling is the process of selecting a sample from the population.

We can perform our analysis on Sample and draw inference for population


==================================================


Sampling methods:

1) Probability Sampling or Random Sampling    (Here all the data points have equal chance to get selected. That means there is no bias for any data point)
2) Non-Probability sampling   (Opposite of Probability Sampling. Each data point has some weight for getting selected for sample)

We focus on Random Sampling in this course.




==============================================

Probability sampling:

Simple Random Sampling
Stratified Sampling
Cluster Sampling
Systematic Sampling
Multi stage Sampling


//////////////////////////////////////

Simple Random Sampling:


Every unit of population has an equal chance of being selected.


In this we only need to care about the sample size.

Sample should be a very good representation of population as we will draw inferences about population on 
the basis of analysis on this sample.



******************


series.str:  Vectorized string functions for Series and Index. NAs stay NA unless handled otherwise by a particular method.
With this we can use replace on series as well


**************************

Function to be used:

If df is the population dataframe, we can use df.sample() to get a sample from our population.



df.sample(n, random_state):  n stands for number of items we want in the sample. By default n is 1.

random_state : optional. Its the seed for random number generator. If we give random_state=1 we will always get the same sample.


This selects without replacement. This means if one record is selected for us that record won't be selected again. While selecting a sample from population this record isn't considered again after being selected once.

We can make it with replacement as well by passing replace=True arguement.

///////////////////////////////////////////////////////////

Sampling Error: Difference b/w parameter and statistic. We should try to make sampling error as less as possible. 


/////////////////////////////////////////////

Code for random sampling:

data=pd.read_csv('startup_funding.csv')
ammount=data['AmountInUSD']
ammount.dropna(inplace=True)
a1=ammount.str.replace(',','')
a1=pd.to_numeric(a1)

pop_min=a1.min()
pop_max=a1.max()
pop_avg=a1.mean()

sample_size=100
population=a1
sample=population.sample(sample_size)


sample_avg=sample.mean()
sample_min=sample.min()
sample_max=sample.max()


print(pop_avg, sample_avg)
print(pop_max, sample_max)
print(pop_min, sample_min)


//////////////////////////////////////////////////////


The mean of a population is denoted by  μ
When sampling with replacement, sample size can be greater than population size


//////////////////////////////////////

=======================================================


Stratified sampling:


Derived from word 'strata' meaning group of elements having some common properties.

ex: set of students belonging to same grade.
/////////////////////////////////////////////////////

How stratified sampling works:

1) First we group the complete population in certain stratas based on some common factor.
2) While sampling we make sure that we pick elements from each and every group. 
3) While picking from each group we follow Simple Random Sampling.

////////////////////////////////////////////////////////////////////


1) Subsets of datasets or population are created based on a common factor, and samples are randomly collected from each subgroup

2) Should be used when we want some members from every group.

////////////////////////////////////////////////


Examples:

1) Music taste of students
2) Average salary
3) Average package offered


For ex: Music taste can differ on the basis of age groups. To ensure we have a good sample, we should get students from all age groups. Thats why 
we rely on Stratified sampling.


============================================================

Building stratified sample code:

stratas=['Bangalore', 'Mumbai', 'Hyderabad', 'NCR', 'Pune']

sample=pd.DataFrame()

for strata in stratas:
    temp=cities[cities['CityLocation']==strata]
    temp_sample=temp.sample(20, replace=True)
    sample=sample.append(temp_sample)


Like this first we can divide our data on the basis of stratas and then use SRS(Simple Random Sampling) on each strata


==================================================


Proportial Stratified Sampling:

In this type of sampling we dont take equal mumber of units from each strata. Rather, depending on the size of stratas
number of units are picked. That is number of units selected for a sample from a sample are proportial to strata size.


This can be achieved with this formula:

(No_of_values_ith Strata / population_size ) * Sample_Size



=============================================


Cluster Sampling:


Just like Stratified Sampling we create groups of units on some criteria.
Then randomly we select some of these groups completely whereas those not selected are left completely.



In SS we were getting data from each group wheras in CS we pick complete clusters instead

////////////////////////////////////////
Disadv: We won't get data from certain groups of poulation.


Adv: Easy to implement this algorithm. In SS we have to iterate over all the stratas and do SRS. Here we just randomly select the stratas themselves.


/////////////////////////////////////////////////
Two variations of Cluster Sampling

1) Single Stage Cluster Sampling  : Simply divide in clusters and pick some clusters completely

2) Two Stage Cluster Sampling: Divide in clusters, Select some clusters, instead of taking complete cluster, perform SRS(Simple Random Sampling) on selected cluster instead to select some units from each cluster.


===========================================

Systematic Sampling:

We don't select randomly rather we follow a system.

Adjust our data points in a sequence.
Then define an interval size say i. This interval is called sampling interval.


We select element at posn 1.
Then we pick element at posn 1+i
Then  1+2*i
and so on.

In this way we build our sample from population.


 
In essence here we pick elements at regular interval of time.


Here an example of  sequence can be on the basis of time. 

Pros: We pick data in a systematic manner. Here we ensure that all data is considered, start to end. We manage to avoid a case where because of randomness we completely avoid certain parts of the data set.
/////////////////////////////////////


Read about multi stage sampling



=========================================================


Categories of Sampling:

				Statistics
	Descriptive					Inferential


Descriptive: All those techniques with which we can represent our data quantitatively.
Suppose we have dataset for 1000 students having their marks. To get some idea about students' performance 
we can use stats. Maybe we can use plots like scatter plot. So all these measeures with which we represent our data quantitavely comes under descriptive statistics.


Inferential: In real world situations, we have to use sampling so that we have a manageable sized dataset on which we can perform our descriptive stats /or other analysis.
Once we have done this we also want to draw inference for the complete population from this analysis on the sample. That's where inferential statistics comes into play.





population ->   sample  -> descriptive stats-> results  ------Infrentail stats-----
  |                       drawing inferences for the population		         |
   -------------------------------------------------------------------------------------------						                             --


======================================================================================


Descriptive statistics:

Representing and summarizing our data quantitatively
 
We have 4 different measures for Descreptive Statistic:

1) Measure of Central Tendancy   (Here we try to get the central point of the distribution)
2) Measure of Spread/ Dispersion   (How data is spread w.r.t central value)
3) Measure of position  ( space where a p articular data point can fall)
4)  Measure of Association (relationship b/w two attributes)

///////////////////////////////////////////////////////

========================================================================

Measures of Central Tendancy:


1) Mean or average
2) Median
3) Mode


///////////////////////////////////////////////////////

Mean:
1) mathematical average
2) takes into account each and every value in dataset
3) sum of deviataions from the mean is always zero.
4) can be used only with numerical data
5) sensitive to outliers

Data: 5,0,10,12,3

Mean=  sum(xi)/n:   30/5:   6


Mean of population:  greek symbol mu 


Mean of population:      x bar
	_	
	x



sum of deviataions from the mean is always zero:
Here: (6-5)+(6-0)+(6-10)+(6-12)+(6-3)= 0




Outliers: The data points that are very far from the majority of data points.

/////////////////////////////////////////////////////////////////


=============================================================================


Median:  
1) Exact middlepoint of sorted dataset
2) Can be used when data is having outliers.
3) It is also used with numerical data only


data= 12, 0, 9, 5, 4, 3 

sorted data:   0,3,4,5,9,12  :   median will be avg of 4 and 5. This is because we have even no. of datapoints and when this
is the case we consider the two middle elements. If we had odd number of data points we would have considered only the middle point.

Here median :   (4+5)/2 =  4.5

sorting the data in increasing or decreasing order doesn't matter.


==========================================================================


Mode:

1) most frequently occuring value or category in data set.
2) Normally, the mode is used for categorical data where we wish to know which is the most common category.
3) A data can have one or more than one mode.
4) Mode can be used with both numerical and categorical data.



==================================================================================


Measures of Centeral Tendancy 2:


		Mean		    Median 			Mode

Type of		only numerical	only numerical			both numerical and categorical
data


when		data is symetric	data is skewed or having		data is categorical
to use		and no  outliers	outliers





So when to use which measure :

Mean: data is symetric and no outliers
Median: data is skewed or having outliers
Mode: data is categorical, can b


//////////////////////////////////////////////////////////////////////////


Symetric data:

Watch video Measures of Central Tendancy 2


Also called as Normal Distribution

Majority datapoints lies close to the mean. As distance from mean increases we get less and less data points.
Data distribution to the left of mean and data distribution to the right of mean is same
Here Mean, Median and Mode are all same.


Whenever we are taking data of 1000s of people like height, weight etc it usually is a normal distribution.



///////////////////////////////////////////


Skewed:


Left skewed:
We can get this curve when we plot salary vs frequency. Here salary in salary vs frequency is categorical 

The plot can be histogram, line plot or some other plot as well.
We have a long tail at the left hand side here, that's why this is called as left skewed data


Here Mean < Median < Mode



Right skewed:
We can even get this curve when we plot salary vs frequency, just depends on the sample we are plotting upon. 
Here salary in salary vs frequency is categorical 
Here we have a tail to the right.

Here Mean > Median > Mode

As this is categorical plot we need to use mode. Mode is the key here. The peak will be maximum at the mode.



Normally because of this skewness mean fails for these distributions. Noramally majority of data is to the left of Mean 
for right skewed and vice versa for left skewed.

//////////////////////////////////////////////////////////


==========================================================


Ques. Normal Distribution
For data to be normally distributed, which of the following characteristics should it have ?

To be defined as 'normal' a distribution should be symmetrical about the mean, it should meet x axis at infinity and it should be bell shaped.


=======================================================================

 Measure of Spread:

Measure of Central Tendancy can only tell us about the centre of dataset. It doesn't tell us about anything regarding the spread.
For that we rely on measures of spread.

Case 1:  5 5 10 90 95   :    Mean = 41
Case 2: 30 35 45 45 50:  Mean = 41

Both have same mean but both distributions are very different.
We don't if values are near mean or far from mean.

Thats where measure of spread comes into play.

Measure of Spread represents the amount of dispersion in dataset i.e. how spread out are the values.
How far away the data points tend to fall from other.
This type of measure only applies to ordinal and numeric data that can be ranked.




When distribution has lower variability, the values of dataset are more consistent. Values are closer to mean here. Mean itself can be used to represent data.
When variability is higher, the data points are dissimilar and extreme values become more likely.



///////////////////////////////////////////////



Types of Measures of Spreads:

1) Range
2) IQR (Inter Quartile range)
3) Mean Absolute Deviation
4) Variance
5) Standard Deviation


==================================================================================


Range and IQR:

Range:
Simplest measure of spread
Difference b/w largest and smallest value.


Case 1:  5 5 10 90 95   :    Mean = 41,    Range: 85
Case 2: 30 35 45 45 50:  Mean = 41,  Range : 20

Here clearly case 1 is more spread out than case 2
////////////////////////////////////////////////////////////////////////////

Range disadvantages:

1) Does not represent actual data distribution.

2) Very sensitive to outliers.

3) Does not consider each value of dataset.


////////////////////////////////////////////////////////////////////////


Inter Quartile Range:

1) Percentile 
2) Quartile
	Divide sorted data in quarters
3) Standard score

All of the above are Measure of Positions. Will discuss standard score later.

///////////////////////////////////////////////////////////////////////////////////////

Percentile:   80 percentile means this data point is greater than 80% of the dataset.

Median is always the 50th percentile as it breaks the data into two parts.



Quartile: Quartile divides dataset into 4 parts.  We can have 3 quartiles, Q1  , Q2 and Q3 where Q2 is also the median


0 --------  25 ------50--------75-----------100
	Q1         Q2           Q3


Q1 is the 25th percentile.
Q3 is the 75th percentile



IQR:   measure of where the majority of the values lie.  
Quartile tells us about the spread of a dataset by breaking the data set into quarters, just like the median breaks it in half.
Inter Quartile Range= 3rd quartile-1st quartile


Quartiles are useful measure of spread because they are much less affected by outliers or skewed data than equivalent measures.
This is because we simply ignore the outliers as we only worry ourselves with data b/w 25th percentile to 75th percentile.

Quartiles are often reported along with median as the best choice of measure of spread and central tendancy, when dealing with skewed and/or data with outliers.


Basically: 

Normal distributed data and no outliers:   Mean and Range can be used


Skewed and Outliers:  Median and Interquartile Range




========================================================================================



Variance and Standard Deviation:




Mean Absolute Deviation:

1) Represents the amount of variation that occurs around the mean value in data set.

2) Taking average of sum of absolute difference b/w each value of data set and mean.

Formula:

sum|xi-mu|/ n
i=1 to n


We don't want the sign effect to actually cancel the magnitudes hence we are taking absolute difference.


///////////////////////////////////////////////////////////////////////////


Variance:

variance=     sum(xi-mu)^2/N
	       i= 1 to n
variance is a numerical value used to indicate how widely individuals in a group vary. It measures dispersion around the mean.

If values in data are spread out, the variance will be a large number.

If values in data are spread closely around the mean, variance will be a smaller number.

////////////////////////////////////////////////////

Problems in using variance:

1) Gives more weight to extreme scores. (Because we end up squaring the difference). So if data has outliers it is not good to take variance as measure of spread.

2) Variance is not in the same units as values in our data set. It is measured in units squared.



=============================================================================


Standard Devaition:

Measurement of average distance b/w each qty and mean. That is how data is spread out from mean.

It is the square root of variance.

A low standard deviation value indicates the data points tend to be close to the mean, while a high value indicates that the data points are spread out over a wider range of values.


////////////////////////////////
Here  we have removed both problems with standard deviation. The unit is same as datapoints and also we have removed the squaring of difference.

Standard deviation can't be negative.

Standard deviation is also not good to work with outliers. If you look at the formula for standard deviation above, a very high or a very low value would increase standard deviation as it would be very different from the mean. Hence outliers will effect standard deviation.
=====================================================================

Range and IQR don't consider each and every value to get measure of spread.
Rest 3 consider each and every value to get measure of spread.


////////////////////////////////////////////////////////////////////////////////////////////////

===============================================================================================

Measure of position

position of a datpoint with respect to other data points in the dataset.


Percentile
Quartile
Standard Score(z-score)

Already discussed Percentile and Quartile

///////////////////////////////////////////////////////////

Standard Score(z-score)

A standard score indicates how many standard deviations an element is away from the mean.

Z-score is a measure how much the data differs from mean. 
We can determine whether is is different enough to be significant.



Calculating z score:

To calculate z-score, we take the individual value and subtract the mean and then divide this difference by standard deviation.

If z is negative than x is below average.

If z is 0, then x is equal to average

If z is positive then x is above the average.

/////////////////////////////////////////////////////////////////////////


We can use z score to compare two values in different distributions.


Ex 

Distribution 1


Marks= 1200

Average = 1000

SD= 200


Distribution 2:

Marks = 30

Average= 21

SD= 5

Here, the question is how do these 2 datapoints compare.

We can use z score to compare both.

How far both are from their distribution mean.

===========================================================================

What is Q3 for the following data set ?
{20, 40, 50, 65, 70, 75, 80, 100}


Getting 50th percentile:
67.5

Take 70, 75, 80, 100 as 2nd part and get the mean of this distribution

Mean: 77.5

This is the Q3

//////////////////////////////////////////////////////////

===============================================================

Intro to Inferential Statistics:

Descriptive statistics:

Helpls organize data and focuses on main characteristics of data
provides summary of data numerically.


Inferential Statistics:

all about describing larger picture of analysis with a limited set of data(sample) and deriving conclusions from it.

we use methods that rely on probability theory and distribution helping us to predict, in particular, the population's values based on sample data.

Inferential statistics helps us answer the following questions:
1) Making inferences about a population from a sample
2) Concluding whether a sample is significantly different from the population.

=================================================================

Why inferential statistics??

1) Used to draw inferences beyond the immediate data avaiable.
2) we use methods that rely on probability theory and distribution helping us to predict, in particular, the population's value based on sample data.


There are two main areas of inferential statistics:

1) Estimating parameters: taking a statistic from sample data(foe example sample mean) and using it to find something about a population parameter(i.e. population mean)

2) Hypothesis testing: use sample data to answer research questions. For example, finding if a new cancer drug is effective or not. Or if breakfast helps children perform better in schools.

 
/////////////////////////////////////////////

Pre-requistes for inferential statistics:

1) Descriptive statistics
2) Probability
3) probability Distributions


///////////////////////////////////////

What is the purpose of an inferential statistical test ?

To check the probability of our result applying to the wider population.

/////////////////////////////////////////////////////////////


==================================================================

Probability Distribution:

Type of data:
1) Discrete(eg rolling a dice) 
2) Continuous(eg weight of a student)

Terminologies:

1) Random variable:  Whose value is determined by the outcome of a random experiment. Random  experiment  can be something like rolling  a dice.

2) Discrete Random Variable: Whose set of assumed values is countable (arises from counting)

3) Continuous Random Variable: Whose set of assumed values is uncountable (arises from measurement)

/////////////////////////////////////////////////////////////////////

in statistics, with distribution we usually mean probability distribution

A probability distribution is a function that shows the possible values for a variable and how often they occur.

A probability distribution is a list of all the possible outcomes of a random variable along with corresponding probability values.

For eg. rolling a dice, tossing a coin, measuring weight of a student etc


Eg:
Tossing a coin

Possible outcomes:  Heads and Tails.

Heads--------> 1/2----------->0.5
Tails----------->1/2----------->0.5


This is a probability distribution function.

//////////////////////////////////////////////////////////////


Kind of variable determines the type of probability distribution:

1) Discrete probability distributions for discrete variables.
2) Probability density functions for continuous variables.

////////////////////////////////////////////////////////////

Discrete Probability Distribution:

Also known as Probability Mass Functions
For example: coin tosses, rolling a dice.
Each possible value has non-zero likelihood
The probabilities for all possible values must sum to one.


There are a variety of discrete probability distributions that you can use to model different types of data. The correct
discrete distribution depends on properties of your data.

Types of Discrete Distribution:
1) Binomial distribution   (eg tossing a coin, where we can have two outcomes.)
2) Poisson distribution (count of something)
3) Uniform Distribution 

//////////////////////////////////////////////////////

Continuous Probability Distribution:

1) Also known as Probability Density Functions
2) For example- often measurements on a scale, such as height, weight and temperature
3) Specific values in continuous distributions have zero probability because we can have infinite values even within a range.


Probability for continuous distribution are measured over ranges than single points.
Probability indicates the likelihood that a value will fall within an interval.
On a probability plot, the entire area under the curve equals 1.
The most well know continuous distribution is the Normal Distribution.



===============================================================================


Normal Distribution:

Most common and widely used distribution, approximates to wide variety of random variables

Also called 'bell curve' and 'Gaussian curve'

//////////////////////////////////////////////////////////////
Characteristics of Normal Distribution:

1) Mean= Median = Mode
2) Symetric, perfectly centred around mean
3) Area under the curve is 1. This is the property of any continuous probability distribution function
4) Entire family of normal distribution is differentiated by two parameters:
	Mean and Standard SDeviation
5) Denoted as :  N ~ (mu, sigma ^2), here N is normal distribution , mu is mean and sigma^2 is variance where sigma itself is standard deviation.



================================================================================


Standard Normal Distribution:


The standard normal distribution is special case of normal distribution:
1) when a normal random variable has a mean of zero and
2) a standard deviation of one

So if we shift the mean by mu and the standard deviation by sigma for any normal distribution we will arrive at the standard normal distribution.
We use the letter Z to denote it:

z=(X-mu)/sigma

The normal random variable of a standard normal distribution is called a standard score or a z score.



/////////////////////////////////////////////////////


Why do we need standard Normal Distribution:

1) Makes predictions and inferences much easier
2) Compare different normally distributed datasets
3) Detect normality
4) Detect outliers
5) Create confidence intervals
6) Test Hypothesis

========================================================================


Sampling Distribution:

A sampling distribution is a probability distribution of a statistic obtained through a large number of samples drawn 
from a specific population.

For example: 
Assume we repeatedly take samples of a given size from this population and calculate teh arithmetic mean for each sample.
Each sample has its own average value, and the distribution of these averages is called the 'sampling distribution of the sample mean'.




Code is written in jupyter notebook

Code:
df=sns.load_dataset('flights')
means=[]
for i in range(1000):
    sample=df.sample(30)
    avg=sum(sample['passengers'])/len(sample)
    means.append(avg)

sns.kdeplot(means, shade=True, legend=True)


Result:
The plot that we obtain is a normal distribution.
The distribution becomes more simmilar to a normal distribution as we increase the sample size for a sample


*********************
Generally a sampling distribution behaves like a normal distribution irrespective of population distribution

 
=========================================================================

A sampling distribution behaves much like a normal curve and has some interesting properties like:

1) The shape of Sampling Distribution does not reveal anything about shape of population

2) Sampling Distribution helps to estimate the population statistic, using Central Limit Theorem


///////////////////////////////////////////

xbar and Sx: mean and standard deviation of the sample
mu m and sigma m: mean and standard deviation of the sample
mu and sigma: mean and standard deviation of the population

============================================================================================


Central Limit Theorem:


Sampling distribution can be very useful in making inferences about the overall population

To find- how much sample means differ from each other, we'll use standard deviation of sampling distribution

This standard deviation is called the standard error.
Standard error(SE) = s/sqrt(n),   s is the standard deviation and n is size of sample



Central Limit Theorem:

Central Limit Theorem states (given that sample size>=30):
1) The samppling distribution of sample mean has an approximately normal distribution
2) The mean of sampling distribution equals to the population mean. This depends on sample size and number of samples drawn, as both increase the accuracy increases as well.
3) The standard deviation of sampling distribution equals the standard deviation in the population divided by the
square root of sample size (i.e. standard error)


Points to note:

1) Central Limit Theorem holds true irrespective of the type of distribution of the population.

2) Now, we have a way to estimate the population mean by just making repeated observations of samples of a fixed size.

3) Greater the sample size, lower the standard error and greater the accuracy determing the population mean from standard mean.
We already know    SE= s/sqrt(n), here as n increases SE decreases



Significance of Central Limit Theorem:

1) Analyzing data involves statistical methods like hypothesis testing and constructing confidence intervals. These methods assume that
the population is normally distributed. In case of unknown or non-normal distributions. wetreat the smapling distribution as normal 
according to the Central Limit Theorem

2) Increasing sample size decreases error.
We already know    SE= s/sqrt(n), here as n increases SE decreases


===========================================================================
Inferential Statistics: Hypothesis Testing



Hypothesis Testing:

Hypothesis: An idea, suggestion etc that we want to verify if it will work or not.


Hypotheses are always statements or assumptions about the population which we want to verify.

For eg.

1) Will I improve my if I spend 4 hrs studying daily ?
2) If breakfast helps children perform better in schools ?



idea made from limited evidence and is a starting point of further investigation.

This is where we can use the sample data to anwer the research questions.


A famous saying:

" A fact is a simple statement that everyone believes. It is innocent, unless found guilty. A hypothesis is a novel suggestion that no one wants to believe.
It is guilty, until found effective."


Formal definition:

Hypothesis Testing is an inferential procedure that uses sample data to evaluate the credibility of a hypothesis about a population.

Hypothesis test is a rule that specifies whether to accept or reject a claim about a population depending on the evidence provided by the sample of data.

Hypothesis testing is a lind of statistical inference that involves asking a question, collecting data and then examining what the data tells us about how to proceed.

========================================================================

Null and Alternate Hypothesis:


For drawing some inferences, we have to make some assumptions that lead to two terms that are used in Hypothsis Testing:

1) The null hypothesis (H0)
2) The alternate hypothesis (H1 or HA)

///////////////////////////////////////////////////////////////////////////


Null Hypothesis: In hypothesis testing, we begin by making a tentative assumption about population paramter. The tentative assumption is called null hypothesis.

A statement about the population paramter i.e. theb statement which we want to test

Can include:  =,>=,<=

Alternate Hypothesis (H1 or HA):
A  statement that directly contradicts NULL Hypothesis

Can include :  not =, > , <


So any Hypothesis having equal to sign has to be NULL Hypothesis only

////////////////////////////////////////////////////////////////////////////////////

Example:

A school claims that on average their students get at least 70% marks

Claim- Average marks >=70%                            equaL TO,  ONLY IN nNULL Hypothesis so this is NULL hypotheiss
Counterclaim - Average marks < 70%		Alternate Hypothesis

Example 2:
average numberof hours spent by phd students in their research work is more than 10 hours per daya

Claim: Average hours >10 hrs			Alternate Hypothesis
Counter claim : Average hours <=10		we have equal to here, this is NULL Hypothesis

============================================================================================


Significance Level:

denoted by alpha.
Refers to degree of significance in which we accept or reject the null-hypothesis
It is the probability of rejecting the null hypothesis, if its true. We can also say, significance level is probability of error.
Typical value of Alpha are: 0.01, 0.05, 0.1

0.01 means 1 percent
0.05 means 5 percent
0.1 means 10 percent

choice of alpha is determined by context we are operating in but 0.05 or 5% is the most commonly used.

Alpha =0.05 means, your output should be 95% confident to give similar kind of result in each sample

Based on level of significance, we make a decision to accept the NUll or Alternate Hypothesis

If we are rejecting NULL Hypothesis, we in turn are accepting the alternate Hypothesis

Lower the alpha, we are even more strict
============================================================================================

Test Statistic:

Steps so far:

1) Pick a random sample
2) Define NULL and alternate hypothsis
3) Set acceptance critical/significant level


NULL hypothesis is to be rejected or failed to be rejected(meaning accept the NULL hypothesis).
To decide on this decision we need to have a metric which eventually decided if this NULL hypothesis should be accepted or rejected


4) Find the metric:    
We have two options:

a)  Find Test statistic
b) p-value


We focus on Test Statistic

5)  Find critical value
6) Compare Test Statistic and Critical Value


//////////////////////////

Test Statistic:

A Test Statistic is calculated from sample data and used in a hypothesis test.
It is used to determine whether to reject or accept the null hypothesis.


There are 4 test statistics which we can use in hypothesis testing:

Z-test : Z-score
T-test: T-score
ANOVA: F-statistic
Chi-square test: Chi-square statistic



The calculated test statistic is compared to the respective critical statistic to decide the rejection or acceptance of null hypothesis.

We will use Z-score here

////////////////////////////////////////////////////////////

z-score:
statistical test used when:
1) Data is normally distributed
2) Sample size is large, i.e. n>=30

Expression for z-test:

z=xbar-mu/(s/sqrt(n))

xbar is sample mean , mu is Hypothesized mean ,   s/sqrt(n) is Standard Error



===========================================================================


Critical Value:
Suppose we are 
Working with a company making tyres. Company claims tyres last 36 months

1) Pick a random sample --------------->  40

2) H0: Avg lifetime= 36 months
H1:  Avg lifetime!=36 months

3) Getting average lifetime of sample
Sample average= xbar= 30 months


4) To be sure if this xbar passes or not we need a cutoff value.
We call this value as critical value.

Critical Value:
1) the critical values of a statistical test are the boundaries of the acceptance region of the test.
2) If  a test statistic on one side of critical value results in accepting teh null hypothesis, a test statistic on other side will result in rejecting the NULL hypothesis.

3) Steps for using critical value in hypothesis testing:
a) Calculate Test statistic
b) Calculate critical values based on significance level (alpha)
c) Compare test statistic with critical value.

Say critical value in our example is 32 months.
This simply means we will be rejecting anything below 32 months and accepting above 32 months.


///////////////////////////////////////////////////


We can also visualize like this:

When we have sampling distribution we get a Normal curve.
On both ends of this curve we can have a critical value. Inside these critical values, if our value falls, we accept the hypothesis


////////////////////////////////////////////////////////
Rejection region/ Critical Region:

the region of the sampling distribution where we have to reject the NULL Hypothesis
////////////////////////////////////////////////////////////////////////////


====================================================================================

Test statistic:

Depending on nature of alternate hypothesis, nature of test will be used.

Different type of tests:

1) Left tailed Test  (Critical value lies to left of mean in the sampling distribution)
2) Right Tailed Test (Critical value lies to right of mean in sampling distribution)
3) Two Tailed Test (Critical value lies on both side in sampling distribution)


////////////////////////////////////////////

Which test to use??
1) If there is less than sign in alternate hypothesis, then we use left tail test
2) If there is greater than sign in alternate hypothesis, then we use right tail test
3) If there is not equal sign in alternate hypothesis, we use two tail test



In all cases we can assume mean/statistic to be the peak of sampling distribution.


For example:



A school claims that on average their students get at least 70% marks

Claim- Average marks >=70%                           
Counterclaim - Average marks < 70%


Here peak can be at 70%

When we have < in alternate hypothesis we are intereseted only in left side as critical point falls there, that's why we end up doing left tailed test.


Simmilarly we have other cases as well.

=========================================================================

Errors in Hypthesis testing:

We have 2 claims to be tested: NULL hypothesis and alternate Hypothesis. Only one of them can be true.

Ideally we should not reject the the null hypothesis when it is true and we should reject it when it is false(or alternate hypothesis is true)

There are 2 types of errors:

1) Type 1 error
2) Type 2 error

-------------------------------------------------------------------
		H0 True		H0 - False
Reject H0    (P)       |	Error(Type 1)|	Good	
----------------------------------------------------------------
Accept H0     (N)    |	Good	      |	Error(Type 2)
(Don't reject H0) 
--------------------------------------------------------------------------

Reject H0 is taken as postive as by default we want to reject the NULL Hypotheis. That is taken as standard in statistics


Type 1 error:   False positive. Here H0 should have been accepted but we rejected it. As rejecting H0 is taken as positive in inferential stats, we term this as false positive
Here we were inside acceptance region but still rejective.
Also in this case we have a higher rejection region that's why there is a chance of this error.
Rejection region depends on alpha. Higher alpha means higher rejection region.
So Type 1 error is directly proportional to alpha

So Type 1 error can also be called alpha
///////////////////////////////////////////////////

Type 2 error: False negative. 
Can also be called beta


////////////////////////////////////////////////////
H0 true and accepted:  1-alpha

///////////////////////////////////////////////////

H0 false and rejected: 1- beta

(1-beta) is also called power of test
/////////////////////////////////////////////////



===========================================================

Relationship b/w Type 1 error and Type 2 error:

As we increase rejection region the probability of increasing Type 1  error increases but probability of Type 2 error dereases.


This is because acceptance region is decreasing and it becomes hard for us to accept wrong

This is all influenced by alpha

So we have to decide on a trade off. We have to keep in mind which type of error is moere critical for us.

========================================================================


Machine Learning

Making machines learn with past data.


AI VS ml:


AI: part of CS which aims to make machines take smart decisions.

ML: subset of AI. With time ML is nearly becoming equivaluent to AI with time as rule based learning isn't giving us good results.


Data Mining: Finding information from huge data. We an do this via data analysis, machine learning or other technologies as well.

/////////////////////////////////////////////////////////////


Machine Learning Types:

1) Supervised Learning: We have input as well output. In future if we get the input data, we want the output. Basically we have labelled data.

2) Unsupervised Learning: We only have input, output is not clearly marked. Basically we don't have labels.

3) Reinforcement Learning: For positive behaviour we get incentive, for bad behaviour we get penalty. Self driving cars fall under this itself.

//////////////////////////////////////////////////////////////////////////////

======================================================================================

Supervised Learning:


We have labels in the dataset. We have clear demarkation which is the output in the dataset.


Within supervised learning we have two types of learning:

1) Regression: Output here is continuous. For example: price of a house
2) Classification: Output here is discrete. Ex: identifying the animal with image.


========================================================================================


Steps for supervised learning:

We will have training data

It will have say n cols. n-1 cols will be features. Last column will be output/labels.

Say we m of such rows.

////////////////////////////////////////////////////////////////

Steps to be followed:

1) Find data : From where do we get our data

2)  Data Loading and Cleaning: Data can come from multiple sources. We will have to do lot of cleaning. This can be
something like string manipulation, handling missing values etc.
We can add new column here, we can drop some columns as well

3) Pick and Train the Algorithm

4) Test the algorithm

////////////////////////////////////////////////////////////////

The idea is not to perfrom well on the past data, rather we want to perfrom on the new data.
Hence once we have done training on the training data, we don't give training data again.

/////////////////////////////////////////////////////////////


Suppose we have 100 data points.

We can keep some portion , say 20 datapoints on which we will train our data.

Now we have 80 datapoints.
We can further divide these into 56 datapoints and 24 datapoints .
56 can be used to train and with the help of remaining 24, we can optimize our model.


========================================================

Loading Dataset

Loading Boston Dataset


already present in scikit/sklearn 

It already has inbuilt algorithms




Code:

from sklearn import datasets
boston=datasets.load_boston()


The type of this data is 

type(boston)
sklearn.utils.Bunch

///////////////////////////////

This is still not in the format that we can unbndersta

This looks like a dictionary.

It has data and targets stored seperately.


///////////////////////////////////


X=boston['data']
Y=boston['target']

Both X and Y are numpy arrays

////////////////////////////////////////

Converting to dataframes

df=pd.DataFrame(X)
df.columns=boston['feature_names']   # Converting the column names to correct names

///////////////////////////////////////////////////

=========================================================

Training an algorithm:

We will use Linear Regresion as algo. Will learn it at later point.

Step 1:  Spilt data into two parts, for training and testing.  We can split data ourselves as well. First
	80 % data in training and rest 20% data in testing. This is not a good idea.
	The data can be chronologicaly ordered and the this data can have some trends.

	Hence we normally want to split our data randomly so that we don't ahve any bias in our training or testing dataset.

	sklearn provides us that function

	Code/ Function:   from sklearn import model_selection
		               X_train, X_test,Y_train,Y_test=model_selection.train_test_split(X,Y)	


In input for model_selection.train_test_split(), we have to give numpy arrays
We get 4 arrays as output from train_test_split function
Each row in the X and Y numpy arrays either go to train dataset or test dataset.

Step 2: Training our model


	from sklearn.linear_model import LinearRegression
	alg1=LinearRegression()   # Creating algorithm object
	alg1.fit(X_train,Y_train)    # Fitting our algo on training data
	alg1.predict(X_test)        # Predicting the results
	

Step3: Comparing predicted and actual values:	
	import matplotlib.pyplot as plt
	plt.scatter(y_pred, Y_test)
	plt.xlabel('Prediction')
	plt.ylabel('True Values')
	plt.show()
	


====================================================================================


Regression:

A process by which we estimate the value of dependent variable on the basis of one or more independent variables is called.

=======================================================================================
Linear Regression:

Given an input, we want algo to start predicting some output

Xtrain, Ytrain -------> Function ------------>Y


Function--> Hypothesis

This Hypothesis fits the data.

Linear Regression puts a linear function aout there


y= m1x1+m2x2+m3x3+b

We are dependant on each feature linearly.

To get to this function we need m1,m2,m3 and b.

We need to learn these paramters.


When we get training data, we need to figure out a way to find out m1,m2,m3 and b.

If we have these values , when we have a testing data we simply plug the values of x1,x2, x3 and get the output.

//////////////////////////////////////////////////////////////////


Linear Regression - > is assuming Linear relation b/w x and y


For now lets work with single x and single y


Over here we want to find the best fitting line. We want to get the best fit line, we want to minimize the error

In two dimension eqn of line is y=mx+c.

Simmilarly if we have n features we have find n+1 values as we have a constant as well which is our intercept

Once we have trained the model that is gotten the value of m and c, to get the predictions we simply place the input in x and get the prediction from the function.


/////////////////////////////////////////////////////////////

If the value of any regression coefficient is zero, then two variables are:

Independant.

Reason:
The independant variable doesn't have any impact on the dependant variable.

/////////////////////////////////////////////////////////////////////

========================================================================


Analysis of LR using Dummy data:



**********
np.load_txt(file path, delimiter): fn that loads data from text file.

path is the file path
delimeter is by default taken as white space. When operating with white space the delimiter will be ','



///////////////////////////////////////////////////////////////


In algo.fit(), we have to give 2d array, 1d arrays results in error


If we have 1d arrays we might have to use reshape function of numpy arrays

//////////////////////////////////////////////////////////////////////
ex: suppose x is a 1d numpy array

we can do  x.reshape(-1,1) ,    -1 signifying keep the rows same as before

////////////////////////////////////////////////////////////////////////

To access intercept and coeffecient of the model:


algo.fit(x_train,y_train)

algo.coef_

algo.coef_  returns us the array of coeffecients

/////////////////////////////////////////////////////////////


Plotting:

We want to plot the created line alongside the scatter of X_train and y_train

To plot straight line, we already have m and c.
We just need to get dummy values of x and correspondingly we can calculate y values by plugging dummy values into mx+c
To get dummy values we can use np.arange or np.linspace. Then correspondingly we can calculate y values
To plot we can use plt.plot as it by default plots line plot.


plotting mx+c line:

m=algo.coef_
c=algo.intercept_

x_line=np.arange(30,75,0.1)
y_line=m*x_line+c

plt.plot(x_line,y_line.reshape(x_line.shape[0]))
train_1d=x_train.reshape(75)
plt.scatter(x_train,y_train,c='y')

////////////////////////////////////////////////////////////////////////////////

We can plot testing data as well


=============================================

Coeffecient of determination:


So far we were plotting and seeing how good our model is performing.
That won't work always.
Suppose we have n features. It won't be possible for us to actually plot n features.

We need objective way to find out how good our algo is performing

We need a score which lets us know how close or how far is our prediction from our actual values.

/////////////////////////////////////////////////////


Suppose actual values:  2,3,4 
Predicted values: 1.8, 2.5, 3.5

We use coeffecient of determination to score our predictions as compared to actual values.

Coeffecient of determination:   1- u/v

u= sum(yiT-yiP)**2

yiT: True value of ith data point
yiP: Predicted value of ith data point

v=sum(yiT-yTmean)**2



We want the score to be b/w 0 and 1.
The higher the error, the better the result.



/////////////////////////////////////////

Explanation of Coeffecient of determination:

We compare how far the prediction is from true value compared to the easiest prediction. Easiest prediction will be assuming mean to 
be the predicted value for all data points.  We get the ratio, if the ratio is nearly 0 it means we are very close to right prediction and the final result comes out to be close to 1..

On the other hand if for all values prediction is the easiest value, that is mean ,the ratio comes out to be 1 and final result turns out to be 0 



//////////////////////////////////////////////////



We don't need to calculate this, a function already exists to do just this


algo.score(x, yT)

The function predicts the values and compares with yTrue

///////////////////////////////////////////////////////////

train_test_split does random split. 
If we run the code again, coeffecient of determination can change. Its not fixed.
Even m and c are not fixed.

////////////////////////////////////////////////////////////

=======================================================

Cost function:

We will discuss what is the mathematical model and how do we find best fit line in Linear Regression


To define which is the best fitting line, we use an error function.

For some y=mx+c, we want to find the error. Then we want to minimize that error.

/////////////////////////////////////////////////////////

For a single point,   say xi,yi


xi: input vector:  [x1 x2 x3.........xn]  for n dimensions.
Line that we will predict:  m1xi1+ m2xi2+m3xi3+...................................mnxin+c

Error for this point:

yi-(m1xi1+m2xi2+..............+c)

Total error for all pts:

sum(yi-(m1xi1+m2xi2+..............+c))
i=1 to n

/////////////////////////////////////////////


There is a small problem with this:

Errors b/w two different points can have different signs. That can result in cancellation of errors. Net error will become less than desired.

We can instead add mod to avoid taking sign.

sum(|yi-(m1xi1+m2xi2+..............+c)|)
i=1 to n
 
/////////////////////////////////////////////////


Another problem:

We want the penalty to be severe when the line moves away from points.
Case1:

p1
|
------------------------------
	|
	|
	|
	p2


Case2:

p1
|
|
------------------------------------------------------------
	|	
	|
	p2


Both cases will result in same error.

But we want error for case 1 to be larger because  line 2 seems better option.

For this reason we instead chose to square each individual error

sum((yi-(m1xi1+m2xi2+..............+c))**2)
i=1 to n


For now we will focus on 1D for now.


////////////////////////////////////////////////

Cost fn:    sum((yi-(mxi+c))**2)
	i=1 to n


We have to find the line for which cost is minimized.
////////////////////////////////////////////////////////////////
To minimize a fn we differentiate a fn

cost(m,c)=   sum((yi-(mxi+c))**2)
	   i=1 to n

We can take partial derivative:


dcost/dm=0...........................................1

dcost/dc=0...............................................2


With these two eqn we can get the minima points. At minima the slope becomes 0
Now we have two variables and we have two equations.

cost=sum((yi-(mxi+c))**2)
	   i=1 to n

summation doesn't matter in differentiation

dcost/dm=  sum(d/dm(yi-(mxi+c))**2)

First we differentiate outer expression and then we move to inside expression


dcost/dm=  sum(2(yi-(mxi+c)) * d/dm(yi-(mxi+c)))


dcost/dm=  sum(2(yi-(mxi+c))(-xi)=0               yi is independant w.r.t m and c is also independant w.r.t to m


sum(xi*yi)-m*sum(xi**2)-c*sum(xi)=0

Divide all by n, effectively on diving by n we end up with means.

  
sum(xi*yi)/N-m*sum(xi**2)/N-c*sum(xi)/N=0


sum(xi)/N:   x.mean()                 Assuming x to be 1d array

sum(xi**2)/N:  (x*x).mean()        Element wise multiplication in x and then mean calculated


sum(xi*yi)/N:       (x*y).mean()              Element wise multiplication b/w x and y


(x*y).mean()-m*((x*x).mean())-c*x.mean()....................Eqn 1

////////////////////////////////////////////////////////////

If we see cost fn is quadratic. Its plot is parabola. That's why we get a minima/maxima on doing the derivative.

Regardless of the format, the graph of a quadratic function is a parabola. 
The graph of any quadratic equation is always a parabola.
=============================================================================



Mean Square Error:

(1/N)* sum(yi-(mxi+b)**2)
           i=1 to n


N is total no of observations
1/N*sum()  is the mean 
  i=1 to N

yi us the actual value of an observation and mxi+b is our prediction
====================================================================================



Optimal Coeffecients:


Doing the same analysis on dcost/dc=0 we get another eqn

dcost/dc=0

dcost/dc=d/dc*sum(d/dm(yi-(mxi+c))**2)

dcost/dc= sum(2(yi-(mxi+c)) * d/dc(yi-(mxi+c)))

dcost/dc= sum(2(yi-(mxi+c)) * -1=0


sum(c)=sum(yi)-m*sum(xi)



n*c=sum(yi)-m*sum(xi)               (   sum(c)   ==   n*c because we are summing a constant for n times)

Diving by n on both sides

c=y.mean()-m*x.mean().......................................Eqn 2





Eqn 1 was:


(x*y).mean()-m*((x*x).mean())-c*x.mean()....................Eqn 1


Putting c in Eqn 1:


((x*y).mean()-m(x**2).mean() - x.mean()*y.mean()+m*x.mean()*x.mean()=0

(x*y).mean() - x.mean()*y.mean()  /    x**2.mean() -x.mean() * x.mean() =m



So 
m= (x*y).mean() - x.mean()*y.mean()  /    x**2.mean() -x.mean() * x.mean()

In the same way ,  we can get value for c as well

c=y.mean()-m*x.mean()


Replace m's value here. No need to completely form a new formula for c, simply replace m's value here.


These m and c give us the best fit line.
///////////////////////////////////////////////////////////////////////


============================================================


Coding Simple Linear Regression:


def fit(X,Y):

Calculate m and c using previous eqns and return m and c



def predict(X, m , c):

Predict the values, return y predicted using mx+c



def coeff_Determintaion (y_predicted, y_test):

1- u/v



def cost(X,Y,m,c):

sum (yi - (mxi + c))**2

=================================================================

Coding Simple Linear Regression 2

************

test_size paramter in train_test_split function: We can give our own paramter to be treated as testing size. Range is b/w 0 and 1

Ex: model_selection.train_test_split(x,y, test_size=0.3)

Here 30 % data goes in training.

******************


score and cost can be calculated on training data as well as test data. Its our choice.

*************

Most of the times when we return cost, we most often than not divide complete cost with N

So cost=(1/N) * (sum((yi-(mxi+c))**2))
	             i=1 to n

This returns us average cost per datapoint and is a better indicator of cost


///////////////////////////////////////////////////////////////////////////////


Code for Simple Linear Regression:

 
def fit(x_train,y_train):
    numerator= (x_train*y_train).mean() - (x_train.mean()*y_train.mean())
    denominator=(x_train**2).mean()-x_train.mean()**2
    m=numerator/denominator
    c=y_train.mean()-m*x_train.mean()
    return(m,c)

def predict(X,m,c):
    return(m*X+c)

def score(y_truth, y_pred):
    #print(y_truth.shape, y_pred.shape)
    u=((y_truth-y_pred)**2).sum()
    v=((y_truth.mean()-y_truth)**2).sum()
    return(1-u/v)

def cost(x,y,m,c):
    return((1/y.shape[0])*((y-(m*x+c))**2).sum())


m,c=fit(X_train,Y_train)
Y_pred=predict(X_test,m,c)
test_score=score(Y_test, Y_pred)
cost1=cost(X_train,Y_train,m,c)
print(m,c,test_score,cost1)


///////////////////////////////////////////////////////////////////////////////


In all the functions, we are working with numpy arrays ,
we didn't have to run for loops, we could just use vectorization and simply operate on numy arrays in one statement

/////////////////////////////////////////////////////////////////////////////////

np.savetxt:  Used to save data in a text(csv) file. 

np.genfromtxt: Load data from a txt(csv) file. It automatically handles missing values whereas load_txt doesn't handle missing values.
load_txt should be used when no data is missing.


=================================================================================

Multivariable Regression and Gradient Descent:


Problem:

Linear Regression tries to fit a linear line on the dataset but the data might not be linear.

Suppose we have two attributes x and y in our dataset. x is independent wheras y is dependant. We want to find
a parabola as complex boundart with linear regression.
We might think that Linear Regression will come short here considering Linear Regression can only plot a linear line.

What can happen instead is:

Suppose dataset is: 
x   y
1   6
2   9 
3   13



We can add a dummy feature   x_x which is basically x**2

x   x_x  y
1     1     6
2     4     9
3     9     13


The eqn for this becomes:   m1*x+m2*x_x+c


When we find m1,m2 and c we get a plane instead of line. When we plot this plane on 2d and the plane takes shape of parabola

This is because we added a dependant attribute which is square of x.

New eqn actually is: 

y=m1*x+m2*(x**2)+c

This resembles a parabola.



Simmilarly we can add dummy columns to get even more complex functions to fit our data.

Linear Regression finds a line in those dimensions, but in a way turning into a very complex function.

In this way, we can increase the power of Linear Regression many folds.
////////////////////////////////////////////////////////////////////////////////
Instead of just squaring or cubing, we can multiply two columns.
For example:

We can multiply x1 and x2 to create x1x2 column, can multiply x2 and x3 to create x2x3 column and so on.

We can take logs of column to be fed into Linear Regression.
Ex:  log(x1)  ,  log(x2)   and so on.

/////////////////////////////////////////////////////////////////////////////////////////


Suppose we have n cols , how many unique cols can we add by multuplying 2 cols together??

Soln: nC2

Basically in this way we can add O(n^2) cols

////////////////////////////////////////////////////////////////////////////



With above mentioned solution we can very well fit our data. But that can also be a bad thing.

We can remember our dataset completely, but it may perform very bad on testing dataset.
This means its effectively resulting in overfitting.



Overfitting: When our dataset performs very well on training dataset but performs very bad on testimg dataset.




/////////////////////////////////////////////////////////


Code: Will work on the boston dataset again, but we'll add another feature to see if we can improve the results.

Age looks important so we will add age as well


*******  To convert the content of dataframe to numpy array we  can use  dataframe.values  
It returns us the numpy array of the values.



 Also we may have to fix the split, to compare the two different datasets and eventually models.

We can pass a paramter called random state. How train_test_split works is, it picks a random number and corresponding to that random number sends that datapoint to training or testing dataset.
We fix random_state to a particualr number and this effectively ensures that same split is generated every time.



Code:
data_df['age_age']=data_df['AGE']*data_df['AGE']
X2=data_df.values 

type(X2)
X2_train, X2_test, Y2_train, Y2_test=model_selection.train_test_split(X2,Y, random_state=0)
algo2=LinearRegression()
algo2.fit(X2_train,Y2_train)

algo2.score(X2_test,Y2_test)
////////////////////////////////////////////////////////////////////////

Result:

The testing score improves, with adding one feature.

////////////////////////////////////////////////////////////////////////// 
What if we add more features like adding all 2 degree polynomial feautures.

With 2 degree polynomial we mean square of all the feautures and also product of all the feautures taken 2 at a time.
Basically say we have feautures x1 and x2

Square of features:  x1**2,    x2**2

Product of feautures taken 2 at a time:  x1*x2

Result:

Here training score will improve but testing score will not. Effectively we will suffer from overfitting.


Training score has risen up to 0.95 but testing score decreased to 0.60

//////////////////////////////////////////////////////////////////////

==============================================================================

Complexity analysis of Normal Equation:

y=mx+c


We have already seen how to get best fit line for 1 feauture case. 
We need to simply differentiate the eqn w.r.t m and c and put the two eqns to 0.

We can solve this 2 and 3 feautures as well.


But what about generic case??

y=m1x1+m2x2+m3x3+........................................................mnxn+c

To generalize this lets replace c with m(n+1)x(n+1)




All x(n+1) are 1, m(n+1) is equal to c

So:

y=m1x1+m2x2+m3x3+........................................................mnxn+m(n+1)x(n+1)

where x(n+1) are 1, m(n+1) is equal to c


Also assume no of samples is equal to M

We create a matrix X, of dimensions M x (n+1)  where last column is equal to 1.

Y=  M x 1 matrix 

For each row of X matrix we have a vector corresponding to all the feauture values.

So basically we have

x11	x12	x13	...............      1
x21	x22	x22       ........................1

.
.
.
.
xM1........................................................1


Here x11 means first parameter for first datapoint


To find Ypred we multiply  matrix X with 

m1
m2
m3
.
.
.
.
.
m(n+1)


This array created above is coeffecients array.

To get the coefficient array, we need to get the coefficients which result in least cost function.

We don't differentiate, we can use a formula:



coeff:    ((XtX)^-1)*(Xt)*Y

Xt is X transpose

////////////////////////////////////////////////////////////////////


Dimension of X:  m*(n+1)
Dimenison of Xt: n*(m+1)
Y: Dimension of Y is  m*1

///////////////////////////////////////////////////////////////////////////////

We can use this eqn to get the best coeffecients for any value of n.

/////////////////////////////////////////////////////////////////////////////


There is a little problem with this method:

Time to get Xt:  O(n*n), we will have to see all the elements

Multiplication of matrices: 
The  two matrices are:
(n+1 *m) , (m * n+1)  :       

Time complexity:   n+1 * m * n+1 :  O(m*(n^2))

Then we have to do further matrix multiplications

This has very huge Time Complexity.

Hence we don't prefer this method, we go for another method. We may not get exact value but we can get very close with the method that is to follow.
////////////////////////////////////////////////////////////////////////////////


================================================================================


Gradient Descent

Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).




For now lets work with 1 feauture

y=mx+c


We have to get to a line for which cost function is minimum

cost= sum((yi-(mxi+c))**2)

The cost fn  is a parabola, its a quadratic eqn.

We can plot cost with m, and we get a parabola.

We plot w.r.t m because m is the variable taht will change with each epoch.


We have to find the minima of this parabola.

///////////////////////////////////////////////////////////

what Gradient Descent does:

We start with some random value of m and c, we get some random value of cost.

Now we have to move to a better cost.


If we look to the left side of minima will have negative slope
Right side of minima will have positive slope
At the minima, slope will be 0

Suppose we have m. We want to move towards minima.
We should subtract slope from m

m'=m-slope. If we are to the right of minima, slope is +ve and we in turn move left. If we are to teh left of minima, slope is -ve and we in turn ove right.


Another factor to consider is slope can be too great, we can switch sides when moving either right or left and end up left or right of minima respectively.
So we should add a factor which restricts how fast we move towards the minima



m'=m-alpha*slope

alpha is called learning factor.

We keep repeating  this updation of m and c on the basis of slope till we have achieved a significantly less cost. Or we can repeat this for fixed number of times.



So basically we change m and c in the following ways:

m'=m-alpha*slope

slope is the slope of cost vs m. Basically  dcost/dm, taking the derivative w.r.t m


c'=c-alpha*slope

slope is the slope of cost vs c. Basically dcost/dc, taking the derivative w.r.t c


//////////////////////////////////////////////////////////////////////////////////

cost= 1/N  *  sum((yi-(mxi+c))**2)                   Over here we have taken the average cost per data point, also called mean square error

dcost/dm=   1/N * sum (  2*(yi-mxi-c)*(-xi) ) :   -2/N  * sum((yi-mxi-c)(xi))

dcost/dc=  1/N * sum( 2*(yi-mxi-c)*(-1) ):   -2/N *sum ((yi-mxi-c)(1))


So how gradient descent works:

We start at a random point.  Calculate slopes w.r.t m and w.r.t  c. Then we update our m and c accordingly using
gradient descent. Then we land up at a new point. We repeat the steps again for this new point


In this code, we have used for loops we could go for vectorization as well. I have coded using vectorization
///////////////////////////////////////////////////////////////////////////

====================================================================================


alpha:

Learning Rate

m'=m-alpha*(dcost/dm)

c'=c-alpha * (dcost/ dc)


There might be a case if we subtract too much from m and c depending on slopes we may end up on the other side of minima. 
This is not desired as we have possibly skipped minima itself.  Also we might be stuck in  a loop going from one side of minima to another and never reaching minima.
That's where alpha comes into picture. Alpha regulates this to make sure that we don't subtract huge ammounts from
already existing m and c.

Other case is also possible. We might be going too slow toward minima. Here as well alpha comes into picture.

Basically alpha helps us to regulate the rate of change of the paramters as we move towards minima.

 
We have to make sure alpha is appropriate. It shouldn't be too high and it shouldn't be too low as well.

If its high we may always overshoot and never reach minima. If its too low, it may take huge ammount of time to reach minima and whole point of gradient descent over normal eqn was that we avoid taking so much time.

///////////////////////////////////////////////////////////////////////

We can have fixed alpha.
We can also have adaptive alpha.

We start with high value of alpha, as we come close to minima, alpha should decrease.
Also another thing to keep in mind is that if we overshoot, cost actually increases. So we have to
build this adaptive alpha scheme in such a way that if we overshoot, alpha should decrease.


=============================================================================================


Coding Gradient Descent:

For now lets not split data, lets just focus on coding gradient descent

m,c   =   gd(data, learning_rate, no_of_iterations)


no_of_iterations:   epochs
////////////////////////////////////////////////////////////////////

Code:

def step_gradient(points, learning_rate,m,c):
    m_slope=0
    c_slope=0
    M=points.shape[0]
    #print(points.shape[0])
    for i in range(points.shape[0]):
        x=points[i,0]
        y=points[i,1]
        m_slope+=(-2/M) * (y-m*x-c)*x
        c_slope+= (-2/M)*(y-m*x-c)
    new_m=m-learning_rate*m_slope
    new_c=c-learning_rate*c_slope
    
    return(new_m,new_c)
        
def cost(points, m, c):
    total_cost=0
    M=points.shape[0]
    for i in range(M):
        x=points[i,0]
        y=points[i,1]
        
        total_cost+=(1/M)*((y-m*x-c)**2)
    return(total_cost)
    
def gd(points, learning_rate, no_of_iterations):
    m=0  # Initializing m with 0, also a single m as we are working with simgle feauture
    c=0  # Initializing c with 0
    
    for i in range(no_of_iterations):
        m,c=step_gradient(points, learning_rate, m,c)
        print(i,'Cost: ', cost(points,m,c))    
    return(m,c)
    
    
def run():
    data=np.loadtxt('data.csv',delimiter=',')
    learning_rate=0.0001
    num_iterations=100
    m, c= gd(data, learning_rate, num_iterations)
    
    
    print(m,c)


run()

//////////////////////////////////////////////////////

Observations while coding


When we were coding, when we kept num_of_iterations as 1000 and learning rate as 0.001, we were getting error.

m and c were nan and we got a warning that m and c are overflowing.

This is because m and c were  becoming very huge that we were not able to contain them within varaibles.

 
We reduced no_of_iterations and added print statements to see what was wrong. Turns out learning rate was huge 
and we were overshooting. We reduced learning rate and then we saw that it stopped overshooting.



0.0001 seems like a good learning rate
/////////////////////////////////////////////////////////////////


==================================================================================

Generic Gradient Descent:

Earlier for simple Linear Regression input was m*1, where m is no of datapoints.
Now input is m*n where m is no. of datapoints and n are no of feautures.

Because of these n feautures, hypothesis fn looks like:

h(x)= m1x1+m2x2+m3x3+......................................+c

Task of Gradient Descent is to find, m1 ,m2..................c

////////////////////////////////////////////////////////////////////////////


How to denote different feautures:


i th feauture of first data point will be:

x1i

Normally when writing on paper, 1 comes in subscript and i comes in superscript.


For some jth feauture of ith row, we have:

xij

//////////////////////////////////////////////////////////////////////

How a normal data matrix looks:

x11    x12   x13   1
x21    x22   x23    1

The last column is always filled 1s to generalize this matrix.
We take c as m(n+1)*x(n+1), where all x(n+1) values are 1, m(n+1) is c
	
////////////////////////////////////////////////////////////////////////

Easy way to remember:
This is just like indexing 2d arrays,  rows come first, cols come second.

Data matrix can be assumed as a 2d matrix where rows signify different datapoints and cols signify different attributes.



//////////////////////////////////////////////////////////

cost:    1/M sum(yi -   ( m1*xi1 + m2*xi2+.........................m(n+1)*x(n+1)))
	i=1 to M



Now new gd fn will be

mj=mj-alpha*dcost/dmi

Here mj will be a generic feauture. Basically this will be the jth feature

For each feuature we will take partial derivative of cost w.r.t this feature to get slope corresponding to this feauture 



//////////////////////////////////////////////////////////////////////////////////



Generic formula of gd for ith row and jth feature


dcost/dmj=1/M sum (2 *(yi-(m1xi1+m2xi2+............................))) (-xij)
	           i=1 to M

Eventually we sum up for all i , to get slope for a particular feature

/////////////////////////////////////////////////////////////////////////////////////////////


Changes to be made in earlier code:

Instead of passing m as single value, it will now be arrays. 
Initialize all m's with 0. 
The last value of this array will be c.

Also slopes will be arrays rather than single values.

///////////////////////////////////////////////////////////////////////////////////



=======================================================================


Variations of Gradient Descent:

1) Standard Gradient Descent /  Batch gradient descentt: The gradient descent that we have been working with so far.
Here we go through complete dataset to get a cumulative slope and then we perfrom updations in our parameters



2) Stochastic Gradient Descent: 
Two ways:
1) Here instead of going through complete dataset to update a parameter, we rather update the parameters after every point. This is the method Ankush Sir told.

2) Another way can be we pick a random datapoint and just perform Gradient descent on this random point.
This counters a problem of Batch GD. In Batch GD we have to perform huge computations, whereas that is not the case with SGD.
 SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.
But there are too many oscillation in SGD in the path to reach minima.

In a way both approaches are nearly doing the same thing. Instead of going through complete dataset to perfrom updation to parameters, we instead perform updations at a much faster rate.

3) Mini Batch gradient descendant: 
Two approaches:

1) Go through some part of dataset, update parameters. Then go through some part of data update parameters and so on. Method told by Ankush Sir.

2) Here instead of going through complete dataset, we can sample a small numer of datapoints on which to perfrom
Gradient Descent. This sort of serves as middle point of Batch GD and SGD.

/////////////////////////////////////////////////////////////////////////

Batch Gradient Descent defn:
We compute the cost gradient based on the complete training set; hence, we sometimes also call it Batch Gradient Descent. 
In case of very large datasets, using this gradient descent can be quite costly since we are only taking a single step for one pass over the training set.

////////////////////////////////////////////////////////////////////////


====================================================================================

Feauture Scaling:

Part of preprocessing our data


This basically say , let's get all our feautures into a uniform range.

Suppose we consider Boston DataSet

We have 1 feauture say f1, signifying square footage in sqft. We have another feauture f2, having bedroom number.


Suppose sq footage varies b/w 1000 - 2000. Suppose bedroom number varies from 1 to 5.

Impact of having such varying ranges??

In many algos, sq footage will completely overpower bedroom number feauture due to the range of values.


//////////////////////////////////////////////


Manhatten distance:   mh(x1,x2) = sum   | x1i-x2i|
			       i=1 to n


Feature wise subtraction and then absolute values addition.


Example:

We have dataset like this

		sq ft		Bed room no
TD1		1500		3
TD2		1600		3
TD3		1550		1


Here 
mh(TD1,TD2)= 100
mh(TD2,TD3)=52

Here we might conclude that TD2 is closer to TD3 than TD1. But if we observe closely TD1 and TD2 seem much closer.
This is because sq ft is completely overpowering Bed room no feauture.

 ///////////////////////////////////////////////////////////////////////////

Here feature scaling comes into picture

Methods of feauture scaling:

1)  Min Max Scaling: Gets our data in range of 0 and 1

ith feautrue vector:
xi->    x1i,x2i...................xmi

We have m datapoints here.

Formula for  Min Max Scaling is:

(Xi-Xi min)/(Xi max-Xi mins)

Xi min: min for this feature vectir
Xi max: max for this feature vector 
Xi: any data within this feature vector

////////////////////////////////

Reasoning for this formula:

We want min to become 0 and max to become 1.

For any datapoint corresponding to a feauture, we get the ratio of Xi-Xi min  and Xi max-Xi min.

If Xi==Xi min, result becomes 0 as numerator becomes 0
If Xi==Xi max, result becomes 1 as numerator==denominator

Rest of the values will be b/w 0 and 1

/////////////////////////////////////////////////////////////////////////////


Conerting range to some particular range:

Xi minmax=Xi 01 * (max-min)+min


Xi minmax: feuature corresponding to min max range

Xi 01: feauture correpsonding to 0 , 1 range


Here when , Xi 01 ==0 ,  that is min value,we get Xi minmax=min
When Xi01==1, that is max value,we get Ximinmax== max

////////////////////////////////////////////////////////////


2) Standard Scaler: Bringing mean to 0 and std deviation to 1

xi-mu / sigma


mu is the mean for this ith feauture and sigma is the std deviation.


Reasoning for this formula:
When we do xi-mu, means gets reduced to 0. 

Ex:
Suppose data is:   1,2,3 

Mean: 2

When we do xi-mu, data becomes:  -1, 0,1
Mean=0


Why this is the case??

Suppose data is x1,x2,x3,...............,xn

We know   x1+x2+x3............xn  / n  = mu

When we subtract mu from each datapoint

x1-mu+x2-mu.........................+xn-mu  /  n

(x1+x2.................xn/ n   ) - n mu/n

mu-mu=0    


When we divide by sigma, std deviation of while dataset becomes 1.

 Hence because of this doing ,    (xi-mu)/sigma, gets the mean of our dataset to become 0 and standard deviation to become 1


///////////////////////////////////////////////////////////////////////////////////////////


===================================================

Ques. Feauture Scaling Class choice:

In which class of Machine Learning problems output should be scaled along with the input features?

Ans: None of these

Description
Explanation: In the classification problem, output variables are discrete so there is no need to normalize it. 
In the regression problem, scaling the output does not have any effect on the shape of the function. 
So, only input features should be scaled and not the output variable.

====================================================================

Feature Scaling in sklearn:

Importing the library:

from sklearn import preprocessing



Different functions for feauture scaling:
1) preprocessing.scale() : This fn centers around 0 for a particular axis. Basically makes mean to be 0. Also makes variance to be 1.

**********
To get std deviation of numy array :  np_array_name.std()


Code:
X_scaled=preprocessing.scale(x)




Probem with scale function: 
Suppose we have split independent and target feautures. To use this function, we will have to combine the two, us ethe dataset
and then split back the dataset. This is because we want that both training and testing dataset should be scaled to the same mean effectively they are scaled in same way and
not different ways. If we don't ensure this and scaling is done on seperate scales, it leads to huge problems.


Another thing to note is in real time we only have training data. The testing data comes real time so it won't be possible there to use scale function.



Ideally what should be done is:

We have x_train.
Scale this data and store the scaling parameters.
When we get testing data, we should scale the test data using these scaling parameters.


x_train  ->  scale -> store the scaling parameters  
		^
		| use scaling parameters to scale test data
		x_test


2) We can instead use 

preprocessing.StandardScaler():  Does everything like scale function

Code:
scaler=preprocessing.StandardScaler()     # Creating standard scaler object
scaler.fit(x)             # It finds the parameters on which to scale
scaler.transform(x)  # Using the found paramters, it scales our dataset

Benifit of this over normal scaler fn:
Because how this methodology works, we can save the parameters on which to scale.
Then using the we can scale other datasets as well.

We can pass paramters as well like , mean signifying what mean do we wabt for the feature, variance signifying what variance do we want and so on.


3) min_max_scaler():
does min_max_scaling.
The methodology adopted is same as standard scaler. 
Parameters are saved so that they can be used for other data as well.
We can pass paramters like min value, max value etc

4) max absolute scaler:
Does mean absolute scaling. In max absolute scaling we simply divide all value in a feature,  by max value in the feature.



Most of the times we use standard scaler.



//////////////////////////////////////////////////////////


==========================================================================

Handling Classification Problems:

Linear Regression works very well for continuous output data.


Now we will talk about, when our output/target is discrete.
Eg. Checking if an email is spam or not

////////////////////////////////////////////////////////////////////////////////

For now we will discuss binary classification. This means that class can be 0 or 1.

How can we handle binary classification with Linear Regression??

We will not continuous y's here, we will have discrete y's.
So plotting a scatter graph won't be so cluttered, rather will be concentrated at some places.

/////////////////////////////////////////////////////////////////////////////////


One hack that we can use to use Linear Classification for classification problem is
that we can asssume output to be 1 if predicted value is >0.5, else we can assume output to be 0.


Problems with this approach:
1) Just adding some few outliers/ extreme values, whole algorithm gets spoiled. This is because the decision boundary changes drastically
with these outliers hence huge problems get created in this way. 

2) Logistic regression gives continuous values. For some inputs we can get huge results way greater than 0, 
wheras for some, we might get results way less than 0. Wheras we only want results b/w 0 and 1.


////////////////////////////////////////////////////////////////////////


Decision boundary: Boundary which seperates two classes. Values on one side fall in one category and values falling on another side fall 
into another category.

/////////////////////////////////////////////////////////////////////////////



====================================================================


Logistic Regression:

Classification Algorithm

We want:

1) Outliers don't create such a big problem
2) The values are b/w 0 and 1 only.


////////////////////////////////////////////////////////////

Sigmoid fn:

S(z)= 1/ (1+e^(-z))


Here z goes to -infinity,  e^-z goes to infinity,  S(z) goes towards 0
As z goes to infinity, e^-z goes to -infinity, e^-infinity is 0 , so S(z) goes towards 1

When z=0, S(z)=0.5


///////////////////////////////////////////////////////


Using sigmoid fn in logistic regression:

h(x) = 1/(1+e^(-g(x)))

where 

g(x) =  mx+c

Here g(x) is standard linear regression.
////////////////////////////////////////////////////////////////////////

If we have 3 feautures, so g(x) becomes m1x1+m2x2+m3x3+c

where we can replace c with m4x4 where c==m4 and x4==1

Another way to right this can be g(x)=mT*x

where mT is transpose of matrix m.

m is basically    [m1 m2 m3 ...........................m(n+1)]

where m[n+1] is c


//////////////////////////////////////////////////////////

In essence for logistic regression we can write the hypothesis fn as :


h(x) = 1/(1+e^-(mT*x))

///////////////////////////////////////////////////////

Some observations with sigmoid fn:

1) The graph moves very fast to 0. This is because of the exponentiation factor. Although for x ==0, S(x)==0.5, but very fast moves towards 0 or 1.

2) Here we will assume that if h(x) > 0.5, our predictions will be 1 and if h(x) <0.5 , predictions will be 0.

3) 

h(x) = 1/(1+e^-z)

From the graph of sigmoid fn, we can infer that if 
a) z<0 , h(x) < 0.5
b) z>0, h(x) > 0.5

Here z is mTx

This effectively means that if   
mTx>0,  h(x)>0.5, y_pred=1
mTx<0, h(x)<0.5, y_pred=0


mTx effectively becomes the decision boundary 

So our output is discrete but our decision boundary is continuous. Basically Linear Regression serves 
as decision boundary.

///////////////////////////////////////////////////////////////////////////////////////
The equation of decision boundary itself is "mTx" or

m1x1+m2x2+.............................=0

This is an equation of line

///////////////////////////////////////////////////////////////////////////////////////


0.001*10+0.002*80+0.1

0.01+0.16+0.1
0.27


==================================================================================


Cost Function:




h(x) = 1/(1+(e^-mTx))

h(x)= S(mTx)

So basically, we have to find m, just like we were doing in Linear Regression.

Basically the best fit m such that error is minimized.


///////////////////////////////////////////////////////////////////

E( h(x) , y)

Linear Regression:    sum(( yi -(h(x))**2)
For linear Regression we were having squared error

In Linear Regression, the cost fn was straightforward because hypothesis fn was straight forward.


But now Hypothesis Fn is complicated in Logistic Regression. We cannot use the above cost fn for logistic regression.

The reason is:

1) There are many optimas. Basically there are many local minimas. But we want to reach global minima.
At local minimas as well, Gradient/ Slope will be 0. That will make the squared cost fn useless as we will never be able to reach global minima.
The local minimas are because of sigmoid fn.  In linear regression, because there is a linear relation b/w target and feautures we get a single minima. 
In Logistic regression, relation b/w target and feautures is not linear because mTx passes through sigmoid fn as well


We want a cost fn which gives us a single minima. This type of curver where we have 1 minima is called Convex curve.


A non-convex function is wavy - has some 'valleys' (local minima) that aren't as deep as the overall deepest 'valley' (global minimum). 
Optimization algorithms can get stuck in the local minimum, and it can be hard to tell when this happens.


A good link:
https://stats.stackexchange.com/questions/324561/difference-between-convex-and-concave-functions


////////////////////////////////////////////////////////////////////////


So Error fn that we use:

E( h(xi), yi)  :             


y ==1     -log(h(xi))   


y==0       -log (1-h(xi))

This is for binary classification. That is y can only be 0 or 1

///////////////////////////////////////////////////////////

Reasoning behind the fn:

Here if ,  h(xi) is 1 and y==1,  cost fn is -log(h(xi)), this results in -log(1) which is 0
 
if  h(xi) is 0 and y==1, cost fn -log(h(xi)),   -log(0) is infinity. This is  a very heavy penalty if we say prediction to be 0 and right result to 1


Simmilarly,

if,  h(xi) is 0 and y==0, cost fn is -log(1)  which is 0
if, h(xi) is 1 and y==0, cost fn is -log(0), -log(0) is infinity


Error grows very fast if we make wrong predictions.

/////////////////////////////////////////////////////////////////////////

We can combine above eqn like this:

E(h(xi),y(i))= -yilog(h(xi) - (1-y)log(1-h(xi)) 


if y is 0, we are just left with second part of eqn
if y is 1, we are just left with first part of eqn
//////////////////////////////////////////////////////
So overall Cost fn is:

E(x,y) =    sum (  -yi log(h(xi)) -  (1-yi)log(1-h(xi))
               i= 1 to M
//////////////////////////////////////////////////////////

We can also divide by 1/M to get average error per datapoint

E(x,y) = (1/M) *   sum (  -yi log(h(xi)) -  (1-yi)log(1-h(xi))
                               i= 1 to M

This error fn is convex


/////////////////////////////////////////////////////////////////////

==========================================================================

Finding Optimal values

E(h(x),y) =  1/m *  sum (-yi*log(h(x))-(1-yi)*log(1-h(xi)))
	            i=1 to n


h(xi) = 1/ 1+e^(-mTx)

We want to find the optimal m values.


We will use Gradient Descent here as well.

Idea for gradient descent:
Start at some value of m.
Keep doing:
mj=mj-learning * dE/dmj


///////////////////////////////////////////////////////////////////////////////////////////////////////////


E is also dependant on mj, where mj is some paramter in m  matrix. That's why we do dE/dmj

=======================================================================================


Solving derivatives 1:


E(m) =  1/num  sum ( -yi* log(h(xi))   - (1-yi)*(log(1- h(xi))) 
	      i=1 to num

/////////////////////////////////////////////////////////
Lets see what log(h(xi)) is:

log(h(xi)) = log(1 / (1+e^-(mTxi))  =    log(1) - log(1+ e^-(mTxi))       ( Using  log(a/b)= log(a) - log(b))

log(h(xi))= -log(1+e^-(mTxi))         (log 1 is 0)

//////////////////////////////////////////////////
Lets see what log(1-h(xi)) is:

log(1-1/(1+e^(-mTx))) :  log(e^ (-mTxi)) - log(1+e ^(-mTxi))


///////////////////////////////////////////////////

Let's plug these in Error function and then see:

Ei(m)= -yi (-log(1+e^(-mTxi)) - (1-yi) (log(e^(-mTxi))   - log (1+e ^ (-mTxi))   )

Ei(m)= yi (log(1+e^(-mTxi)) - log(e^(-mTxi))  + log (1+e ^ (-mTxi)) + yi log(e^(-mTxi)) - yi log (1+e ^ (-mTxi))    (Simply opening the brackets)


yi (log(1+e^(-mTxi)) and - yi (log(1+e^(-mTxi)) gets cancelled

log(e^(-mTxi)) =   -mTxi , because  log e^a= a
yi log(e^(-mTxi)) = -yi* mTxi, because log e^a=a
So,



Ei(m)=  mTxi - yi* mTxi + log (1+e ^ (-mTxi))

 
///////////////////////////////////////////////////////////

So for i th feauture


Ei(m)=  mTxi - yi* mTxi + log (1+e ^ (-mTxi))



/////////////////////////////////////////////////////////


=====================================================


Solving Derivatives 2:


Simplifying more:


Ei(m)= mTxi - yi*mTxi + log( 1+ e^(-mTxi))

We know:

mTxi=log(e^(mTxi))

So,

Ei(m)= log(e^(mTxi)) - yi*mTxi + log( 1+ e^(-mTxi))

Ei(m)=  log(e^(mTxi)) +log( 1+ e^(-mTxi))  -   yi*mTxi

Ei(m) = log( e^(mTxi)*(1+ e^(-mTxi))) - yi(mTxi)

Ei(m)=log(1+e^(mTxi)) - yi*(mTxi)



//////////////////////////////////////////////////////

Taking derivative over this simplified equation

Differentiating  yi*(mTxi)

 yi*(mTxi)= yi * (m1x1+m2x2+................m(n+1)*x(n+1))    ,  simply opening mT *xi

So  d yi*(mTxi)/dmj= yi* xij,                      Here mj is the feauture in which we are interested  yi is constant as we are doing partial derivative w.r.t mj. In the summation only term that will be left will be mjxj as all other terms will be constant and their derivative w.r.t mj will be zero

//////////////////////////////////////////////////////////

Differentiating log(1+e^(mTxi))


d log(z)/ d(x)  =  1/z * dz/dx 

d e^z/dx= e^z * dz/dx

So using these and chain rule 

d log(1+e^(mTxi))/ dmj =  1/ 1+e^(mTxi)  *  e^ (mTxi)  *   xij           (using chain rule) 

//////////////////////////////////////
Plugging this in cost fn


d Ei(m)/ dm = 1/ 1+e^(mTxi)  *  e^ (mTxi)  *   xij  - yi xij

d Ei(m)/ dm = xij(1/ 1+e^(mTxi)  *  e^ (mTxi)- yi)   (Taking xij common)

d Ei(m)/ dm = - xij (yi - e^(mTxi) / 1+e^(mTxi) )    (Taking - out)

////////////////////////////////////////////////////////////////

Also:
for e^(mTxi) / 1+e^(mTxi)  ,

Dividing numerator and denominator by e^(mTxi)   

e^(mTxi) / 1+e^(mTxi) = 1/1+e-mTxi

/////////////////////////////////////////////


Plugging into dE/dm

d Ei(m)/ dm = - xij (yi - 1/1+e^(-mTxi) )

1/ 1+e^(-mTxi) is just the hypothesis fn


d Ei(m)/ dm = - xij (yi - h(xi) )

This eqn is the slope of the error w.r.t one datapoint

//////////////////////////////////////////////

To get complete slope:

d Ei(m)/ dm =( - 1/num_data_points)  *sum(xij (yi - h(xi) ))
				i=1 to num_data_points
Dividing by num_data_points jsut to get the average.

/////////////////////////////////////////////////////
Eventually we need to find:

mj= mj- alpha *  d Ei(m)/ dm


/////////////////////////////////////////////////////////


===============================================================

Multiclass Logistic Regression:


So far we have :

if  h(x) < 0.5,   Output is 0
if h(x) >0.5, Output is 1
////////////////////////////////////

As output is b/w 0 and 1, we can use it like probability

So,

h(x) =  P(y==1)

Higher the hypothesis value, higher the probability that prediction is 1.

//////////////////////////////////////

Also if somebody asks about probability==0,
we can just calculate it like:

P(y==0)= 1-h(x)

////////////////////////////////////////////////



Multi class Classification:


We can use a one vs Rest approach.


Suppose our classes can be 

[cat, rat, dog]


We can one model to predict 

y= [cat, not a cat]

We can use Logistic Regression to build this model.


Then we can build another to predict [dog, not a dog].

Then we can build 3rd model to predict [rat, not a rat]

////////////////////////////////////////

Effectively we have built 3 seperate models and trained them seperately

Mode1 ->  Pr(y==cat)
Model 2-> Pr(y==Dog)
Model 3-> Pr(y==Rat)


We pick that class for output for which probability is the highest.

Another thing to note is sum of these probabilities won't be 1. This is because the three models are independant of each other.
We cannot enforce that sum of these probabilities be equal to 1.

One way to enforce is we do something like:

Pr(y==Cat) / ( Pr(y==cat)+ Pr(y==Dog)+ Pr(y==Rat))

We can do this for all probabilities and then effectively the sum of all the probabilities becomes 1

////////////////////////////////////////////////////////////////



Another sophisticated way to handle this is multinomial classification:



Here we will train only 1 model.
It will have many paramters


So 


P(y==j) =  (e^(mjx)) / sum(e^(mix))
		i=1 to k

 (e^(mjx)): paramters correponding to j th class for which we are getting the probability for this datapoint

sum(e^(mix)): We have k classes, we are summing the probabilities of all the classes.
i=1 to k


Here both i and j are in subscripts

Effectively we will have probability of all the output classes for a particular datapoint.

In binary classifications we were learning n paramters.
Here as classes are k, we will learn n*k parameters in one go.


Here the sum of proababilities will be equal to 1 as they are trained by a single model and are related to one another.

In one vs result we do learn n*k parameters but there, we have k models and each model learns n paramters.
//////////////////////////////////////////////////////////////////////////

When we use inbuilt logistic regression model, we can decide which approach to use

/////////////////////////////////////////////////////////////////////////

=========================================================================


Finding complex boundaries:

So far we are only able to come up with linear boundaries.

Just like we did in linear regression, we can add extra feautures to come up with more complex boundaries.



Suppose we have 2 feauture

x1 x2

if we want to add quadratic feautures as well, we can add x1**2 ,x2**2 and x1*X2

x1 ,x2 ,x1**2 ,x2**2 ,x1x2

We can train logistic regression on these extra feautures and come up with more complex boundaries.

/////////////////////////////////////////////////////////////////////


We can get any complex boundary but there is a catch. We can end up overfitting. With that our model won't perfrom 
well on testing data.

////////////////////////////////////////////////////////////

Countering overfitting:

We use something called regularization.

In our cost fn, we can add a penalty

So cost fn can look like:

cost =   1/num  sum ( -yi* log(h(xi))   - (1-yi)*(log(1- h(xi)))    +   lambda* sum(mi^2)
	      i=1 to num+1					i = 1 to num



lambda is called regularization factor and used to control the regularization.

sum(mi^2): with this we intend to add a penalty on the basis of content of matrix m, the parameter matrix.
As the sum(mi^2) increases, basically it means that we have added too much dummy data and cost will rise.

Gradient Descent will effectively have to make some of these paramters 0 or very less, efectively reducing overfitting.

//////////////////////////////////////////////////////////////////////////
Reasoning for regularization
Effectively the high degree feautures will be giving huge outputs. If their mi, that is parameter corresponding to these feautures
is high, then regularization penalty will be very huge. So only option with optimizer will be to reduce mi for high degree feautures


https://www.youtube.com/watch?v=4nqD5TBlOWU
//////////////////////////////////////////////////////////////////////////


We have to find optimal lambda,

if lamda is too high,  too much mi gets rendered useless and it leads to overfitting

If lambda is too low or 0, it leads to overfitting.


/////////////////////////////////////////////////////////////////////////////

This regularization is called l2 regularization.


We can add regularization in this way as well


cost =   1/num  sum ( -yi* log(h(xi))   - (1-yi)*(log(1- h(xi)))    +   lambda* sum(|mi|)
	      i=1 to num+1				i=1 to num


Here instead of squaring mi , we have taken the mod.

This is called l1 regularization


Generally we use l2 regularization

///////////////////////////////////////////////////////////////

Point to note:

There is no regularization on the intercept
This is because intercept doesn't add any dependency. It just tells us to shift our answer a little

hence i only goes to 1 to n and not n+1

////////////////////////////////////////////////////////////

======================================================


Using Logistic Regression from sklearn


Importing the libraries:

from sklearn.linear_model import LogisticRegression

////////////////////////////////////////////
Training the classifier:


clf=LogisticRegression()
clf.fit(canacer_ds.data,canacer_ds.target)
clf.score(canacer_ds.data, canacer_ds.target)

/////////////////////////////////////////////

Predicting the values:

clf.predict(canacer_ds.data)
clf.score(canacer_ds.data,canacer_ds.target)


Here score stands for mean accuracy unlike linear regression where score stood for coeffecient of determination

This tells us on average what is the accuracy of our data

//////////////////////////////////////////////////


Let's check what is the actual probabilities that weare getting.


clf.predict(canacer_ds.data)-canacer_ds.target

# Here after this code snippet, for a point, if we get 0 , than it means the prediction was correct. If we get non zero value, it means predictions were wrong.


To see the actual probabilities given to us by the hypothesis, we can use a fn call predic_proba()

It will return as an array of array.

For each inner array, we will have values, whose count equal to the number of classes available.  Here it will be 2.

possible classes 0 and 1.

in this array of array for each datapoint we have an array and it contains the probability of both 0 and 1,

///////////////////////////////////////////

Code:

clf.predict_proba(canacer_ds.data)

Returns us an array of arrays



Observations from this:

If we have a right answer , probability for right answer is usually very high like 96 % confident.

But when we have wrong answers, difference in probabilities of right and wrong is not that high. this is because of logistic
regression cost fns, penalty is huge for wrong answers. Because log fns escalate the values very quickly,

/////////////////////////////////////

fns seen till now:

1) fit
2) predict
3) score    (here it returns mean accuracy)
4) predict_proba

//////////////////////////////////////////////////////////////////////////

Tweaks that can be done:

1) While regularization, we added a lambda
cost + lambda * sum(mi^2)
In sklearn logistic regression, we have a parameter c.

It is applied in this way:

c*cost+ sum(mi^2)

Basically in the other part of formula, we have applied a parameter.

If we keep c very high, then regularization doesn't have any impact. It leads to overfitting

Very low c means , underfitting


2) solver : with this parameter we can change the optimizers. We have many available.
Some can be variations of SGD, some of BGD or MBGD
We can read about optimizers available for this in the documentation



3) tolerance:  cut of value where to stop. If change in cost is less than tolerance value, gradient descent is stopped on its own.


4) max_iteations: how many max iterations to run

5) multi_class: which approach to take for multiclass classification. If 'ovr' is passed, one vs rest approach is used. If 'multinomial'  
	is passed, multinomial approach is used.

6) penalty: Type of regularization


==================================================================================


Classification measures:

Confusion matrix:


How good or bad our classification solution is

For regression we used coeffecient of determination.


////////////////////////////////////////////////
What to be used in case of classification??

These may be:

1) Accuracy:  How many values we got right and many we got wrong.
    There is a problem with this approach.

Suppose we have a skewed dataset.
100 data points

95 ->  0,         5 ->1
Suppose we have built a classifier that always predicts 0.
We will have an accuracy of 95% with this classifier even though the model is extremely poor.

Hence accuracy is always not the best manner


2) Confusion matrix

Suppose we have 100 datapoints. 50 correspond to class 0 and 50 correspond to class 1.
	predicted   0			predicted 1
	------------------------------------------------------------------------
	|			|			|
0	|	40		|	10		|
True 0	|			|			|
	|			|			|
	|----------------------------------------------------------------------	|
	|			|			|
1	|	5		|	45		|
True 1	|			|			|
	|-----------------------------------------------------------------------|



Suppose 40 0s were predicted 0.
Automatically we know 10 would have been predicted 1, even though their true class is 0.



Suppose 45 1s were predicted 1.
Automatically we know 5 would have been predicted 0 even though their true class is 1.

This tells that are we doing well on all classes or just some classes

The above matrix is called confusion matrix

////////////////////////////////////////////////////////////////////
Some terms regarding confusion matrix:


	predicted   0			predicted 1
	------------------------------------------------------------------------
	|			|			|
0	|	40		|	10		|
True 0	|			|			|
	|			|			|
	|----------------------------------------------------------------------	|
	|			|			|
1	|	5		|	45		|
True 1	|			|			|
	|-----------------------------------------------------------------------|


Here 0-> Neg
1->pos


True positives:  Which were truely positive and also were predicted positive. Here 45 corresponds to True positive.

True negatives: Which were truely negative but were predicted negative. Here 40 corresponds to true negatives.

False positives: Which were truely negative but were predicted positive. Or which were predicted postive but were negative. Here 10 corresponds to False positive


False negatives: Which were predictive negative but wer positive. Here 5 corresponds to false negatives.



//////////////////////////////////////////////////////////////////////


How to remember which is which.

Clause1 Clause2

Clause2 tells what was our prediction. positive means 1 and negative means 0.
Clause 1 tells if our prediction(Clause 1) was right or not. True means means it was right. False means it was wrong.

So to remember, first observe Clause2 and then work with Clause1.



We can use these terms only when we work with binary classification but we can use confusion matrix with all the cases.


////////////////////////////////////////////////////////////////////////////


Code:

Importing:

from sklearn.metrics import confusion_matrix

/////////////////////////////////////////////////

Creating confusion matrix:


confusion matrix(y_true, y_pred)


Ex:  
confusion_matrix(Y_test, Y_test_pred)
/////////////////////////////////////////////////////////////

Ques Confusion matrix 1
A = 80
B = 90

60 20	
15     75


///////////////////////////////////////////////////


Ques Get Accuracy

95/ 150





==============================================================================


Classification measures:

We will look at an objective way to distinguish b/w classification models.

The two important terms in this quest are:

1) Precision
2) Recall





	predicted   a			predicted b
	------------------------------------------------------------------------
	|			|			|
a	|	40		|	10		|
True a	|			|			|
	|			|			|
	|----------------------------------------------------------------------	|
	|			|			|
b	|	8		|	42		|
True b	|			|			|
	|-----------------------------------------------------------------------|



Precision and Recall are for a particular class. They are not just one number overall.

Precison:   How precise I am, in what I am saying. % of predicted a's are actually a.
for
class a

Here precison for a is:  40/48


pre(class b):  42/52

Precision is on predicted values and not truth values.


///////////////////////////////////////////////////////////////////////////////////////////////


Recall:

In truth we had 50 a's , how many could we recall correctly

recall(a): 40/50




//////////////////////////////////////////////////////////////////////////////////////////////


How to remember which is which


precion(class)

precision ===============  predicted.                            Vertical in confusion matrix

We will look at 'prediction' for 'class'  and see how were the results for 'class'


recall=========================== inverse of predictions,  i.e. Truth            Horizontal in confusion matrix          

We will look at  'truth' for a 'class' and see how were the results for 'class'

//////////////////////////////////////////////////////////////////////


We want precision and recall to be high for all the classes we are working on.


=================================================================

Code in sklearn:


Import :

from sklearn.metrics import classification_report

Printing:

print(classification_report(Y_test,Y_test_pred))

////////////////////////////////////////////////////


Sample output:

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        13
           1       1.00      1.00      1.00        14
           2       1.00      1.00      1.00        11

    accuracy                           1.00        38
   macro avg       1.00      1.00      1.00        38
weighted avg       1.00      1.00      1.00        38


//////////////////////////////////////////////////////////

f1 score: Harmonic mean of precison and recall. It helps us to get to a single number instead of bothering with two numbers.


Harmonic mean of x and y:     2 /   (1/x+1/y)


//////////////////////////////////////////////////////////////


support(class) : How many actual true values of a class we had.

==============================================================================


Decision Tree:


Classification algorithm. Very intuitive.


Suppose we have a dataset like:

x1 x2 y
0    0  0
1   0  1
0   1  1
1    1   1

Basically this represents Or operation. Its decision tree can be:

		x1=0
	Yes			No
						
	x2=0			Output 1
       Yes		No
   Output0               Output1



This represents a decision tree.

Basically a decision tree is step by step decision until we reach a leaf node.


=============================================================================


Decision for interview call


Data:


	College		Projects		Intern  		call?

	t1, t2, t3		True/False	True/False	Yes/No
			
intern means has the student done some internships??			

		
				College
		t1 or t2				t3
		intern??				intern??
	Yes		No		No			Yes
	Output  Yes 	project		Output No		Project
							No		Yes
		Yes		No			Output No	Output Yes
		Output Yes	Output No
		


==================================================================================


Building Decision Trees:

We want to start from top.

Idea is to find a feauture which splits our data into two parts.

Suppose we split like this

				College
		tier1/tier2			tier 3
		20 datapoints			30 datapoints	
		7 yes, 13 NOs			3 yes, 27 Nos

Again we will try to split our data on some other feauture


				College
		tier1/tier2			tier 3
		20 datapoints			30 datapoints	
		7 yes, 13 NOs			3 yes, 27 Nos

						Intern??
					20			10
					20 N			3Y  7N


Here wedon't have to split the 20 N, as its a single possible value only now for this branch.


				College
		tier1/tier2			tier 3
		20 datapoints			30 datapoints	
		7 yes, 13 NOs			3 yes, 27 Nos

						Intern??
					20			10
					20 N			3Y  7N
								Project ??
							Yes			No
							5			5
						        3 Y, 2N		            5 N

We have a got a perfect no with No branch. All are Nos in that branch.
 On the other hand in the Yes branch, even though we don't have all values of same class but we cannot further split.
We have exhausted all the feautures for this branch. We take a majority vote here. Majority vote says class should
be 'yes', so we select Yes over here.


				College
		tier1/tier2			tier 3
		20 datapoints			30 datapoints	
		7 yes, 13 NOs			3 yes, 27 Nos

						Intern??
					20			10
					20 N			3Y  7N
								Project ??
							Yes			No
							5			5
						        Output Yes		            Output No


We can do the same on the other side as well.

We always want to build the best possible decision tree.The best Decision Tree will where training accuracy is maximum.


Suppose we have 40 Y and 10 N

	50 
	10 Y  40 N

We decide not to split further.
If we are not splittubg, we will take majority vote.
We decide No for all.
Accuracy is 40/50

Suppose we did decide to split

Split is like this :

		50 
	          10 Y  40 N
	
	15			35
	8Y 7N			2Y 33N

Suppose now we decide not split again.
Here again majority will take place.
Now if closely see, accuracy will be 41/50

==================================================================


Getting to best decision Tree:

Getting the best decision tree is actually very very expensive.

Time Complexity for such an operation is exponential


If we do something like for each level try all the possible permutations for splits, it leads to exponential Time complexity
Its a np hard proble, essentially very hard problem to solve.

We take an alternative approach.
We take a greedy approach instead of Dynamic Programming


At each level we will decide which feauture to split on greedily.

It might happen we may not have eventual best accuracy but if we want to do this in a reasonable time only way is to use
greedy method.


For each split, we check what is the gain that we get on splitting on each feauture.
We select that feauture to split on from which we get maximum gain.


Suppose we have x1, x2, x3.................'

We will check which feautures gives us the best gain, then we split on that feature.

This parameter can be anything on which we are checking our gain.

//////////////////////////////////////////////////////////////////////


The greedy recursive algo is:

1) If a node is pure , i.e. it contains only single class values, output the node.

2) if no feature is left to be split upon, we have used all the feautures and left is still impure, output majority.

3) If both above condns not met, find the best condn to split upon. Recursively call on the splits.

/////////////////////////////////////////////////////

Generally when we are splitting a feauture, we split into as many feautures as possible.

///////////////////////////////////////////////


=========================================================================


Deciding the feautures to split upon:

		100
		20 Y   80N

Feautures left :

College:   t1,t2,t3
Intern: True or False
Project : True or False


Some objective metrics:

1) Accuracy
2) Information Gain
3) Gain Ratios
4) Gini Index

All these are formulaes with which we can get a sense if we are moving in the right direction or not.

So we can use any of the above metrics, code remains same for all.

We use accuracy for now.




Lets say data looks like this 

		100						Here accuracy would be 0.8 if we didn't do any split, we would have to go for mahority. 
	         20 Y    80 N						Majority says  No. So we would have predicted 80 right out of 100

 T1		T2		T3	 
  20		30		50
  12 Y, 8N  	5Y, 25N		3Y, 47 N				Here accuracy becomes 0.84. If we don't perform further split, we take majority vote, For t1 we predict 12 right out of 20, for T2 we predict 25 out of 30
								For t3 we predict 47 out of 50. Total we predict 94 right out of 100.

//////////////////////////////////////////////////////////

Lets try to split on some other feauture.

 						
		100							accuracy =0.8						
	         20 Y    80 N

		intern??
	
	Yes			No	
	25			75					accuracy = 0.91
         18Y,   7N			2y    73 N



/////////////////////////////////////////////////////////////


Let's try to work with prjects now:


		100							accuracy =0.8						
	         20 Y    80 N

		projects??
	40			60 					accuracy = 0.8
     12 Y, 28 N		        8 y, 52 N



////////////////////////////////////////////////////////


We can see that the best choice to split is on the interns.



Another thing to keep in mind is, its not compulsory that everything on the same level will be split according to same feautures.


==================================================================================


Continuous valued Data


So far we have seen splitting on Discrete values.

//////////////////////////////////////////////////////////////////////////


How to split Continuous values??

We can try to put a boundary/ threshold. Anything one one side of that boundary goes to one branch and anything 
on the other side of the boundary goes to other branch.

Example   x1<1.9

Here 1.9 can be such boundary


Suppose we have 3 feautures.


x1  -  Length of some parameter

x2- True or False

x3 ->  a,b,c


Its very easy to split using x2 and x3, but not that easy to split using x1.

Basically we need to get x1 and the boundaries.
This can have infintely many possible values.

Let's try to reduce number of points first.

Suppose x1 is  1.7, 1.9, 1.8, 2.3, 2.7, 2.6

We can plot this in a sorted order on a line:

			        1         2	
1.7---------- 1.8--------------1.9-----|------|---------2.3----------------------2.6-----------------------    2.7


Now it doesn't matter if we split using  1 or  2 in the above figure. Same amt of datapoints go to any branch in either case.

So we can just assume that we will work on the middle points of two different datapoints.

1.7----|------ 1.8-------|-------1.9---------|-----------2.3----------|------------2.6---------------|--------    2.7

We use a brute force approach
We can split on all these middle points, select that which gives us the best accuracy/ any other parameter.

For ex, first we try spliting using 1.75(midpt of 1.7 and 1.8), see how is the metric changing, 
	Then we split on 1.85 (midpt of 1.8 and 1.9)  and so on.

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////


==================================================================


Code using sklearn


Importing Decision Tree:
from sklearn.tree import DecisionTreeClassifier

////////////////////////////////////////////////////
Creating Decision Tree classifier object and fitting:

clf= DecisionTreeClassifier()
clf.fit(x_train,y_train)




///////////////////////////////////////////////////////////////

Visually look at the decision tree:

We can export the decision tree to a pdf and there see how the decison tree looks like


importing function:

from sklearn.tree import export_graphviz


Exporting to a file

We will need to install a module called pydotplus.

We can do :

conda install pydotplus
////////////////////////////////////////////////

Importing pydotplus:

import pydotplus


/////////////////////////////////////////////////////

Exporting to a file:
dot_data=export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names)

Explanation: With out_file= None, this file is not saved, rather we just get dot_data output 
Also feauture names are passed so that the pdf file that is created has these feauture names.
Also we are passing the class names as well

graph = pydotplus.graph_from_dot_data(dot_data)
Explanation:  Creates a graph object using the dot_data variable

graph.write_pdf('iris.pdf')
Explanation: graph.write_pdf:  Writes the graph object to a pdf file

///////////////////////////////////////////////////////////////


=====================================================================


Information gain:

Another way to decide on the feauture to split upon.

//////////////////////////////////////////////////////////////////////////

For every node:

We define entropy/ Information Required as:

InfoN= - sum ( pi * log(pi)) 

Here pi is the probability for a class

Ex:

For a node, say we have

9 Y ,   20 N:

So,

Information gain=
-9/29 log2(9/29)  -  20/ 29 log2(20/29)
 
log2 means log to the base2
So we can say that this is the information required to get rid of impurity in this class.

///////////////////////////////////////////////////////////////////////////////

Reasoning:

Suppose we had this split:

0 Y 29 N

This class is pure

Here The entropy/ Information gain will be:


-0/ 29 * log(0/29)  - 29/29 log(29/29) = 0    , log 1 is 0


Hence for a pure class, information required is 0. The more impurity we have, the bigger this number will be.

/////////////////////////////////////////////////////////////////////////////

==========================================================================

How to split on feautures:

The feauture that should be chosen should result in maximum decrease in information required/ entropy.

Entropy generally means randomness. We want to decrease the randomness.




So how to use this

			node1  					This will have some information,  Info N()
			 40 samples
			split on feauture f				Info required here, lets take it as info F()
	
	f1		f2			f3
	Info f1		Info f2			Info f3
	info req: 0.6	info req: 0.4		infor req=0.8
	20 samples	5 samples		15 samples

Taking the weighted info req sum:  (20/ 40 )*0.6   + (5/40) *0.4  + (15/40)* 0.8

info gain= info N() -info F()


/////////////////////////////////////////////////////////////////////////////////////// 

Initially we will have a base node. We can get the information required for it. 
We decide to split on a feauture and it creates 3 branches

For each branch we will get a new node. We get information required for each node that was created.
We take the weighted sum for all the new nodes. This is because total sum anyway leads to increased information required. That's why we get the weighted information required. 
Then we take the difference b/w the initial information required with the new info required to get the information gain.


So  information gain after splitting on a feature=   info orig()  - info after_split()

We pick the feauture which leads to maximum information gain
////////////////////////////////////////////////////////////////////////////////////////////////

====================================================================================


Problem with information gain:

Suppose we have 10 sample dataset
Suppose we have a feauture, something like product id where we are classifying the products.

When we split on this product id, we will get 10 different branches as all the product ids will be unique.
All the new node will be pure because for 1 id, one product will exist. 
We will get maximum information gain, because all are pure. Info required for each node will be 0.

But when we go for testing data, we will have new product ids and we will end up misclassifying.
Hence Info gain won't work. Info gain favors those splits where we get more and more branches.

Hence its not the best parameter.

///////////////////////////////////////////////////////////////////
==============================================================================


Gain ratio:

Just a change on information gain.

Gain ratio = info gain / split info

In addition to info gain, we also calculate split info for a split.


In essence what we want to achieve is, if we get a lot of nodes out of a split and for each each new node if we have 
very less number of samples, we want to penalize it. split info should be high in that case.
//////////////////////////////////////////////////////////////////
suppose we have 3 splits D1,D2

split info= - |D1| / |D|  * log (|D1| / |D| )  - |D2| / |D|  * log (|D2| / |D| ) - |D3| / |D|  * log (|D3| / |D| )   



If we have many 1 sized splits ,  for each such split we will have:  

- 1/ 10 *log(1/10)=   -1/10 * log2 (1)  + 1/10 log 210  =   1/10 log2(10)

We will have many such 1/10 log 10 terms and it will mean split info is too huge. This will inturn decrease the information gain'


log used is log to the base 2

/////////////////////////////////////////////////////////////

General formula of split info:

-sum(|Di|/|D|  * log (|Di|/|D| ) )
i= 1 to num_new_nodes

 
Di is the number of samples in the new split


////////////////////////////////////////////////////////////////

==============================================================================

Problem with gain ratio:

Case1:
	50						Split info:  0.301
25 samples	25 samples

Case2:
	50						split info= 0.03
1 sample		49 samples


Gini ratio prefers case 2. split info is less in case 2, info gain will be higher. 
It results in favoring case 2.

This often leads to imbalanced splits. 

Hence we may go with gini Index. Gini Index is the default metric to do splitiing of Decision Trees in sklearn.


Gini Index will prefer case 1
//////////////////////////////////////////////////////////////////////////////



===========================================================================

Gini Index:



Suppose we have a node D:

7 Y, 21 N


Gini D =  1- sum(pi**2)

pi is the probability of a class


here, proability of class Y: 7/ 28
probability of class N: 21/28

Gini D=    1 -  ((1/4)**2   +  (3/4) **2)



/////////////////////////////////////////////////////////


*****

In Gini Index, we only do a binary split. In other methods we used to rely upon as many splits as possible
but here we will only look to do  a binary split.


If a feature has k different values, then we have to consider all the subsets.


///////////////////////////////////////////////////////////////////



D=   7 Y , 21 N

We split such that:

D1 :  4y, 2N
D2: 3y, 19N

Gini D1 = 1- ((4/6)**2 + (2/6)**2):   


once we have found Gini Index of all the nodes, we take the weighted sum of all.


Gini Split:   |D1| / |D| * Gini(D1)   +  |D2| / |D| * Gini D2

|D| = |D1| + |D2|


////////////////////////////////////////////////////////


Gini Index of pure class:

1 - sum(pi**2) =  1-1 = 0

So, lower the Gini Index, the better.

Higher Gini Index means higher impurity
////////////////////////////////////////////////////

delta  G  f=  Gini D-  Gini split f

Which ever gives us the maximum gain in Gini Index, that will be picked


////////////////////////////////////////////////////////

Gini Index favors balanced splits and pure classes


/////////////////////////////////////////////////////


===========================================================


Decision Trees and Overfitting:


A very common problem is Decision Trees

This is because we keep splitting till the point we reach absolutely pure nodes.

That leads to 0 training errors.

That works very badly for testing data.


This is a very common problem that we face with decision trees


//////////////////////////////////////////////////////
Some intuitive ways to stop this:

Essentially add more criterias of stopping the splits. For now, we only have two options.
Either we get pure nodes or we exhaust the feautures.

1) Stop early

2) Build complete tree, then do some prunning.


///////////////////////////////////////////////////////


Stopping early:

Two ways of doing it:

1) Define maximum depth:  
a) Problem with this will be how to get to optimal depth where we will stop.
b) Sometimes, in branch we might want to continue splitting and on the other branch, we may not want to continue splitting. We won't be able to achieve this via this approach. 
	This forces balance on both sides.

2) Stop when you don't get significant change in score. This score can be accuracy, Gini Index or anything else.
     This sounds good but there are problems here as well.

Ex: Let's take example of XOR.			x1	x2	y
						T	T	F
	2Y   2N					F	F	F
	x1					T	F	T
    T		F				F	T	T
1Y, 1N		1Y,1N

Initially the accuracy will be 1/2
As accuracy from both x1 and x2 will be 1/2 we choose randomly.
Suppose we split using x1. 	

Here as well accuracy will be 1/2

Same case if we split using x2.

In essence accuracy won't change and we would not split after the root.





///////////////////////////////////////////////////////////////////////////////////

Essentially both these methods have issues, hence we end up chosing Pruning


///////////////////////////////////////////////////////////////////////////////


======================================================================================


Prunning:

As the complexity of graph increases, the training error keeps decreasing.

But the testing data will decrease till some point, but after this point it will increase.

Ideally we should stop making the graph more complex as soon as testing data starts increasing

The problem is how do we decide we have reached the minimimum testing error??

What we do is, we build our tree and then start prunning the tree to reach the minimum training error.


//////////////////////////////////////////////////////////////////////

Many ways to do this, what we'll strive to achieve is, we'll come with a mathematical formula which penalizes adding more splits.



//////////////////////////////////////////////


Complex Tree  vs Simple Tree:


Complex Tree: Having more nodes. Or we can also say have more number of splits. As splits increase, number of nodes increase as well.
So number of split is directly proportional to number of nodes. 
Essentially more number of splits means more number of leaf nodes. So we can measure complexity of tree with the number of leaves.

Simple Trees: Having lesser number of leaves.

///////////////////////////////////////////////////////////////


We will build our tree and get a cost function.

The cost function should try to reduce the error and also try to reduce the number of leaf nodes as well.

cost=  Error  + lambda * num_leafs


If lambda is very very high, we need the num_leafs to be minimum

If lambda is very low, No of leafs can be high which leads to overfitting.


///////////////////////////////////////////////////////////////////////////////

We will build our tree and then we will try to reduce this cost function.


Error: Lets take the error to be something like number of misclassifications.
lamda: For now let's take lambda to be 1.



cost=  Error  + lambda * num_leafs



Idea:
Idea is to see if we get a better cost with the split or without the split. If without the split, the cost 
is better we will get rid of the split altogether.





Suppose this is the tree:


				                   10 Y 40 N
		          7Y 13N				              3Y 27N
	         5Y 2N		    2Y 11N			3Y 1N 		26N
4Y 0N	                    1Y2N                 2Y 1N	    10N                2Y                   1Y 1N
	                 1Y       2N	         2Y        1N		                              1Y          1N


aLL THE nodes are pure so training error will be 0.

Leaf nodes= 10

lambda=1.5

cost right now=  0+1.5*10= 15



Lets consider:
 		1Y2N                
	                 1Y       2N


Suppose we remove tbis splt
We will have:

	1Y  2N

On taking the majority, We will make this No.

Now Error increases to 1, as we will misclassify 1 sample.
Num of leaf nodes now= 9 

New cost:    1+ 1.5*9 = 14.5

We have managed to reduce the error.

With this we know, we should remove this split altogether.

We should do this for all the leaf nodes.

**************************************
One thing to note is this prunning is done on the decision tree built by Training Data itself

**************************************

Other mechanisms can also be used for prunning

We can also have something like:

Cost P  - Cost wp=  Lp - Lwp

Cost P: With Prunning
Cost wp= Without Prunning

So if this cost is justified then only we do prunning, otherwise we don't

////////////////////////////////////////////////////////////////

===================================================================	


***********************
Considering Decision Trees are effectively trees we can easily use recursion for all the ops
****************************


====================================================================

data_list=[[1,1,1],[1,0,1],[0,1,1],[0,0,0]]
data_df=pd.DataFrame(data_list,columns=['x1','x2','result'])

temp=data_df.columns

temp=list(temp)
temp.remove('result')
x=data_df[temp]
y=data_df['result']
features=list(x.columns)


==========================================================================


Random forests:


Extension of Decision Trees.

Combination of many Decision Trees.
//////////////////////////////////////////////////////////////

Why Random Forests??


1) Decision Trees leads to huge overfitting.

Some wayts to counter overfitting:
a) Limit to a maximum depth
b) Limit to min change in our metric
c) Prunning

Prunning is not implemented in sklearn yet. We can use maximum depth but we can't use prunning.

//////////////////////////////////////////////////////////////

When we have larger datasets where data has outliers, we suffer with high variance problem.
Model performs very well on Training Dataset but is not able to work properly with testing datasets.

////////////////////////////////////////////////////////////
What Random forest does??

Instead of creating one decision tree, we implement many decision trees.


Suppose we have data and we have to predict  a class say a nad b.

Suppose we build 3 decision trees and then we take the majority prediction as the right prediction.


////////////////////////////////////////////////////////

Why will individual Decision Trees give different results if we train on same data??

We will not train all the trees using same data. We use some randomness so that one outlier may impact some DT
but not all.

 Some feautures only impact some Trees but not all the trees.


==================================================================================


Data bagging and Feature selection:


We have some feautures and some rows in a dataset.

Instead of building one classifier we will build many classifiers.

We take majority to predict final results.

/////////////////////////////////////////

We don't give same data to all the classifiers.

We have to come up with an approach where we decide, which data  to give to which classifier.

We use a technique called bagging to do this.



/////////////////////////////////////////////////////

Bagging:  Bootstrap Aggregation Algorithm

What it says:

If we have m data points, select k data points but with replacement.



Suppose we have data like this:   d1, d2, d3

We can end up with data like this: d1,d2,d2

/////////////////////////////////////////////

1)
Generally what we will do in Random forests is:

Bagging->    m  datapoints  -> Bagging ->  m datapoints  but with replacement

Some datapoints can even come more than once.


After bagging we have a second step as well, called feature selection.

2)  Feauture selection: We won't train all the Decision Trees on same features. We give some features only for a particular 
Decision Tree.

We can select them randomly. Suppose we chose k features out of n features.This is not with replacement


Generally a safe number to chose is sqrt(n)

So this ensures that if some feature is creating some problem for us, we might be able to avoid that feature itself.


////////////////////////////////////////////////////////////////////////////////////


===========================================================


Extra Trees:

For continuous values:
When we are building Decision Trees we have to get the middlepoints of the continuous values. 
After that we use Information Gain to get the best feature.
So for continuous values, not only do we select which feature to chose, we also have to select which value to chose. 

For Gini Index:
We only do binary splits here.
So suppose our desired feature has 4 possible values, say a, b, c,d 

We have to check all the permutations to decide how the binary split should be.

Here as well not only we have to chose which feature to choose but we have to chose which values to split on as well.


//////////////////////////////////////////////////////////////////

For random forests we can have something extra called Extra Trees.

We can randomize the process of chosing the continuous data points to split, also we can randomize the subset 
formation.

This randomness is termed as Extra Trees.

If we are having a lot of overfitting, we can add this.

////////////////////////////////////////////////////////////////////


Basically with random forests, we chose data randomly, we chose feautures randomly and we can even chose 
the subset formation and points to split on randomly as well


=============================================================================

Regression with Decision Trees aqnd Random forests

Basic algo remains the same.

root has all datapoints

Normally we see how targets stack up in the splits to get the idea of information gain or gini index.

/////////////////////////////////////////////////////////////////////

For regression we will split on those feautures where metrics like Mean Squared Error gets minimized.


			root
	Split1				Split2



initial MSE=  (1/m) * sum(y actual - y mean)**2
	                  i=1 to m


after split we pick the split which leads to maximum decrease in MSE


/////////////////////////////////////////////////////////////////////

So what effectively changes for regression??

1) Predictions:  Take the mean of the values available at leaf node to get the prediction


2) How to split: We split on those feautures which lead to maximum decrease in MSE.

////////////////////////////////////////////////////////////////////


With random forests, to get the final prediction we get the mean of predictions from all Decision Trees.


////////////////////////////////////////////////////////////////////


================================================================================

sklearn Random Forest


Decision Tree                  do well in  Training , but do worse in testing



Random Forest               do worse in Training, but do better than Decision Tree in testing


These statements are general statements. Decision Tree can actually work well in some datasets.

But for datasets with good ammount of data and outliers will always follow the same general statements


//////////////////////////////////////////////////////////////////////////////////////

Importing random forest classifier:
from sklearn.ensemble import RandomForestClassifier
 


Creating the classifier object:
clf= RandomForestClassifier(random_state=0)


# With random_state=0 , we ensure that the random forest classifer generated is always the same.
# Otherwise everytime a different tree is created because of randomness.


/////////////////////////////////////////////////////////////////////////////



Optimization in sklearn decision tree and random forest:

We cannot do prunning, but we can add max_depth parameter



/////////////////////////////////////////////////////////////////////////


============================================================================

Naive Baise Classifier:

Bayes: Uses Bayes Theorem

P( A / B): Probability that A will happen given B has already happened.



P(A / B) :  P(B/A) * P(A) / P(B)


///////////////////////////////////////////////////////////////////////////////////


Given x, we predict y

x is independant features
y is dependant features


Suppose classes are a,b or c.


We can write this as:


P(y=a/ X=x): Basically meaning that what is the probability that y=a given x is the input.

P(y=b/X=x)

P(y=c/X=x)


Sum of these proababilities should be 1.



If we can predict all the probabilities, we can say that the output class will be the class having maximum probability

////////////////////////////////////////////////////////////////////////


So given  x, and possible classes y=a1 a2 a3..........................ak,

We want to predict

max(P(y=ai/ X=x))   ->     class for input x                                    -------------------------  Eqn1
i=1 to n

Here ai is some ith possible class 

/////////////////////////////////////////////////////////


Lets see for a single class i.


We need to get P(y = ai / X=x)

This is not that easy. Instead we can use Bayes Theorem to reverse this.


P(y=ai/ X=x) =   P(X=x / y= ai) * P(y=ai) /  P(X=x)


//////////////////////////////////////////////////////////////////

If we put the above in Eqn 1




max(P(y=ai/ X=x))  = max(    P(X=x/y=ai) * P(y=ai) / P(X=x) )
i=1 to n		 i=1 to n



Here denominator remains the same always


Hence we can get rid of denominator.


Now this won't remain probability and sum won't remain 1.

But we can maximize   P(X=x/y=ai) * P(y=ai)   to know which class to give as output


///////////////////////////////////////////////////////////////


Now the new problem is:

find i such that

max(P(y=ai/X=x)) =   max(P(X=x/ y= ai)* p(y=ai))



Suppose we have

100 samples

40 belong to a1
30 belong to a2
30 belong to a3


Then 

P(y=a1) =   40/100
P(y=a2) = 30/100
P(y=a3) = 30 /100


We will talk about getting P(X=x/y=ai)
 


////////////////////////////////////////////////////////////////////////////////////////////////

===============================================================================================


Independance assumption in Naive Bayes


Need to get P(X=x/ y=ai)


Not that easy to solve.

X=x, input has many features


f1 f2 f3 f4...................


So, we need to find

P( (f1,f2,f3,.........fn) = (x1, x2, x3...xn)) / y= ai)

////////////////////////////////////////////////////////////////


Effectively meaning that we need to get the probability that f1==x1, f2==x2, .............. fn==xn given y=ai

///////////////////////////////////////////////////////////////////////


Why is this classifier called Naive??

It makes a very strong and Naive assumption that lets assume all the features are independant of each other

Then, if suppose 2 features(A and B) are independant of each other,

P(A intersection B) = P(A) * P(B)



Ex:

What is the probability that first time I toss a coin, I get a head. Second time I  toss a coin, I get a tail??

Both events independant of each other.

P(1st= Heads intersection  2nd Time = Tails) = P(1st=Heads) * P(2nd = Tails) =  1/2 * 1/2 = 1/4


//////////////////////////////////////////////////////////////////////////


So effectively what happens

P( (f1,f2,f3,.........fn) = (x1, x2, x3...xn)) / y= ai)  =   P(f1 = x1/ y=ai) * P(f2 = x2 / y=ai)  *...................... P(fn= xn / y= ai)


///////////////////////////////////////////////////////////////////////////////////


In reality, it will not be the case that these features are independant of each other. In reality these will be dependant on one another.

/////////////////////////////////////////////////////////////////////////////////////


So,

P(X=x / y=ai)  =  multiply ( xj = xj / y= ai)
	            j= 1 to n


j is in supersc ript here. So means for some datapoint xj is the j th feature.


////////////////////////////////////////////////////////////////////////


P(X=x / y=ai) * P(y = ai) =  multiply P( Xj = xj / y= ai) * P(y=ai)
	                                 j= 1 to n


//////////////////////////////////////////////////////////////////////////

Probability estimation for discrete values


How to find pur P(Xj=xj / y= ai)


Lets assume for now, all our data is labelled. The features are discrete.

For now we are assuming data is not continuous, its labelled.

For example for a feature like salary we don't have exact salaries, we can have something like low, medium, high


///////////////////////////////////////////////////////////

P(Xj=xj / y= ai)

For now lets just do it for a1. After that we can do the same for all the classes



Suppose we have dataset like this


x1 x2 x3 .................................  class
v1 v2 v3.....................................a1
v1 v2 v3....................................a2
v1 v2 v3.....................................a1
v1 v2 v3....................................a2
v1 v2 v3.....................................a1


v1, v2 , v3 mean the values for respective features for that particular row.

When we are working for getting probabilty for a1 for a particular datapoint, we only bother with those datasets where class == a1
Because it is given that probabilty is a1


P(Xj=xj / y= ai):     Count of training data(Xj=xj and y=a1)/ Count of training data (y=a1)


Here this will be 1/3.

We have 3 data tows where class is a1.

Only 1 of these datarows can have a particular desired entry.



////////////////////////////////////////////////////////////////////////


Ex:

Salary                            loan application
High		      Yes
High		      Yes
Mid		      Yes
High                                  Yes
Mid                                   Yes
Low                                   Yes
Low                                   No
Mid                                    No
Low                                   No
High                                   No



P( Salary = High / y= No)

First we will only look at applications with y=No

Low                                   No
Mid                                    No
Low                                   No
High                                   No



Probability will be  1/ 4  



P(Salary = High/ y= Yes)

We will look at application with yp Yes

High		      Yes
High		      Yes
Mid		      Yes
High                                  Yes
Mid                                   Yes
Low                                   Yes


P= 3/6 = 1/2


///////////////////////////////////////////////////////////////////////////////////////


How do we write code for this to get final probabilities??


Given we will have y=ai.

First lets mantain a dictionary.

The dictionary at top level contains the classes 


So dict [a1]
	[a2]
	[a3]
	..
.. and so on


Then within these dictionaries keys can be features. We can also store the total count with class=ai

dict [a1] ---->    [X1]
	          [X2]
	          [X3]
		..
		..

		and so on
	    total_count

Within x1 we need the possible values x1 can take

X1 ----- >  High
	Medium
	Low

For each value we will mantain the count for each value


High ------------>   count


//////////////////////////////////////////////////////////////////////

Now to get  P(Xj=xj/ y=ai)

Go to dict[ai][Xj][xj] /   dict [ai][total_count]


Xj means which feature we are talking about. xj means what is the value of that feature.


///////////////////////////////////////////////////////////////////////


======================================================================


How to handle zero Probabilities:


Salary                            loan application
High		      Yes
High		      Yes
Mid		      Yes
High                                  Yes
Mid                                   Yes
Low                                   Yes
Low                                   No
Mid                                    No
Low                                   No
Low                                  No


Here P(salary = High / y= No):

Low                                   No
Mid                                    No
Low                                   No
Low                                  No


0/4


Here probability is 0 for this case


This effectively means that we are saying there is no way that if salary is High it is not approved.

This sounds like a very bad classification error.

We don't want this.

To avoid this we generally correct the probability values.

We will add little extra information.


P( Xj= xj/ y=ai)= Count (y=ai and Xj=xj) /  count(y=ai)

We want this prbability never becomes zero.

///////////////////////////////////////////////////
We will add a dummy +1

P( Xj= xj/ y=ai)= (Count (y=ai and Xj=xj) +1)/  count(y=ai)

The repurcusion to this is, this will not represent probability anymore. The sum of all won't be 1.


But we want the sums of this to approximately be equal to 1. So we add something to denominator as well.


P( Xj= xj/ y=ai)= (Count (y=ai and Xj=xj) +1)/  count(y=ai)  + |Xj|


|Xj| represents possible number of values Xj can take.

If Xj can take only values as l or h, we will add 2 to the denominator.

////////////////////////////////////////////////

Reasoning Xj has two possible values.

We would have added in 1 in two such expressions.

These are:
P(Xj=xj/ y=h)
P(Xj=xj/ y=l)


To make it even, we divide |Xj|, the total number of values Xj can take.

Eventually when we sum up the probabilities, they will cancel each other out.

This correction is called Laplace correction and its very important to implement this

/////////////////////////////////////////////////////

====================================================================


Ques. Laplace Outlook:

Consider the data-set as shown in the figure. What is the best probability that should be used for the statement-” given that there is no play, what is the probability that the Outlook is rain ” when using Naive Bayes with laplace correction. ?


Probability:  2/5

Adding Laplace Correction:   2+1/5+3


============================================================================

Implementing of Naive Bayes:

P(Xj=xj / y=ai) * P(y=ai)


We will create a dictionary

dict
[a1]
[a2]
[a3]



For each feature we will store, what all possible values this feature can take.

[a1]   ->   [j ]  ->   [xk 1]
	             [xk 2]
	             [xk 3]

	            .....
   	            .......

	            [xk n]

We just want to store the count corresponding to each value

[a1]   ->   [j ]  ->   [xk 1]  ---------->   count
	             [xk 2] ------------> count
	             [xk 3]

	            .....
   	            .......

	            [xk n]



If we have this, for any P(Xj =xj  / y=ai)

We can just get the count using dict


P(y=ai) =  Total training data in ai / Total training data



//////////////////////////////////////////////////////////////////////////////////


Code:

def fit(X_train, Y_train):
    result= {}
    
    # Getting the top level keys, the unique classes possible
    class_values= set(Y_train)
    
    # Storing the total data points for later use
    result["total_data"]=len(Y_train)
    for current_class in class_values:
        result[current_class]={}
        
        # Getting those datapoints where class is the current class
        X_train_curent= X_train[Y_train==current_class]
        Y_train_current=Y_train[Y_train==current_class]
        
        # Getting N, number of features
        num_feautures=X_train.shape[1]
        
        # Storing total count of data points for current class 
        result[current_class]["Total_count"]=len(Y_train_current)
        
        for j in range(1,num_feautures+1):
            result[current_class][j]={}
            
            # All possible values for current feature
            all_possible_values=set(x_train[:,j])
            
            
            # Storing the count for each possible value of j
            for current_value in all_possible_values:
                result[current_class][j][current_value]=(X_train_curent[:,j]==current_value).sum()
                

/////////////////////////////////////////////////////////////////////


basically we have 3 levels.

The first level is the different possible classes.

For each class we have N feautures. That is the second level.

For each feauture, we can have some unique values. That is the third level. Storing the count of each unique value.

//////////////////////////////////////////////////////////////////////////////////////

===========================================================================================




When probabilities are multiplied it leads to very small numbers.
This is because probabilities will usually be b/w 0 and 1.
Multiplication of these will lead to very small numbers.

So what we have is:

p' multiply(p() )


We can take a log of these:

log(p' multiply(p() ))

logp'+logp1+logp2+..........................

This will not lead to very small numbers.
Also we know that if x>y then log x> log y.

So we can still perform comparisons in the same way.


//////////////////////////////////////////////////////////////////////////////////////////


=====================================================================


Convert Iris Continuous values to labelled:

first value------------------m1-----------------------middle--------------------------m2------------------------- last value


m1 is the midpt b/w first value and middle.
Simmilar case with m2.

m1 is the first change point, mean the second change point, m2 the third change point


first value to  m1 =  0
m1 to middle = 1
middle to m2= 2
m2 to last value = 3


Code:

def makeLabelledData(column):
    second_limit=column.mean()
    
    first_limit= 0.5* second_limit
    third_limit=1.5*second_limit
    
    for i in range(0,len(column)):
        if column[i]<first_limit:
            column[i]=0
            
        elif column[i] <second_limit:
            column[i]=1
        
        elif column[i] < third_limit:
            column[i]=2
        
        else:
            column[i]=3
            
    return(column)

/////////////////////////////////////////////////////////////////////////////////////////////



=================================================================

Finding probabilities for continuous data:


We loose a lot of data if we do labelling.

We want to avoid this.

/////////////////////////////////////////////////////////////////


Problems with Continuous data
How do we calculate P(Xj=xj/y=ai) for continuous data??

We want to get to a probability distribution with which we can estimate Xj.


We will assume the probability distribution to be Gaussian. Basically a Normal Distribution Curve. Other functions can be used to estimate the distribution of the data, but the Gaussian (or Normal distribution) is the easiest to work with because you only need to estimate the mean and the standard deviation from your training data.


So for P(xi,y) =(  1/ sqrt(2*pi*(sigma**2))  ) * e**(-(xi-mu)**2  /   2 * (sigma**2) )  

Where 
sigma**2= variance
mu= mean


///////////////////////////////////////////////////////////////////////


So essentially what happens:

For all training data, we split training data on classes.

Within each class, for each feature we get the mean and standard deviation.

With this we have the ability to get the probability for any possible value of xi. Essentially we have the probability of X=xi given y=ai



////////////////////////////////////////////////////////////////////////////


We can Gaussian Naive Baeyes  classifier of sklearn

Importing the library:

from sklearn.naive_bayes import GaussianNB

Also make sure to implement Gauassian NB to work with continous data

//////////////////////////////////////////////////////////////////////////////////


=========================================================================================


Text Classification using Naive Bayes:

Naive Bayes has a great usage for text classification


Ex:

Given a text , is it spam or not.


///////////////////////////////////////////////////////////////


We will build a vocabulary.
We want our classifier to focus on these.

We may have to for go some stop words. Stop words are common words like a, the, that etc.


/////////////////////////////////////////////////////////////

Suppose we have n words say w1 to wn.

These are our features.

For each datapoint we will have frequency of that word/ datapoint

The feature data is not discrete as we can have any possible value for count.

The feature data is not continuous as well. It doesn't follow Gaussian distribution as well.


//////////////////////////////////////////////////////////////////////////////////////////////////////


Suppose we have to get P(Cricket/y=a1)


For all datapoints with y=a1, let's get count of a1.


P(Cricket/y=a1) =   Count(cricket in a1 docs)/Count(total words in a1 docs)

Can also be represented as:

Na1Cricket / Na1

Na1Cricket: Number of times cricket appears in a1 docs. a1 is in subscript, Cricket is in superscript
Na1: Number of elements in a1


This is also called Multinomial Naive Bayes. Here each feature can take many values but they don't follow Gaussian Distribution. They are not continuous in nature. 
Word count isn't continuous as it can't take decimal values. Gaussian Distribution doesn't work because of this.


If cricket never shows in our training data, we again face a problem

We will have to do Laplace Correction here as well.


New Probability fn:

 (Count(cricket in a1 docs)+1)/(Count(total words in a1 docs) +n)


n: The size of the vocabulary


We have multinomial Naive Bayes implemented in sklearn. It works very well on Text Classification problems. It won't work with iris data 
which works well with Gaussian NB. Iris has continuous feautures.


//////////////////////////////////////////////////////////////////////////////////////////

====================================================



K-nn

K nearest neighbours

Very intuitive


To classify a datapoint into some class, we can find out its nearest neighbours.
We can find out what datapoints are closest to this testing data.


//////////////////////////////////////////


Suppose K=1
We can find the 1 nearest neighbour for this datapoint and assign the same class to the datapoint as the neighbour 

/////////////////////////////////////////////////////////////////////////////

What knn says:

Knn says that lets get the k nearest neighbours of a datapoint and depending on these neighbours decide on the class of datapoints
Majority vote is taken.

//////////////////////////////////////////////////////////////////////////////////


1 nearest neighbour is very prone to outliers and overfitting hence we take Knn instead of 1.


Also k is taken as odd so that we can easily get the majority.

////////////////////////////////////////////////////////////////////////////////

Few things to take in consideration:

1) Distance:  

	a) Manhatten   :       sum   |     x1i- x2i|
			i= 1 to n

	Basically sum of feature wise substraction of datapoints

	b) Euclidean:  sum   sqrt( ( x1i- x2i)**2)
			i= 1 to n


2) How the neighbours will vote??
Do all neighbours get the same vote or the weightage of the vote depends on the distance.

For example we can take the vote as = 1/distance.
So if something is at far away distance, their weightage will be less.


This is uniform voting vs weighted voting

=================================================================

K=1 leads to overfitting

///////////////////////////////////////////////////////////////////////////

KNN does more computation on test time than on train time?

True:

There is no explicit training phase in KNN . It just takes data as input in its training phase . All the actual work , that is calculation of distances,comparisons and taking out nearest k neighbors is done at testing phase when test data is available.



//////////////////////////////////////////////////////////////////////
=====================================================================


Feature scaling:

If we don't apply feature scaling,

suppose we have 2 features:

f1		f2

1500		1
1600		2
1000		3
		9


f1 will completely overpower f2.

1500 to 1600 not that big of jump compared to 1500. Its just increasing by 100/15 %
But going from 1 to 9 means increase of 800%. But this won't be that reflective if we don't do feature scaling.


//////////////////////////////////////////////////////////////////////////////////////////

===============================================================================


Knn in sklearn:

Importing:
from sklearn.neighbors import KNeighborsClassifier


//////////////////////////////////////////////////////


Code:

X_train, X_test, Y_train, Y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=0)
clf=KNeighborsClassifier()
clf.fit(X_train,Y_train)
clf.score(X_test,Y_test)


////////////////////////////////////////////////////////


Some default parameters in KNN classifier:

n_neighbours by default is 5
weights are uniform by default
metric is 'minkowski' by default

Minkowski distance:  (   sum (|x1i-x2i|**p) )  ** 1/p
		    i=1 to n

By default p is 2 in sklearn esentially meaning we are using euclidean distance. 

We can have our own distance function as well.


Algorithm = auto , decides most appropriate algo


n_jobs= If we want to use parallel jobs for neighbour search. By default -1, meaning all processors are used.
================================================================================


Cross Validation:

Very important technique used in many cases.

                             

Suppose we have an algo and we have a parameter which we want to adjust so that we can get the best results.

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////

One approach is is to see how different p affects the training accuracy. But this leads to overfitting.


Another approach to see how p affects the testing data. But this eventually means we are using testing data for training. This is not desired.

 
Third approach is to use cross validation.
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Cross validation:

Split the training data into K parts.

Train on K-1 parts and test 1 part. If we observe closely, we will see that we are still using training data only so its allright to go via this route.

We perform this for all the k subsets. Basically each part of this K subparts will once be used for testing. Other times it will be used for training.


////////////////////////////////////////////////////////////////////////////////


Example:

We have Knn classifier and want to get a p which gets us the best accuracy.

Approach: 
We take K as 10.

Initially assume p as 1.
Run cross validation, essentially we will run 10 times where each subpart is taken once for testing and other times its used for training.


To get a score, we can take the average of all the 10 scores that we get on running CV.


We can repeat the same for other values of p like 2,3,4 and so on.


Then we select the p which provides the best score.
//////////////////////////////////////////////////////////////////////////////////////////////////////

Implementing in sklearn:


Importing:
from sklearn.model_selection import cross_val_score


Using cross validation:

clf=LinearRegression()
cross_val_score(clf,xtrain,ytrain)


///////////////////////////////////////////////////////////////////////////////////////////
Parameters in cross_val_score:

estimator: The algorithm that we want to send for cross validation. Basically that object that implements fit. 

 x: The array like object, the data to fit.

y: The targets. By default this is none.

groups:   which group a particular datapoint belongs to. Basically to which particular subpart of the cv split a datapoint belongs to.

For ex if we have 3 cv splits and suppose data like this:

x1
x2
x3
x4

and if we want x1 to go to a, x2 to b, x3 to c and x4 to a 
we can pass a,b,c,a  as array to groups


cv: determines the cross validation splitting strategy.

Default None. Use 5- fold cross validation
int: to specify number of folds.

For int/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.



fit_params:  If the fit fn of our estimator takes some paramters we can pass them here.




/////////////////////////////////////////////////////////////////////////////////


We can pass Kfold object into cv paramter of cross_val_score to make it behave like we would want.
With this we can even create these subsets


Importing:

from sklearn.model_selection import KFold


Code:
cross_val_score(clf,xtrain,ytrain, cv=KFold(n_splits=4))


Paramters of Kfold:


n_splits: no of splits
shuffle:

Random_splits:


========================================================================

Ques
For k cross validation,smaller k value implies less variance.

True

===============================================================================

Finding optimal K:

How K affects the decision boundary:
If k is small , outliers define the decison boundary

Ex:	+ +  +	
	+   .    +
                   +  + +

Here because there is a dot class inside the plusses, if another datapoint is near this dot class instead of plusses, it will be predicted as dot only.So decision boundary will be very complicated depending on the distance b/w the datapoint and these classes.

If k is High, it won't matter who your neighbours are, we will always predict majority. So, it leads to underfitting.


So we need to find the optimal K.

We can't use testing data to get this.

We will need to use cross validation to find best K

///////////////////////////////////////////////////////////////////////////////

Code:

for i in range(1,26,2):
    clf=KNeighborsClassifier(n_neighbors=i)
    score=cross_val_score(clf,xtrain,ytrain)
    print(i,score.mean())
    x_axis.append(i)
    y_axis.append(score.mean())



plt.plot(x_axis,y_axis,'r--')
plt.grid()
plt.xticks(x_axis)
plt.show()



Explanation:
We are iterating over K=1 to 25 and skipping 2. We want neighbours to be always be odd , so that we can avoid a tie.
Each time a new classifier object is created.

We get cross validation score each time. We take its mean to get complete score of each run.
We can then simply plot it to see how CV score is changing.


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

=======================================================================================


Implement KNN:


In training: We won't do much.
In testing: We get a test point. We get distance of all the points from this. Get the nearest k elements. After that just do majority voting. We will work with uniform voting.


Importing some useful libraries:
from sklearn.metrics import accuracy_score   # Helps us to get the accuracy score for a classification


from collections import Counter   # Helps us to get the count of different elements

Ex:

c = Counter('abcdeabcdabcaba')     # count elements from a string

c.most_common(3)                # three most common elements
[('a', 5), ('b', 4), ('c', 3)]




///////////////////////////////////////////////////////////////////////////////////////


Code:

Pseudo Code:

1) In training we don't do anything.
2) For testing, we have 2 functions. One function predicts result for one point. The other function combines the result and returns the complete result.
3) For function which is predicting for a single point, we get the distance of all the points from testing datapoint. We get the k nearest neighbours for this datapoint.
4) We then simply get the majority using Counter and simply return that.


=======================================================================================


Curse of dimensionality:

Generally in most of classifiers, if we add dummy data accuracy increases.

Knn is different in this regard.

If we have lot of data in KNN, it can lead to noise and it leads to bad results.


//////////////////////////////////////////////////////////////////


If we have corelated data in the dataset, it also causes problem. It can lead to a feature counted twice and spoil the learning. The corelated data are effectively telling the same thing.

///////////////////////////////////////////////////////////////////////////////////////////

Some ways to sort this out:

1) Assign weights to our feautures: When we are adding we should assign some weights and not do simple additkion.



sum  wi * (x1i - x2i ) **2
i =1 to n


Weights ensure that difference b/w 2 features don't lead to same change in the score. It is instead controlled by weights.

How to decide the weights:
Randomly assign the weights. Depending on the results we can take our weights up or down for each feature.

We can even use Gradient Descent if we can come up with some cost fn




2) Feature Selection: Here we work with only few features and don't work with all the features.

Generally for KNN for Feature selection, we use backward elimination.

We loop over all the features. Find the accuracy/cross val score by keeping this feature, find the accuracy/cross val score by removing this feature.
If the accuracy improves, remove the feature.

Hopefully we keep some, we remove some and our accuracy increases.


These are two very standard ways of feature selection in KNN.


/////////////////////////////////////////////////////////////////////////////////////////////

Sometimes if we have two many dimensions it leads to bad results. This is called Curse of Dimensionality




==========================================================================


Ques, Feature management weights:

If we decide to give weights to manage our features in KNN and we have our data as shown below, what might possibly be the weights assigned to feature1 and feature2? There are other features also present in the data-set which are not shown for clarity purposes. Assume that weights vary between 0 and 100. Max weight is 100 and min weight is 0.

Ans:

95, 5
5, 95


Soln:

As we can notice that both the features are having similar values we give importance to only one feature and assign less weight to the other. Any one of the two features can be given higher weight.

==========================================================================================================

Handling Categorical Data:



Suppose we have labelled data.
Suppose we have a column say Yes, No


How can we find distance b/w two such points.

/////////////////////////////////////////////////////////////////////////////////
1) If we have Binary data, we simply use 0 and 1. With Binary data we mean only two possible values and not just 0 and 1.

2) Suppose we have a column having values as Red, Blue, Green.

If here we give 0,1 ,2 to Red, Blue and Green Respectively. What will happen is Red and Green will be very far and Blue and Green are close by
and Blue and Red are close by.

Here d(Red,Blue)= 1
d(Red,Green)=2
d(Blue, Green)=1

There is no evidence to say these distances are right.


If we had labels like Low, Mid, High then we could have give 0 to Low , 1 to Mid and 2 to High.
This is because we know that Low is very far from High compared to other comaprisons.


Essentially:

a) For labels that have some order(ordinal), we can simply give them values like 0,1,2 and so on

b) For labels where all have equal weightage(nominal), we handle differently.

Ex:
Red
Blue
Green

What we can do is convert these into 3 different columns


We can make columns to be:

Red?    Blue?     Green?




Red? means is the value red??
Same case with blue and green.

Basically each column will tell if this datapoint is Red, Green or Blue.  

For a datapoint only one column will have a 1, other columns for this datapoint will be 0.

Eventually each of these 3 columns become Binary.


Red          Blue        Green 
0                 1	0                                  This means datapoint is blue as Blue? is 1
0                 0                1	                   Simmilarly,    Green
1                  0                0                                 Red


/////////////////////////////////////////////////////////////////////////////////////

===========================================================================================

Other Algos for KNN:

How we have implemented KNN is using Brute Force.

We find distance of testing data with all the training data.


We can use other methods as well:


KD Trees

Ball Trees

Basic Intution is to build a Binary Search Tree to get the K nearest neighbours. 

When we come to the top, we will know kif we want to go to left or right.

Eventually we keep going down. When we reach the leaf we will have a few options.



/////////////////////////////////////////////////////////////////////////


A KD Tree(also called as K-Dimensional Tree) is a binary search tree where data in each node is a K-Dimensional point in space. In short, it is a space partitioning data structure for organising points in a K-Dimensional space. It is used in Nearest Neighbours algorithm. True or False?


//////////////////////////////////////////////////////////////////////////////

We are doing O(n) work on each data-point in our implemented brute force method. Till which of the following will KD-Tree reduce the work done on each data-point

O(log(n))

Explanation:

We try to cut the space (representation of features. 
2D space if only 2 features are present in a data-point) into halves and then just do the search on one of the halves.
As we do this for each node of the tree the height of the tree is O(log(n)). 
So, for searching, we will just need to traverse at most O(log(n)). This is similar to the case of Binary Search Tree.

///////////////////////////////////////////////////////////////////////////////////


===============================================================================

Pros and Cons of KNN:

Pros of KNN:

1) Easy to understand and code
2) Multiclass class classification works as well


Cons of KNN:

1) Testing time is very high. Its okay to have high training time but if testing time is high, the end user, using this application won't have a good time and will have to wait high ammounts of time to get the outputs.

2) If the training data is biased, that is one class has more data than other class than KNN ends up being biased as well.

3) High dimensionality/ too many attributes   leads to bad accuracy.


We can solve 2) and 3) points but 1) point is a very big problem



Neural Networks are converse of this. They take huge time to train but while testing they are very fast.

Its okay to take Huge time while training but for testing throughput should be high.


==============================================================================

SVM: Support Vector Machines

Classification Algo

One of the most powerful algo

///////////////////////////////////////////////////////////////////////////////

What happens in Logistic Regression:

Hypothesis Fn:    h theta (x) = 1 /   1+  e **(-theta T *x)

theta in h theta (x) is in subscript.
theta are the parameters

 When we draw hypothesis fn with respect to theta T * x

When theta T * x is 0, hypothesis is 0.5

if hypothesis is less than 0.5, then it is class 0, if hypothesis is greater than equal to 0.5, the class is 1.



Effectively:

if theta T *x >=0 , than class is 1

theta T* x<0 , class is 0.




//////////////////////////////////////////////////////////////////////


What SVM says:

Lets not just settle on 0. We should try to increase this margin.

For y=1, we should have  theta T x to be any other number instead of 0.

For y=1, we generally choose thetaT x>=1


For y=0, we want theta T x<=-1



We want the gap b/w 0 and 1 to be huge.

Still when theta T x =0.5 , we predict 1 but we want greater confidence when we make prediction about some class.


The only idea is if we have large margin we will be more confident.

We will change the cost function in such a way that this is ensured.



///////////////////////////////////////////////////////////////////////////////////////////////////////////////

================================================================================================
 Cost of Logistic Regression:
for y=1 , 

-log(h theta x)



h(theta x) =   1 / 1+ e**(-theta x)


For y=0,

-log(1 - h theta x)

log( 1  -  1 / 1+ e ** (-theta x))


/////////////////////////////////////////////////////////////////////////////////////

Plotting for  cost vs theta x

 
when y=1.

At theta x=0,   y=0.5

At theta x=  infinity,  y asymptotes to 0.    

At theta x = -infinity, y goes to infinity



when y=0

inverse of y=1

At theta x=0, y =0.5


////////////////////////////////////////////////////////////////


We don't want theat x >=0  for y=1. We want enough margin. We want it to be greater than equal to 1.


We will change this cost fn.

We want the cost to be 0 at theta x=1, from here on cost should remain 0.

Before that we want cost to be linear starting from infinity for theta x=-infinity.

It comes down linearly to reach 0 at theta x = 1



For y=0, we want cost to be 0 for theta x from infinity to 1.

From here cost increases linearly to go to infinity for theta x == infinity.


///////////////////////////////////////////////////////////////////////////////////////////


Now the cost fn will be a combination of the above two cost fns.

Logistic Regression cost fn:    -yi log(h theta (x)) - (1-yi)log(1-h theta(x)) + lamda/2* sum(theta**2)

For SVM:     yi*Cost1(theta*x ) + (1-yi) Cost 0 (theta *x)+ lambda/2*sum(theta **2)


For SVM generally we take lamda common:

1/ lamda * (yi*Cost1(theta*x ) + (1-yi) Cost 0 (theta *x))  + 1/2*sum(theta **2)


We take 1/ lamda as c.

//////////////////////////////////////////////////////////////////////////////////////
So, for SVM generally the cost fn is:

c* (yi*Cost1(theta*x ) + (1-yi) Cost 0 (theta *x))  + 1/2*sum(theta **2)

////////////////////////////////////////////////////////////////////////////////////////


===============================================================

Decision Boundary and c.


A good decision boundary is one which KEEPS GOOD GAP B/W  the different classes.

Two parts associated with cost fn:

c * ___________     + 1/2 sum(theta i**2)

The cost fn and regularization parameters.

How do we tweak them to get to best fit line??


////////////////////////////////////////////////////////////////////////////////


Basically we want that decision boundary should be far from classes.

So what we can say is:

thetaT *X >=1   for y=1

theta T * x= d *|theta| >=1

|theta| is just sqrt(theta**2)
d is the distance b/w closest point to decision boundary with decision boundary.


If d is less theta will have to be very high  so that theta T x remains greater than 1.


Same is the case with when y=0 and we want theta T x to remain less than -1.


////////////////////////////////////////////////////////////////


We want d to be as high as possible, the higher d is, the lower theta can get.



////////////////////////////////////////////////////////////////////////////////////


Sometimes we may have a situation if we focus on  misclassifying some classes or make decision boundries with less d and eventually higher theta.

Which boundary we select depends on c.


Can look into video to get better view on this.

/////////////////////////////////////////////////////////////////////////////////////////



SVM  from sklearn


importing svm:
from sklearn.svm import SVC


Displaying different classes with different color:
x=np.array([[1,1],[2,1],[1,2],[1.5,1.5],[3,4],[2,5],[4,3],[7,2],[3,5],[2,6],[6,2],[3,4],[4,4]])
y=[0,0,0,0,1,1,1,1,1,1,1,1,1]
x_x1=x[:,0]
x_x2=x[:,1]
plt.scatter(x_x1,x_x2,c=y)
plt.show()


Plotting after fitting:

svcLinear=SVC(kernel='linear',C=1).fit(x,y)
svcLinear.coef_,svcLinear.intercept_

x1=np.array([0,5])
x2=-1 * (svcLinear.intercept_+svcLinear.coef_[0][0]*x1)/svcLinear.coef_[0][1]
plt.plot(x1,x2)
plt.scatter(x_x1,x_x2,c=y)
plt.axis([0,8,0,8])
plt.show()


////////////////////////////////////////////////////////////////////////////////////////////////////

How to plot SVM decision boundary:

SVM cost fn looks like:  

theta1*f1+theta2*f2+theta3=0


When we do svcLinear.coef_,svcLinear.intercept_, we get the values of theta1, theta2 and theta3.

We simply take 2 dummy values of f1, we already know theta1, theta2 and theta3. 
We simply get the f2's . We will have two points for cost fn. We can simply plot this decision boundary using plot fn.

/////////////////////////////////////////////////////////////////////////////////////////////////////////


Observations of plotting:


Lot of points of one class which are closest to decision boundary  at    x1+x2=3


Lot of points of one class which are closest to decision boundary      x1+x2=7



And Decision boundary is exactly midway at     x1+x2=5



theta1 and theta2 are not 1, they are nearly 0.5



Observation2:

If we change one of the classes, the fit still remains the same.
This is because c value isn't that large.
 

Observation3:

iF C=100000,  boundary changes. Now the error part of cost fn is so huge that regularisation is completely shadowed.

Now the boundary is a vertical line making all classes on one side of decision boundary.


//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


========================================================================


Finding Non Linear Decision Boundaries:

Logistic Regression: We add dummy data. Issue here is which features to add for these dummy data.


SVM:  We can choose some landmark points. Say these landmark points are l1,l2 and l3.
If we get a point x. 

We can find how simmilar x is with these points.

f1 -> sim(x,l1)
f2 -> sim(x,l2)
f3-> sim(x,l3)


If we had n features originally, we take it to some other dimesion say n'

This new dimensions represents n' landmark points. We don't keep the initial features.
Get new landmark pts and simmilarity fns.

/////////////////////////////////////////////////////////////////////////////////////////////////////

Some questions??
How do we choose landmark points
What is the simmilarity fn

/////////////////////////////////////////////////////////////////////////////////////////////////////////

=============================================================================================

Choosing Landmark points:

We move from   R n  ->  R n'

In SVM:   m * n ->  m datapoints and n features.

We make all m training points as landmark points.

For an datapoint in R n dimension, we get simmilarity with all training data.

We get a vector of size m which contains simmilarity of datapoint with m training data.


Essentially, with this new vector we move to m featured set.


For each datapoint we move to m feature set.


/////////////////////////////////////////////////////////////////


This may seem very simmilar to knn but in knn distance is being used and in SVM simmilarity is used. 
This simmilarity is with all the datapoints and easily influences the class our datapoint belongs to.

unlike in KNN, in SVM how much far away is a data point from a landmark also matters.
In KNN only the class of nearest k elements matter, here the distance also matters.




It is very much faster than knn at testing time.
////////////////////////////////////////////////////////////////////////

===============================================================================

Various Simmilarity Functions:

Suppose landmark points are l1,l2,l3,l4

We have a datapoint x having 2 dimensions.


To move to new feature set, we have   

fi =  sim(x,li)

Here the meaning of this expression is that ith feature is simmilarity b/w a datapoint and ith landmark point.

///////////////////////////////////////////////////////////////////////////////////////


Initially we have xi datapoint as 

xi  -> Rn              -------------------------------->           xi -> Rm

The above expression means we are in  n  feauture set.

Using simmilarity fn, we then move to m feature set.

//////////////////////////////////////////////////////////////////////////////
 Also we assume that m > n and hence we end up moving to higher dimension set.

If this is not the case, then we don't use this technique.


////////////////////////////////////////////////////////////////////////////////////


Options for similarity fns:

1) Linear Kernel:  We stay in the same dimensionality.    
If we are in R n , we stay in Rn . The theta that we are finding are dependant on the old feature set only.


theta 0 +theta 1*x1+theta 2*x2............................theta n * xn

When to use??
a) m < n
b) m is huge.   If we move to such a dimension, the running time can increase. Hence we don't use the kernel trick at all.



2) Gaussian Kernel:   sim(x,li)   =   e  ** (    (- |x-li|**2  )    /     2 * (sigma**2))

sigma is a paramter that we can control.

if x==li, than we have e**0  = 1.  signifying x and li are completely same.

if x and li are far, then     |x-li|**2 will be huge,     e ** -infinity  is nearly 0

Gaussian Kernal gives values b/w 0 and 1. If our datapoint is very near to a landmark point, simmilarity is 1 and if they are far apart, simmilarity is 0.


When to use??
When m > n.
if m is huge. Upto 50k we use Linear.


Gaussian Kernal is also called rbf kernal



3)  Polynomial Kernal:

poly(x, li)  =   (xT li + a) **b

We can choose a and we can choose b.

Most of the times we use either linear or rbf kernal.

////////////////////////////////////////////////////////////////////////////////////////////////////

========================================================================


More about Gaussian Simmilarity Kernel:


sim(x,li)  = e ** (- |xi - li|**2  /    2 * sigma **2)


If we draw this ,   

if xi==li, simmilarity is high.
if xi and li are far, simmilarity is low.


This   sim  vs  li curve is a Normal distribution curve

Max is 1 and Min is 0.

//////////////////////////////////////////////////////////////////////////////////////////////


sim(x,li)  = e ** (- |xi - li|**2  /    2 * sigma **2)

Sometimes in some packages    1/ 2*    sigma **2    is treated as lambda.  lambda is just the inverse of sigma **2


/////////////////////////////////////////////////////////////////////////////////////////////////

Effect of sigma on the curve:


As sigma**2 increases, the effect of moving away from the max of curve will be far less. Effectively the curve will become smoother and flatter without any prominent peak.


if sigma**2 is 1, if we move by 1 unit from max we have covered 1 standard deviation

if sigma**2 is 10, if we move by 10 units from max, then we would have covered 1 standard deviation.


The spread is much higher in the second case.

In second case we have made denominator a large number. To make simmilar effect on complete division, we would have to change numerator by a large number as well.


////////////////////////////////////////////////////////////////////////////////////////////


lambda has opposite effect considering lambda = 1/2*  sigma **2



///////////////////////////////////////////////////////////////////////////////////////////////////////////


What are the repurcuison of sigma **2??


Suppose we have three landmark points l1, l2 and l3.

If sigma**2 is large, li will have higher impact on far away points as well.

If sigma**2 is small, li will only impact near points.


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////

If sigma**2 is small, it leads to overfitting as a landmark point doesn't impact far away points. For a datapoint we will have a say from very less datapoints.


If sigma**2 is very large, it leads to underfitting.
////////////////////////////////////////////////////////////////////////////////////////////////////////


======================================================================================================================



How to move to new dimensions??


Input data   X.

We will choose kernel.


For each xi in X:
xi belongs to R n

We want to move xi' to R m.
///////////////////////////////////////////////////////////


xi'  looks like        sim(xi,l)
	             sim(xi,l2)
                               sim(xi,l3)
		.
 		.
		.
	             sim(xi,lm)


Here one of the landmark points will be xi itself as all the landmark points are training data itself.

So  sim(xi,xi) will be 1.

//////////////////////////////////////////////////////////////////

We should observe that


Training data:

All diagonal entries will be 1. This is because one of the landmark points will be xi itself as all the landmark points are training data itself.


1
	1
		1
			1



Testing data:
Simmilarly we will apply for testing data but its not necessary that diagonal entries are 1


Simmilar operations are done.

x test  

xi  ->  R n
xi' -> Rm


sim(xi,l1)
sim(xi,l2)

.
.
.



where l1 is the first point of training data.

landmarks are training datapoints irrespective of if we are converting training data or testing data.
//////////////////////////////////////////////////////////////////////////////////////////

===============================================================================


Multiclass classification:



How SVM handle multiclass classification.


y ->   a     b     c

///////////////////////////////////////////////////////////////////////////

One vs Rest

If we have k classes, we train k different models.
1st classifier trains   1st class    vs   rest
2nd classifier does  1st class  vs rest 

and so on.


Ex:  Doing for 3:

a vs rest  ------------->   thetaa T * x  = 1.5
b vs rest --------------> thetab T *x =  -0.6
c vs rest-----------------> thetac T *x=  0.1

Choose one with the max theta T *x and predict that class.


///////////////////////////////////////////////////////////////////////////////////////////////////


2nd option 


one vs one

We builld   K C 2 classifiers with all the possible combinations of two classes.

Ex: 3 classifiers
Once we do a vs b,  b vs c, c vs a.

To get the final prediction, go through all the classifiers and give the majority prediction from all the classifiers.

Here suppose for a datapoint:
a vs b: b
b vs c: b
c vs a: a

Predict b


This is very expensive.

///////////////////////////////////////////////////////////////////////////////////////////


===============================================================================



SVM on iris dataset

iris=datasets.load_iris()
x=iris.data
y=iris.target


x_train,x_test,y_train,y_test= train_test_split(x,y)


clf= svm.SVC()
clf.fit(x_train,y_train)


///////////////////////////////////////////////////////////


Parameters in SVC:

C=1.0,  the inverse regularization paramter.   If c is high we have overfitting, if C is low we have underfitting.
class_weight= If we wanna give some weight to the classes. By default None meaning equal weightage for all.

coeff0= used in case of Polynomial Kernel. This is the a parametrt.

gamma=   1/ 2*   sigma**2 .      if gamma is too high we will have overfitting, if gamma is too low we will have underfitting.


kernel:  By default rbf



SVC uses one vs one scheme.

We have another parameter decision_function_shape.

decision_function_shape : {'ovo', 'ovr'}, default='ovr'
    Whether to return a one-vs-rest ('ovr') decision function of shape
    (n_samples, n_classes) as all other classifiers, or the original
    one-vs-one ('ovo') decision function of libsvm which has shape
    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
    ('ovo') is always used as multi-class strategy. The parameter is
    ignored for binary classification.


We can see that ovo will always be used but we have an option of how the fn is returned.

//////////////////////////////////////////////////////////////////////////////////////////


We have multiple implementation of Support Vectors in sklearn.

We also have LinearSVC. This uses Linear Kernel. Also, this implements One vs Rest scheme.



/////////////////////////////////////////////////////////////////////////////////////////////////

Plotting decision boundary

We create a grid. We will consider all the points/pixels on the grid. The distance b/w One pixel from another is 0.02 units.
We predict classes for these points/pixels.  We plot these points and make different classes have different colors using the c paramter.


For visualization, we can only use 2 attributes.





Code:

def makegrid(x1,x2, h=0.02):
    x1_min,x1_max=x1.min()-1,x1.max()+1
    x2_min,x2_max=x2.min()-1,x2.max()+1
    
    a=np.arange(x1_min,x1_max,h)
    b=np.arange(x2_min,x2_max,h)
    
    xx,yy= np.meshgrid(a,b)
    return(xx,yy)




Explanation:

np.meshgrid(a,b)

Replicates a multiple times and b multiple times.


Essenially if we have   1,2,3  and 4,5,6 

We want

1,4    2,4    3,4
1,5    2,5    3,5
1,6     2,6    3,6

Essentially we want to replicated 1 multiple times , 2 multiple times and so on.




iris=datasets.load_iris()
x=iris.data[:,0:2]                            #  only taking 2 columns as it gives us easy visualization
y=iris.target
x_train,x_test,y_train,y_test= train_test_split(x,y)
clf= svm.SVC()
clf.fit(x_train,y_train)




xx,yy=makegrid(x[:,0],x[:,1])
predictions=clf.predict(np.c_[xx.ravel(),yy.ravel()])
plt.scatter(xx.ravel(),yy.ravel(),c=predictions)
plt.show()



Explanation:

.ravel:    Returns a flattened array

np.c_ : Makes pairs

Essentially we create xi, yi using np.c_ amd then feed it to the model to make prediction

////////////////////////////////////////////////////////////////////


We can use other kernels and fiddle with paramters to see how fitting changes.


/////////////////////////////////////////////////////////////////////////////////////////////////




=========================================================================================




Choosing Paramters using Grid Search:




Grid Search: We can use SVM, but we can use many values of C and gamma. How to get to the best possible values.

Choosing best values of c and gamma is a part of training process and as such it should use training dataset.

We can use cross validation.


//////////////////////////////////////////////////////////////////////////////

Cross validation is used for a single value.


For c we may have values from 1 , 10, 100 and so on
For gamma we may have values from 1/1000, 1/100, 1/10 and so on.


We may wanna try all the possible combinations of gamma and c.
This is just like iterating in a grid and as such is called grid search.

There is a class available in sklearn to use grid search.



////////////////////////////////////////////////////////////////////////


Code:

Importing the library:
from sklearn.model_selection import GridSearchCV



Implementing:

clf= KNeighborsClassifier()

grid={"n_neighbors":[3,5,6,7,9]}

abc= GridSearchCV(clf,grid)
# This creates a Grid CV object. It needs two arguements. The estimator and grid. grid is a dictionary where keys are names of 
parameters and values are list of possible values for a particular parameter.


abc.fit(x_train,y_train)            # Fitting on the gridSearch object


abc.best_estimator_       # Returns the best possible value of the paramter we are trying to hypertune.


abc.cv_results_          # Show us the different results



//////////////////////////////////////////////////////////////////////


Doing the same for SVM:

clf= svm.SVC()
grid={'C':[1e2,1e3,5e4,1e5],   'gamma':[1e-4, 5e-4,1e-4, 5e-3]}

abc=GridSearchCV(clf,grid)
abc.fit(x_train,y_train)

abc.best_estimator_


////////////////////////////////////////////////////////////////////////////

1e2 means 100, 1e3 mean 1000, 1e-4 means 0.0001, 5e2 means 500

///////////////////////////////////////////////////////////////////////////



========================================================================================


Using SVM for regression:

Most of the maths remains the same.

Only some changes.


*****************
The line/ Decision Boundary that was earlier used to seperate two or more classes will be used to give predictions

************************





We use Boston Dataset for working with this.

Importing

Earlier we were using   svm.SVC   

Now we will use svm.SVR



=========================================================================


PCA:


Intuition behind PCA:

pRINCIPAL Component Analaysis.


We want to reduce the dimensionality.


If we have 1000 features there is a chance that they are corelated to each other.
They maybe representing same kind of data.

Is there a way to remove these features and still keep majority of data in dataset.

//////////////////////////////////////////////////////////////////////


Suppose we have data in a  2d plane. 

If we are able to fit a line that explain these datapoints, what we can do is  draw this line and a perpendicular line to this.

This line can be are x1 '  and perpendicular line can be y1'.  We can explain all the data using x1' only and we won't need to use y1'.

Majority changes will be with respect to x1' and only minor changes will be there with respect to y1'.

So essentially using x1' we can explain the data using one dimension only.

///////////////////////////////////////////////////////////////////////

What we can essentially say is:

x1' is the direction which explains our data well.

x1' has the maximum variance.


///////////////////////////////////////////////////////////////////////////////////////


What essentially we try to achieve is if we 1000 features say x1,x2,x3.................x1000, we don't want to eliminate 
features. We look for new features which explains our data really really well.

////////////////////////////////////////////////////////////////////////////////////////////



We may be loosing some data but there are other benifits to this.


Benifits:

1) Speed: less data, fast training
2) Memory: Need less memory to store less features
3) Visualization: We may use PCA to reduce our data to 2d or 3d so that we can visualize our data.



//////////////////////////////////////////////////////////////////////////////////////////////////


Few things to consider while using PCA:

When we only pick x1' and loose y1' , we only use values along/of  x1' and we loose the values along y1'.
We are loosing information but hopefully the information lost is not much.

We try to loose the information lost. Information lost is the sum of perpendicular distance from  datapoints to x1' for all datapoints.


When going from 3d to 2d it might seem we are loosing too much data, but if we have 1000 features and we try to reduce the dimensions to 100,
the data loss may not be that great.

////////////////////////////////////////////////////////////////////////////////////////////////////////


This looks like Linear Regression but it is not linear regression.

Linear Regression is supervised.

We predict y given x.

In PCA, we are not looking at y.

We look at x and try to reduce its features.



In case of Linear Regression we try to minimize the difference b/w y values. That is y predicted and y true.

In case of PCA we try to minimize the perpendicular distance b/w datapoints and x1's
 

/////////////////////////////////////////////////////////////////////////////////////////


Feature scaling:

We have one data that takes values b/w 1000 and 2000.

We have other feature that takes values b/w 0 to 1.


Feature 1 will completely outweigh Feature 2 and we will just end up with Feature 1. The variance will be higher in Feature 1.


But a change of 0.5 may be huge compared to a change of even, 1000 in Feature 1.

Hence we should apply feature scaling before doing PCA


////////////////////////////////////////////////////////////////////


Ques??

Principal Component Analysis algorithm affects the labels in case of classification problems. True/False?

False


////////////////////////////////////////////////////


=================================================================================


Applying PCA to 2d data

We will be using sklearn here.



Importing PCA:
from sklearn.decomposition import PCA



We are taking a dummy data and we will apply PCA here.



np.c_[x1,x2]:   Combine x1 and x2 in pairs and gives us a 2d array.

Ex:
 np.c_[np.array([1,2,3]), np.array([4,5,6])]
array([[1, 4],
       [2, 5],
       [3, 6]])




Code:
x1= np.array([1,2,3,4,5,6])
x2=np.array([7.5,11,16,18,20,26])


X=np.c_[x1,x2]    # This will be fed to PCA object

pca=PCA()
x_transformed=pca.fit_transform(X)    # Transforms the data using PCA


x_transformed

array([[-9.26049952e+00,  9.65263430e-03],
       [-5.62047228e+00,  2.38560598e-02],
       [-5.36562171e-01, -3.68391297e-01],
       [ 1.65958219e+00,  5.22629108e-02],
       [ 3.85572654e+00,  4.72917119e-01],
       [ 9.90222524e+00, -1.90297426e-01]])


We see over here that data in 1st column is way larger than data in second column. 

So this means that we majorly have variation along 1st column and that will be kept. 2nd column isn't storing that much information.



pca.components_           #  ndarray of shape (n_components, n_features)   Principal axes in feature space, representing the directions of
                                            #maximum variance in the data. The components are sorted by
                                           #``explained_variance_``.

Output:
array([[ 0.27096719,  0.96258858],
       [ 0.96258858, -0.27096719]])


0.27096 , 0.9625     is the unit vector for first direction

0.96258858, -0.27096719     is the unit vector for second direction.


One of them will be x1' and one will be y1'. We will keep data along one of the axes.

Also here:  The components are sorted by ``explained_variance_``. This is according to documentation. So first one is more important than second one.



/////////////////////////////////////////////////////////////////////////

Moving data to a particular dimension.


Here we moved the data from 2d to 2d.


To move the data from 2d to 1d we do:


pca=PCA(n_components=1)
pca.fit(X)
x_transformed=pca.transform(X)

x_transformed


Output:
array([[-9.26049952],
       [-5.62047228],
       [-0.53656217],
       [ 1.65958219],
       [ 3.85572654],
       [ 9.90222524]])


Now we only have 1 feature



We can experiment with number of components to get to best possible num_components

/////////////////////////////////////////////////////////////////////////////////////////////////////////////

Reversing data back to original data

PCA converts our data along the new dimensions.

inverse_transform reverses the data back.

Code:

pca.inverse_transform(x_transformed)


Output:

array([[ 0.99070848,  7.50261555],
       [ 1.97703643, 11.00646421],
       [ 3.35460926, 15.90017805],
       [ 3.94969232, 18.01416153],
       [ 4.54477538, 20.12814502],
       [ 6.18317813, 25.94843564]])


We have lost some data as num_components was 1 and we decided to let go of one dimension.
To get original data back num_components should be equal to original features

We may have data in a  way where  we may loose a lot of data as well. In this example this loss wasn't that great.

We will see in later videos how to approach this.

================================================================================================

Applying PCA on 3d:

First of all we will generate our 3d data.

Code:
np.random.seed(1234212) # Seeting up a seed so that always the same data is generated

mean_vec1= np.array([0,0,0])   # Mean for the different features
cov_mat1=np.array([[1,0,0],[0,1,0],[0,0,1]])   # covarience b/w the three features
class1=np.random.multivariate_normal(mean_vec1,cov_mat1,100)   # generates data using normal distribution


mean_vec2= np.array([1,1,1])
cov_mat2=np.array([[1,0,0],[0,1,0],[0,0,1]])
class2=np.random.multivariate_normal(mean_vec2,cov_mat2,100)


Plotting in 3d :

Importing Libraries:


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D, proj3d


Code:

fig=plt.figure(figsize=(8,8))            # Adds a figure to the canvas
ax=fig.add_subplot(111,projection='3d')   # Adds a subplot to a a figure. 111 means this figure will have 1 row, 1 column and this is the first posn that we are looking to fill.   443  would mean we have 4 rows, 4 cols and we want to fill posn 3.
ax.plot(class1[:,0], class1[:,1], class1[:,2],'o')    # Plot fn for 3d, we are providing x,y,z
ax.plot(class2[:,0], class2[:,1],class2[:,2],'^')       # Plot fn for 3d, we are providing x,y,x
plt.show()



/////////////////////////////////////////////////


Moving 3d to 2d


if num_components not passed it stays in same dimensions but just moves to new set of  axes.

all_data=np.concatenate((class1,class2))
pca=PCA(n_components)
transformed_data=pca.fit_transform(all_data)
transformed_data


Plotting in 2d:

plt.scatter(transformed_data[0:100,0],transformed_data[0:100,1], c='r')           # First 100 datapoints were from class1 
plt.scatter(transformed_data[100:200,0],transformed_data[100:200,1], c='b')   # Last 100 datapoints were from class 2
plt.show()



//////////////////////////////////////////////////////////////////////////////////////////

In 2d to 1d we saw that when we moved our data back from 2d to 1d, all our data was lying on a single line.

Here when we move back from 3d to 2d, our whole data will be lying on a single plane.

It may not be that apparent but it does lie on a plane.

The eqn of the plane may be something like:     ax1+bx2+cx3=1

We can solve this eqn using 3 values from transformed data and get to know the values of a , b and c.

For all points in transformed data, they will satisfy this transformed data


////////////////////////////////////////////////////////////////////////////////////////////////////////

===============================================================================================


Ques. Grid posn:


For the code shown below which of the following is true


fig=plt.figure(figsize=(8,8))
ax=fig.add_subplot(234, prohjection='3d')


Output:

Grid looks like

1	2	3
4	5	6

Clearly 4th index is:
2nd row and 1st col


///////////////////////////////////////////////////////////////////////////////////////////

=================================================================================


Analysis on Breast Cancer:

PCA on breast Cancer

We run Logistic Regression on Breast Cancer Dataset. Get the accuracy and running time.

Then we use PCA to reduce the dimensionality and see how accuracy and time are affected.


We apply PCA only on training data.

/////////////////////////////////////////////////////////////////////////////////////////

To get time taken b/w two operations we use time module:


start=time.time()

# ops

end= time.time()


time_taken=end-start




Depending on the processes running on our system at any point time_taken for same ops can vary a lot as well.
//////////////////////////////////////////////////////////////////////////////////


Code to see how accuracy and time are being affected after PCA


Without PCA:

start=time.time()
lr.fit(x_train,y_train)
end=time.time()
print(end-start)
print(lr.score(x_test,y_test))


With PCA:

lr=LogisticRegression()
start=time.time()
lr.fit(x_train_pca,y_train)
end=time.time()
print(end-start)
print(lr.score(x_test_pca,y_test))




////////////////////////////////////////////////////////////////////////////////////////////

=======================================================================================


Math behind PCA:



What PCA says is it requires direction in which the data changes the maximum or in other words, the direction in which variance is maximum.
We don't have much information in the direction where data doesn't changes that much.


So for finding these principal components we do:
1) Find covariance matrix. How is f1 covarieng with other different features.
So if we m*n data, covariance matrix will be n*n

    f1  f2   f3   f4 ...................
f1
f2
f3
f4
.
.
.

2) We need to find eigen values and eigen vectors.

Main eigen vector is the vector in  direction having max variance.

Second eigen vector is the second most variance explaining vector.

and so on

First Eigen value tells us how much variance is explained by the first direction.

Second Eigen value tells us how much variance is explained by second direction.

and so on.

To find these eigen vectors we apply SVD( Single value decomposition) on covariance matrix.


If eigen value of a direction is very less we can let go of that direction/component.


in sklearn we saw pca.components_ , they represent the eigen vectors.



We also have another attribute called pca.explained_variance_ , which tells how much variance a particular component is explaining.


//////////////////////////////////////////////////////////////////////


=================================================================================================

Ques. covariance symmetric

Covariance matrix is not necessarily a symmetric matrix.


False


////////////////////////////////////////////////////////


Ques. Eigen values and vectors

SVD is applied on Covariance matrix to obtain eigenvalues and eigenvectors. Which of the following is true about it.

Eigenvectors represent the dimensions and eigenvalues are directly proportional to the co-variance in that direction.


///////////////////////////////////////////////////////////
================================================================================================


Let's code our own PCA:

To get covariance matrix we can use np.covariance

covariance: covariances indicate how two features vary together.


////////////////////////////////////////////////////////////////////////////////////////
By default np.cov calculates covariance along the rows.

If we directly feed the data into np.cov, covariances of different data points with one another will be calculated.

To counter we can take transpose of the data and then feed to np.cov function.


To get transpose of a numpy array we can use: np_array.T. We can also us ethe transpose function.

/////////////////////////////////////////////////////////////////////////////////////


To get eigen vectors we use:


np.linalg.eig:   Computes eigen values and eigen vectors of a square array



Returns:

w : (..., M) array
    The eigenvalues, each repeated according to its multiplicity.
    The eigenvalues are not necessarily ordered. The resulting
    array will be of complex type, unless the imaginary part is
    zero in which case it will be cast to a real type. When `a`
    is real the resulting eigenvalues will be real (0 imaginary
    part) or occur in conjugate pairs

v : (..., M, M) array
    The normalized (unit "length") eigenvectors, such that the
    column ``v[:,i]`` is the eigenvector corresponding to the
    eigenvalue ``w[i]``.



////////////////////////////////////////////////////////////////////////////////////////////

Code:
all_data=all_data.T

cov=np.cov(all_data)

np.linalg.eig(cov)

# The first array are the eigen values and the the second array corresponds to eigen vectors  

# The eigen vectors are along the columns instead of rows.
# So here first eigen vector is

# -0.65571735   -0.52641337   -0.54122427

eig_values,eig_vectors=np.linalg.eig(cov)

# As eigen values aren't sorted, we need to sort them first to get top eigen vectors


eig_value_vector_pair=[]

for i in range(len(eig_values)):
    eig_vec= eig_vectors[:,i]   # The eigen vectors are column wise and not row wise
    eig_value_vector_pair.append([eig_values[i],eig_vec])


# Sorting according to eigen values

eig_value_vector_pair=sorted(eig_value_vector_pair,reverse=True)


# Now we can pick the top eigen values and corresponding to them we will have our principal components


# We can see that pca.components_ of sklearn pca is exactly simmilar to our eigen vectors
# We can also see the same for eigen values.
# Sometimes they can be different as well.
# The only way these can be different is if we pick one direction as our Principal componente
# whereas PCA is picking exactly oppisite direction. In any case both directions will be 
# explaining the data in a simmilar way only


/////////////////////////////////////////////////////////////////////////////////////////////////////////

==========================================================================================


Ques  PCA direction

A direction D1 defines a particular data-set nicely in one dimension. There will exist another direction which defines this data-set equally well?


True

Explanation:

Direction D1 and the direction directly opposite to D1, both will define the data-set equally well.



==========================================================================================


Finding optimal number of components:

How ro choose num_components??


Suppose we have 30 features.

Corresponding to that we can get 30 eigen vectors and 30 eigen values.

We can get the total of eigen values.
This total will represent total variance and in a way total information regarding the dataset.

For each eigen value we can get :

eigen_value_i / total  ,which represent percentage of variance we mantain if we keep this feature.

Suppose we are mantaining k features, the %age of variance mantained will be:

(eigen_value_1+eigen_value_2+........................eigen_value_k ) / total


We can mantain a thresh hold and if the variance kept exceeds that thresh hold, we can stop.


////////////////////////////////////////////////////////////////////////////////////////////////////////
Code:


total=sum(pca.explained_variance_)
k=0
current_variance=0

while current_variance/total< 0.99:
    current_variance+=pca.explained_variance_[k]
    k+=1


///////////////////////////////////////////////////////////////////////////////////////////////////////

With this, we have a trade off b/w data to be mantained and components to be mantained.

This is a very standard way of reporting how much variance are we working with.

We can say we are running PCA with 95% variance.

////////////////////////////////////////////////////////////////////////////////////////////////////

We use this always to get the best value of k.

////////////////////////////////////////////////////////////////////////////////////////////////////

========================================================================================


Magic of PCA:


Assume we are working with University Courses dataset

5 features talking about job Prospects??
7 features talking about Good Research??
3  talking about Expenses

We can see that the 5 features in first category are highly corelated, next seven are also correlated and simmilar with the third one as well.


If we try to reduce this to 3 dimensions we will be able to see that 1 feature represents job prospects, 1 represnts Research
and 1 represents expenses. 


**************
We can find interesting trends using  PCA although PCA is used to save resources.

////////////////////////////////////////////////////////////////////////////////////////

Apply PCA when we need to, it need not be default.

For huge datasets we may need to apply PSA more often than not.

===========================================================================


PCA 2:

PCA on images:


What is so special about images??

images satisfy the basic reuqirements for applying PCA.

////////////////////////////////////////////////////////
When do we want to apply PCA??

1) Images have huge number of features.

Suppose we have a 28*28 image.


Basically we have 28*28 pixels.

****
Each pixel is used as a feature. What this curtails is we have huge number of features because of this.

This leads to slow testing and training time



2)  A lot of covariance b/w the features. If we know one pixel's value, most of the pixels surrounding it will also have same values.
So they are highly corelated.

////////////////////////////////////////////////////////////////

More often than not we will be applying PCA on images.


//////////////////////////////////////////////////////////////////////////////////

===============================================================================


PCA on Olivetti images:

Already available in sklearn.


Importing:

from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

////////////////////////////////////////////////////////////////////////////////////////////////////////

Fetching Olivetti images:

olive= datasets.fetch_olivetti_faces()

# fetch unlike load functions first downloads the dataset if it isn't so far downloaded


/////////////////////////////////////////////////////////////////////////////////////////////////////////


# Plotting the images
fig=plt.figure(figsize=(8,8))  # used to create a figure
# we will draw 64 images, we will not plot all the 400 images
for i in range(64):
    ax=fig.add_subplot(8,8,i+1)   # within a figure, we can add subplots. Three integers num_rows, num_cols, index. indexing starts from 1
    ax.imshow(olive.images[i]) # imshow plots the data as an image
plt.show()
    

# We see that first 10 images are of first person, next 10 images of second person and so on.
# This dataset contains data of 40 people with 10 images each

//////////////////////////////////////////////////////////////////////////////


In ax.imshow we can cmap arguement as well so that we can give theme for images

ax.imshow(olive.images[i], cmap=plt.cm.bone)

//////////////////////////////////////////////////////////////////////////////////

**********

If we have 13 features and we don't pass num_components what is the dimension of components_ array??

Explanation:
If we have  13 features we are 13 in dimensional space.

Each principal component will be having 13 components.

So total size of .components_ array will be 13*13


 What happens here??

# Here we have 4096 features. The size of pca.components_ should be 4096*4096
# 4096 features, with each feature having 4096 components.
# But we only get 400 features having 4096 components
# This is because we can see in the documentation of pca module that 
# n_components == min(n_samples, n_features). Here n_samples is only 400 and n_features= 4096 so 400 features are only kept


///////////////////////////////////////////////////////////////////




======================================================================================


Reproducing images:


One thing that helps a lot while working with PCA on images:

We have a whiten parameter in PCA() function. By default its value is False,
While applying PCA on images, we should set this to True and see if performance improves.

What this essentially does is, it scales the PCA ed features. It ensures that the new features that we have obtained have zero
mean and unit variance.


When we are trying to do some classification, regression etc we can toggle this flag to True and see how score is changing


////////////////////////////////////////////////////////////////////////////////////////////////////////



======================================================================================

Eigen faces:

After running PCA with 95 % variance on orivetti dataset, we reduced our features to 123 from 4096.


1st component :   4096 size. Because initially we were in 4096 dimension space and to represent this space, we need 4096 components.


So we have

x1
x2
.
..

.
.
x4096


What was x1??

x1 was the first pixel in any image
x4096 was the last pixel in any image.


In a way we can say that our eigen vector has some pixel values.

Let's try to draw these components


////////////////////////////////////////////////////////////////////////////////////////




eigenfaces=pca.components_.reshape(123,64,64)   # Reshaping the eigen vectors so that we can try to draw them


fig=plt.figure(figsize=(8,8))  # used to create a figure
# we will draw 64 images, we will not plot all the 400 images
for i in range(64):
    ax=fig.add_subplot(8,8,i+1)   # within a figure, we can add subplots. Three integers num_rows, num_cols, index. indexing starts from 1
    ax.imshow(eigenfaces[i], cmap=plt.cm.bone) # imshow plots the data as an image
plt.show()



Explnation: What we have essentially done is gotten the eigen vectors, reshaped them so that we can plot them

and then simply plotted them. 

We see that they look somewhat like faces only.

That's why for images we call eigen vectors as eigen faces only.

Each of the new principal component represnts some image about the original image only.

Essentially we want this only from any principal component only. Many features combine together to show some data.
We want these principal components to do the same as well. Principal components explain about the data only.



x1 essentially may be represnting something abstract but when combined together each component represnts something about the images/ original data itself.

/////////////////////////////////////////////////////////////////////////////////////////////////////////


=====================================================================================================================


Classification of LFW images:


images of famous people. 

We do everything we did in the previous project, we will try to classify images on top of that.


When we apply classification using Random forest we get accuracy of only 68 % when not using PCA.
With PCA it drops to 48%.

This is not perfornming well at all. The precision for class 2 is very low and it is having the highest number of samples.

We may need to do something about it.




//////////////////////////////////////////////////////////////////////////////////////



==============================================================================


NLP:

Natural Language Processing

//////////////////////////////////////////////////////////////////////////////////////////

There are various Challenges to use words for classification.

1) We need to mantain the cotext. Suppose 'not good' comes in our documents. If we start using words as features we will assume that good is a part of feature set on our document.

2) Suppose we have helper, help, helping coming in our documents. It might make sense to combine all and treat them as 'help' itself.
This is just like taking words to root word.


3) Suppose we have good and nice in our documents. We may want to make them 1 considering both have same meaning.


///////////////////////////////////////////////////////////////////////////////////////////////


Basics of word processing:


We will use module called NLTK. Very good with working with text.


sample_text="Does this thing really work? Let's see."



We want to tokenize this. We want to create seperate seperate words from this.


/////////////////////////////////////////////////////


Importing the libraries:
from nltk.tokenize import sent_tokenize,word_tokenize


sent_tokenize: Used to split sentencies to words

word_tokenize: Used to convert words to tokens.

///////////////////////////////////////////////////////////////////

sent_tokenize(sample_text)

Answer:
['Does this thing really work?', "Let's see."]



////////////////////////////////////////////////////////////////

word_tokenize(sample_text)

Answer:

['Does', 'this', 'thing', 'really', 'work', '?', 'Let', "'s", 'see', '.']


/////////////////////////////////////////////////////////////////////////////////////////////


Now we have tokens. We need to get rid of stop words.


Getting the stop words
from nltk.corpus import stopwords
stop=stopwords.words('english')

Getting rid of stop words:

clean_words=[w for w in words if w not  in stop]



stop words are always lower. 

So we should be careful while cleaning.

We can make the text lowercase before sending it to tokenize using word_tokenize method.

This can lead to loss of information as well so we should decide this lowering at case by case basis.


/////////////////////////////////////////////////////////////////


Getting rid of punctuations:

We can add the punctuations to the stop words list.


To get the punctuations:

import string
punctuations=string.punctuation


///////////////////////////////////////////////////////////////////////////////////////

====================================================================================

Stemming:

played, play, playing :   we don't all these to become different words, we want them to become the same word.

Stemming is used to achieve this.



NLTK has a  porter stemmer.


Importing from nltk.stem import PorterStemmer


//////////////////////////////////////


Code:


to_stem_words=["play","playing","player","played"]
ps=PorterStemmer()
# Creating PorterStemmer object

stemmed_words=[ps.stem(w) for w in to_stem_words]
# We have to call stem function on all the words seperately.



print(stemmed_words)


Output:
['play', 'play', 'player', 'play']

Explanation:

Player is a person whereas others reflect the same thing. Hence they have been dealt accordingly.


///////////////////////////////////////////////////////////////////////////////////////////////////////


*** porter stemmer is not that smart and works on fixed set of rules.

Not that smart but does a decent job.

////////////////////////////////////////////////////////////////////////////////////////////////////////

================================================================================================



Part of speech tag:


if a word is used as a noun, if a word is used as an adjective etc.


We use POS for that


Importing:

from nltk import pos_tag


For this we will use state_union which contains many texts

from nltk.corpus import state_union


Code:


Its very simple. We have to pass an array of words.

For a particular text we can simply tokenize and pass it as input to pos_tag.


pos = pos_tag(word_tokenize(text))
pos



Output looks somewhat like:

[('PRESIDENT', 'NNP'),
 ('GEORGE', 'NNP'),
 ('W.', 'NNP'),
 ('BUSH', 'NNP'),
 ("'S", 'POS'),
 ('ADDRESS', 'NNP'),
 ('BEFORE', 'IN'),
 ('A', 'NNP'),
 ('JOINT', 'NNP'),
 ('SESSION', 'NNP'),

and so on......................

///////////////////////////////////////////////////////////////////////////////////////////////////////

Tag List:

CC coordinating conjunction
CD cardinal digit
DT determiner
EX existential there (like: “there is” … think of it like “there exists”)
FW foreign word
IN preposition/subordinating conjunction
JJ adjective ‘big’
JJR adjective, comparative ‘bigger’
JJS adjective, superlative ‘biggest’
LS list marker 1)
MD modal could, will
NN noun, singular ‘desk’
NNS noun plural ‘desks’
NNP proper noun, singular ‘Harrison’
NNPS proper noun, plural ‘Americans’
PDT predeterminer ‘all the kids’
POS possessive ending parent’s
PRP personal pronoun I, he, she
PRP$ possessive pronoun my, his, hers
RB adverb very, silently,
RBR adverb, comparative better
RBS adverb, superlative best
RP particle give up
TO, to go ‘to’ the store.
UH interjection, errrrrrrrm
VB verb, base form take
VBD verb, past tense took
VBG verb, gerund/present participle taking
VBN verb, past participle taken
VBP verb, sing. present, non-3d take
VBZ verb, 3rd person sing. present takes
WDT wh-determiner which
WP wh-pronoun who, what
WP$ possessive wh-pronoun whose
WRB wh-abverb where, when



///////////////////////////////////////////////////////////////////////////////////////////////////




=======================================================================================

Lemmatization:


To do Lemmataziation there are some requirements.


Importing the library:

from nltk.stem import WordNetLemmatizer


Creating lemmatizer object:

lemmatizer=WordNetLemmatizer()


Lemmatizing the word:
lematizer.lemmatize('Raj',pos='v')


Lemmatizer object needs two arguements.
The word and the part of speech denoted as pos


////////////////////////////////////////////////////////////////


Why does it require Part of Speech arguement??

Suppose we are working with painting.

Now painting can be a noun as well us verb. To distinguish one from another, we need to pass part of speech arguement.


/////////////////////////////////////////////////////////////////////////////////////////////


pos tag  gives us tags like NNP for noun.
Here we need to provide 'n' as pos arguement for the lametizer.




////////////////////////////////////////////////////////////////////////////////////////////////////




Simple fn to convert pos tag to desired form:


def get_Simple_pos(tag):
    if tag.startswith('J'): # There can be many kinds of adjectives but all start with J
        return wordnet.ADJ # This is a constant present in wordnet library
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN # If nothing matches, take it as a noun.


=====================================================================================



Movie Reviews dataset:


We have a lot of reviews and we know if a review is positive or negative.


mAKE A CLASSIFIER, Which given a review predicts, if the review is positive or negative

///////////////////////////////////////////////////////////////////////////////////////////////////////


Loading the dataset:


from nltk.corpus import movie_reviews


//////////////////////////////////////////////////////////////////////////////


This dataset has two categories:

movie_reviews.categories()

Output:

['neg', 'pos']

///////////////////////////////////////////////////////////////////////////


To access the reviews:

Reviews are within various files.

We can access file ids  as :   movie_reviews.fileids()



/////////////////////////////////////////////////////////////////


To get only negative file ids:


movie_reviews.fileids('neg')



//////////////////////////////////////////////



To get words written within an article:


We use words method and need to pass the file id.

movie_reviews.words(movie_reviews.fileids()[7])  # Accessing the 8th movie.


////////////////////////////////////////////////////////////////////////


Folder name tells the category of the review.


================================================================================================



Data Cleaning:

To load the dataset:
We will create an array. Each element of array will be a tupple. One part of tupple will be the words, the second will be the category.



Code:

documents=[]

for category in movie_reviews.categories():
    for fileid in movie_reviews.fileids(category):
        documents.append((movie_reviews.words(fileid),category))



//////////////////////////////////////////////////////////////////////////////////////////////


Shuffling:


import random
random.shuffle(documents)


///////////////////////////////////////////////////////////////////////////////////////////////


Cleaning each of the files:

1) Removing stop words.
2) Doing Lemmatization


/////////////////////////////////////////////
Removing stopwords:


Getting the stop words:

from nltk.corpus import stopwords
stops=set(stopwords.words('english'))

Adding the punctuations:
import string
stops.update(list(string.punctuation))


//////////////////////////////////////////////////////////


Converting pos tags to the ones which Lemmetization accepts:


from nltk.corpus import wordnet
def get_Simple_pos(tag):
    if tag.startswith('J'): # There can be many kinds of adjectives but all start with J
        return wordnet.ADJ # This is a constant present in wordnet library
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # If nothing matches, take it as a noun.

/////////////////////////////////////////////////////////////////////////////////////////////

Function to apply Lemmetization:

 
def clean_review(words):
    output_words=[]
    lemmatizer=WordNetLemmatizer()
    
    for w in words:
        if w.lower() not in stops:
            pos = pos_tag([w])  # Pass the exact word and not the lowered word, otherwise we may loose some information
            clean_word=lemmatizer.lemmatize(w,pos=get_Simple_pos(pos[0][1]))
            output_words.append(clean_word.lower())  # Returning lower as  Playing and playing represnt nearly the same dataset
    return(output_words)


////////////////////////////////////////////////////////////////////////////////////

=============================================================================================


Building Feature Set:

The format in which we require the data is:

For a datapoint
We need to provide a dictionary with all the features and category of the datapoint .

{

f1:   value corresponding to f1

f2: value corresponding to f2

.
..
.
.
.

}, category




This was for a single datapoint.

If we want to do it for all, we store all in a list.

/////////////////////////////////////////////////////////////////////////////////////////////////


What are the features that we will use??

We use the top words that we have in the whole dataset.

*****
While creating the feature set we don't use complete dataset. We use the training dataset only.


Code:

# Splitting into training and testing data


training_documents= documents[0:1500]  # We have already shuffled the dataset
testing_documents= documents[1500:2000]


# Building array with all the words

all_words=[]
for doc in training_documents:
    all_words += doc[0]   # doc[0] contains the words
    

# Getting the frequency distribution
freq= nltk.FreqDist(all_words)   # Returns us a frequency distruiibution object, Can us Counter from collections as well here


common=freq.most_common(3000)  # rETURNS the 3000 moast common words
features= [i[0] for i in common] # wE only need the words, we don't need the frequency of words

# For each document, creating the dictionary, where each dictionary contains if a word(feature) is appearing in the document. Also contains the class. For ex, suppose feature is right. We see if right comes in the document or not.


def get_feature_dict(words):
    current_features={}
    words_set= set(words)  # Creating a set so that lookup is O(1)
    for w in features:
        current_features[w]= w in words_set
    return(current_features)


# Building training data by running on complete words list

training_data= [(get_feature_dict(doc), category) for doc, category in training_documents]

testing_data= [(get_feature_dict(doc), category) for doc, category in testing_documents]



//////////////////////////////////////////////////////////////////////////////////////////////////////////////


=======================================================================================


Classification using NLTK Naive Bayes:


We created the training data in this format because NLTK classifiers require data to be in this format

Importing:
from nltk import NaiveBayesClassifier


Syntax is a bit different:

No need to create model object
Training the classifer
classifier= NaiveBayesClassifier.train(training_data)


Getting the accuracy:
nltk.classify.accuracy(classifier,testing_data)

/////////////////////////////////////////////////////////////////

Very cool function:

classifier.show_most_informative_features()

// Gives us very cool insights with regards to the classification

Here the output is:


Most Informative Features
                  seagal = True              neg : pos    =     12.9 : 1.0
               ludicrous = True              neg : pos    =     11.7 : 1.0
             outstanding = True              pos : neg    =     11.1 : 1.0
                   inept = True              neg : pos    =      8.9 : 1.0
                 idiotic = True              neg : pos    =      7.5 : 1.0
                flawless = True              pos : neg    =      7.1 : 1.0
                    anna = True              pos : neg    =      7.1 : 1.0
                   anger = True              pos : neg    =      6.6 : 1.0
                lifeless = True              neg : pos    =      6.5 : 1.0
                  poorly = True              neg : pos    =      6.3 : 1.0

Explanation:

If seagul is True , that is seagul is present then the odds for the document to be negative is 12.9 times to that of being positive.


We can get these insights and then what these words and maybe if we want perform certain analysis on the same.

////////////////////////////////////////////////////////////////////////


===================================================================================


NLP 2:

Using sklearn classifiers with nltk:

nltk and sklearn both have classifier implementations

Basic difference b/w nltk and sklearn work is the data format required.


nltk:   

A list is required.

Each element of the list is a tupple.

One element of tuple is a dictionary. For a datpoint the dictionary contains the features and stores if a particular feature is present or not.

The other element of the tuple is  the class of the datapoint.



sklearn:   x, y format

x is 2d array,  y is 1d array containing the classes.



////////////////////////////////////////////////////////////////


We don't need to manually convert nltk format data to sklearn format data.

nltk provides us a feature with which we can achieve the same.



importing:

from sklearn.svm import SVC
from nltk.classify.scikitlearn import SKlearnClassifier


SKlearnClassifier is a dummy classifer. If we pass SVC to this, it changes data accordingly.
features can be passed as earlier used. The format will be nltk desired format.

Code:
svm=SVC()

classifier_sklearn= SklearnClassifier(svm)


We can pass any estimator to the SklearnClassifier.

///////////////////////////////////////////////////////////////////////////////
Training using SKlearnClassifier wrapper:

classifier_sklearn.train(training_data)

//////////////////////////////////////////////////////////////////////////////////////////////////


=============================================================================================


Count Vectorizer:

How to get data into a form which sklearn requires

We will use nltk just to clean the data. We will run models on it using sklearn.

//////////////////////////////////////////////////////////////

Importing the library:

from sklearn.feature_extraction.text import CountVectorizer


///////////////////////////////////////////////////////////////////

Let's work on some dummy data:

train_set={"the sky is blue", "the sun is bright"}

count_vec= CountVectorizer(max_features=3)
a=count_vec.fit_transform(train_set)



Explanation:

We create a count vectorizer object and pass num_features which is basically the top n features which we will include in our dataset.

Then we fit transform this on training data and get a sparse matrix as output.

The sparse matrix is document vs feature matrix.

A cell i, j  contain for document i and feature j what is the count.



/////////////////////////////////////////////////////////////////////////////////////////////


To get feature names that have been picked:

count_vec.get_feature_names()


Count vectorizer tokenizes the array and picks the best features for us based on count and then creates a 2d array for us.
We may have to clean as this doesn't remove stop words.


//////////////////////////////////////////////////////////////////////////////////////////

fit function of Count Vectorizer should be used on training set. Training set should be used top figure out the best features.

Count Vecto0rizer requires strings as inputs. Rather a list of strings. 


///////////////////////////////////////////////////////////////////////////////////////



data cleaning will be done by using nltk. We will tokenize, remove the stop words, create the dataset.
Then we can use count vectorizer to get data in a format which can be used by sklearn models.

/////////////////////////////////////////////////////////////////////////////////////



Using Count Vectorizer on Movie Dataset Documents:

# documents is the data in nltk desired format

categories = [category  for document, category in documents]


text_documents = [" ".join(document) for document,category in documents]



from sklearn.model_selection import train_test_split
x_train,x_test,y_train, y_test = train_test_split(text_documents, categories)

# Creating the sparse matrix
count_vec=CountVectorizer(max_features=3)  # Randomly have picked 3.
x_train=count_vec.fit_transform(x_train)
x_test=count_vec.transform(x_test)

/////////////////////////////////////////////////////
x_train.todense()

Output:
matrix([[0, 9, 5],
        [4, 4, 2],
        [2, 1, 0],
        ...,
        [0, 6, 3],
        [7, 5, 4],
        [0, 4, 1]], dtype=int64)

//////////////////////////////////////////////////////////

Getting the features:

count_vec.get_feature_names()

Output:
['film', 'movie', 'one']

/////////////////////////////////////////////////////////////////

x_train.todense() is the input feature for the model.



/////////////////////////////////////////////////////////////////


Count Vectorizer can't take care of stop words and we need to do that part of pre processing before hand.

Soln:

False

There is  an option in count vectorizer(stop_words) which takes list of stop words and can do the work for us. 


//////////////////////////////////////////////////////////////////////////////////


=============================================================================


sklearn classifiers:

There is a difference b/w features that we created ourselves and those that CountVectorizer created.


When we did ourselves using NLTK: 

fim: True or False

We used shuffle to create train test split.



Count Vectorizer:


film:  How many times film comes. 

So we can't expect same accuracy and same results using the two different approaches.

We used the train test split function.


/////////////////////////////////////////////


Code using sklearn SVM:

from sklearn.svm import SVC
svc=SVC()
svc.fit(x_train,y_train)
svc.score(x_test,y_test)


/////////////////////////////////////////////////////////////////

We directly fed the sparse matrix. We didn't need to convert to dense matrix

////////////////////////////////////////////////////////////

========================================================================

N gram:

Different options available to use while using Count Vectorizer:


1) max_features: We have already seen this.

2) ngram:  Very good technique that can lead to great results in few cases.

3) analyzer: 'words'

////////////////////////////////////////////////////////


ngram:

So far we have done 1 gram.

By 2 gram we mean , if text is  "the sky is blue",  possible words will be "the sky" , "sky is", "is blue".
This means a combination of features.


So if somewhere we are getting not good, that will be taken care of as well.



Here intuition is if not good is very frequent it can get picked and get us better classifications.


Now in addition to normal features we will consider these bigrams as well.


How to give this input:

In ngram paramter we can give an option:    (1, 3):  This means that we want 1gram, 2 gram and 3 gram

//////////////////////////////////////////////////////////////////////////////////////////////

Code:


count_vec=CountVectorizer(max_features=2000, ngram_range=(1,3))
x_train,x_test,y_train, y_test = train_test_split(text_documents, categories)
x_train=count_vec.fit_transform(x_train)
x_test=count_vec.transform(x_test)
svc=SVC()
svc.fit(x_train,y_train)
svc.score(x_test,y_test)

//////////////////////////////////////////////////////////////

Generally we keep 1gram but it is upto us, if we want to go to 2 or 3.


//////////////////////////////////////////////////////////////////////////


In some cases this produces better resuolts, sometimes this leads to worse results


//////////////////////////////////////////////////////////////////////////


Ques:

N-grams are combination of N keywords together. How many bi-grams can be generated from given sentence:

“Today is a sunny day.”

Ans:  4
####There are total 4 bi grams - "Today is", "is a ", "a sunny " and "sunny day".


///////////////////////////////////////////////////////////////////////////////

Number of N-grams for a sentence with X words will be

Output:
X-N+1


////////////////////////////////////////////////////////////////////////////////////


====================================================================================


TF-IDF:

These work really well in text search and word search.


TF: Term Frequency
IDF: Inverse  Document Frequency:     1/ Document Frequency



For a word w  ,  document frequency   df is defined as no of docs that contain w.


/////////////////////////////////////////////////////////////////////////////////////////////////////////

Intuition:
If a word is present in a lot of words, it is not that important for classification.

We want words that have high frequency but have low document frequency.


A word that comes many times but in less docs is far more important than a word that appears in lot of docs.



/////////////////////////////////////////////////////////////////////////////////


So we will be looking at tf* idf

When we use count vectorizer, we can a give very intersting args


max_df, min_df:


Suppose max_df is 0.8 and min_df is 0.1

While we build vocabulary we essentially mean , we won't pick any word presnt in more than 80% docs and
a word presnt in less than 10% docs.



/////////////////////////////////////////////////////////////////////////////////////////////////


This helps in building vocabulary. We may still need to remove stop words although this might take care of it on its own.



For ex: Suppose we are working with emails dataset. This email may be presnt in a lot of datapoints and maybe not giving us any information.
And it is not presnt in stop words as well. So this technique takes care of it.




////////////////////////////////////////////////////////////////////////////////////////////////////////////



tf(wi, di) is    How many times word wi comes in document di.


tf(w,d) is to build our feature set so far.


	f1         f2                       f3    f4 .........................

d1          tf(w1,d1) tf(w2,d1)
.
.
.
.
.
d100	tf(w1,d100)


Sometime its better to use tf*idf instead of just tf.


Intuition:
What this essentially does is, even if some of these common words words get into vocabulary, they will get less priority.
This is because idf will be very low.



For that we have a TF-IDF vectorizer if we want to use.

//////////////////////////////////////////////////////////////////////////////////////////

Whenever we are working with any text classification, we can experiment with both tf-idf vectorizer and count vectorizer.
Also we can experiment with min_df and max_df

////////////////////////////////////////////////////////////////////////////////////


Ques:
In a collection of N documents, one document is randomly picked. The document contains a total of T terms and the term “cricket” appears X times.
What is the correct value for the product of TF (term frequency) and IDF (inverse-document-frequency), if the term “cricket” appears in approximately one-third of the total documents?
TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).

IDF(t) = log_e(Total number of documents / Number of documents with term t in it).


====================================================================================


Neural Networks:

Deep Learning.
All about neural networks.




AI contains ML contains DL

DL is very powerful and gets us very great results.

////////////////////////////////////////////////////////////////

Neural Networks??

Why use NNs??
NNs not new, they have been around for long time.
Earlier they were computationally expensive.
Now we have great computational power.




Logistic Regression: We used sigmoid function.     1/  1+e**(-theta *x)


Even though h(x)  is non linear, still we could only reach Linear Decision Boundaries.

This was because we put in limits. We assumed if h(x) > 0.5, we predict 1 and if h(x)<0.5 we predict 0.

This is turn meant that we were dependant on theta*x.

If theta*x > 0, we predicted 1 and if theta*x < 0, we predicted 0.

This is this becomes linear boundary.



How we got to non linear decision boundaries??
For Logistic Regression, we added dummy data

f1 f2 f3 |   f1**2  f2**2  f3**2 f1f2........


LR will find a linear boundary in this new feature set. For this new feature set, decision boundary will be linear but in original feature set
Decision Boundary will be non linear.


This is a pretty decent soln but the problem is, we have to find out which features we need to select.

Instead of being quadratic, decison can be sin, log, tan etc  of the feature as well.



For SVM:
We used kernels but here as well, we as devs have to choose the kernels as well.
SVMs are also computationally very expensive.


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



What happens in LR:


             x0=1
	        \  theta0
                             \
          theta1
x1------------------>
		      sum              --------->                 LR
        theta2	     everything                               function
x2----------------->



We first calculate sum :    z= theta0*x0+theta1*x1+theta2*x2


Then we calculate    sigmoid(z)       to get the final hypothesis function.



We are getting linear boundary because if theta*x > 0, we get 1 and theta*x <0, we get 0.

Intution behind neural networks is in addition to passing through single Logistic Regression unit, why not pass through multiple logistic regression units.


eX:  
Diag1 

Drawn in one notes app

Saved as pdf. Can be seen from there.



Combined Output is non linear.


LR3 will be linearly dependant on LR2 and LR1. But it is not linearly dependant on x1,x2,x0.

We have a lot of weights now.


x1   ->   LR1 ->  w11

x1 -> LR2  ->  w12

Net function isn't linear w.r.t inputs.

We won't need to add input features on our own.


/////////////////////////////////////////////////////////////////////////////////////////////


Its not necessary that we choose sigmoid. We can choose other functions as well.

We find z in exactly the same way.

But we can apply some other functions as well.


/////////////////////////////////////////////////////////////////////////////////////////////


=============================================================================================================


Example with Linear Decision Boundary:

x	y	
1	0
0	1


Basic Logistic Regression:


Diagram 2


We need to pick w1 and w2 so that this works fine for us.



When  x=0, we want output as 1.

we h(z) >0.5  ,   1/ 1+e**(-z)

we want  w0+x1*x>=0

as x=0,  we have   w0>=0


We will keep w0 high, so that z is high and h(z) is much closer to 1.

We take w0 to be 50.


So x=0, gets taken care of this way




/////////////////////////////////////////////////


When x=1, we want y=0

So w0+w1*x <0


Here x=1

So we want  w0+w1<0.

We already have w0 as 50.


So w1<-50

We will take w1 as -100

/////////////////////////////////////////////////////////////////////////////



Let's look at 2 feature dataset. Lets predict OR


x1	x2	y
0	0	0
0	1	1
1	0	1	
1	1	1



This is once again a linear boundary


Ex:   Diagram 3 


Neural Network:  Diagram 4



When both x1 and x2 are 0, we want output to be 0. 
This means x0 is -ve
Lets take x0 to be -50


//////////////////////////////////////////////////////////////////////

x1=1, x2=0, we want y=1


w0+w1*x1+w2*x2 > 0

-50 + w1> 0

w1> 50

Let's take w1 as 100.

So z=50.

and h(z) easily becomes 1

/////////////////////////////////////////////////////


With same logic as above w2 becomes 100 as well.



/////////////////////////////////////////////////////////////



With z values and predictions after getting w0, w1 and w2

x1	x2	y	z		predictions
0	0	0	-50		0
0	1	1	50		1
1	0	1	50		1
1	1	1	150		1




/////////////////////////////////////////////////////////////////////



====================================================================================

For and if we try to plot, we will find that something like this can work:


w0=-100
w1=75
w2=75


Alone neither w1 neither w2 can counter w0.
Together they can.



====================================================================


All of this works for Linear Decision Boundary.


========================================================================



Finding Non Linear Decision Boundary:

Working with XOR	


Diagram 5


This is a  non linearly seperable data




x1	x2	XOR
0	0	0
1	0	1
0	1	1
1	1	0


We can do it in parts:


We can find and and we can find nor.


For nor, 

we want when both x1 and x2 are 0 , output class is +ve. So w0 can be 50.
But even if one is 1 , output class should be 0. So w1 and w2 can be -100.

////////////////////////////////////////////////////////////////////////////////////////////////////////

Now we have:



x1	x2	XOR	AND	NOR
0	0	0	0	1
1	0	1	0	0
0	1	1	0	0
1	1	0	1	0


/////////////////////////////////////////////////////////////////////////////////////////////////////


Now we can combine AND and NOR in a linear way, to get to a decision boundary.


Combining AND and NOR, 

when x1 and x2 are 0, output class is 0.  So w0 can be 50

when one is  0 and  other is 1, output class is 1. So w1 and w2 can be -100.

This is again working as NOR.

Final Output is XOR


//////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Diagram 6:   How we can achieve Non Linear Decision Boundary.


////////////////////////////////////////////////////////////////////////////////////////////////////////////////


=====================================================================================

mAGIC HEre is, 1 layer is Linearly seperable. But adding two layers allow us to create non linear decision boundaries.


We can essentially say with hidden layers we get new intermediary features and with them we identify final values.
With raw inputs it might not be easy to get to final results. But with these intermediary features we can get to final results easily.

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Self intution:
We can say that the model cannot predict completely by just using the raw data as raw data might be too complicated.
For ex: Simple Logisitic Regression can only figure out a single linear boundary.

But we can simplify this and get some new features with these hidden layers. Now if we try to figure out with the help of these features 
NNs can easily predict.

For ex: we are predicting cats and dogs with NNs. It might be very hard for algo to get the prediction right with just the images.

But the hidden layers can get some new features. For example they can identify a curve like their nose, ear etc which may be common to individual class.
Now, with the help of these curves/new features its easy to predict which is cat and which is dog. 

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Ques. Function by Neural Network:


x1    x2          f1      f2        final
0        0	     0        1         0
0         1        1         1	         1
1         0          1       1         1
1         1        1         0         0


Clearly XOR

///////////////////////////////////////////////////////////////////////////////////////

Ques :   Neural Network for function:


x1	x2	f1	f2	final
0	0	0	1	1	
0	1	0	0	0
1	0	0	0	0
1	1	1	0	1



Its XNOR



///////////////////////////////////////////////////////////////////////////////////

=========================================================================================


Neural Networks Terminology:



We have input units, then we have some layers. Each of these layers can have some units.

Also for each layer we add a bias as well.

Finally we have our output layer.


/////////////////////////////////////////////////////
The layer having input units is called input layer.

The intermediary layers are called hidden layers.

The final layer is called output layers.



************

The more layers we add and more units in these layers, more weights we will have to train upon


/////////////////////////////////////////////////////////////

The only decision we take is how many layers we want to add and how many units we want in each of these layers. 

Optimizer gets us the final paramters.


//////////////////////////////////////////////////////////////

What all we decide:

1) How many hidden layers
2) How many units in each layer 
3) The function we apply in these units. We can have different functions in different layers. Generally we pick same function for units in same layer.
The function that we apply is called as activation function.


///////////////////////////////////////////////////////////////


At any particular unit, we sum up the inputs in a weighted manner, add the bias and apply the activation function


===================================================================================================

Ques:


A neural network with hidden layers will always produce non linear decision boundary.

Answer:  False


Make activation function  as 1. Same input and output

=============================================================================================

Number of paramters in a Neural Network:


How many weights do we have ??

Input layer:   4
Hidden Layer 1:  10

Hidden layer:  8

Output Layer:  1



Total paramters:      At each input layer we will have to add 1 to the number of parameters as we add the bias unit.

Input --- hidden layer 1 :  (4+1)*10 = 50

Hidden Layer 1----- Hidden layer 2:   (10+1) *8 =  88

Hidden Layer 2-------------  Output Layer:   (8+1)*1= 9


//////////////////////////////////////////////////


For weights connecting j th layer and (j+1) layer:

Suppsoe we have  uj units in jth layer   and u(j+1)  units  in j+1 th layer


We will have            (  u(j) + 1 )         *     (u(j+1)  )



/////////////////////////////////////////////////////////////////////

Having so many paramters can lead to overfitting



///////////////////////////////////////////////////////////////////////////////



==========================================================================



Forward and backward propogation:


Forward Propogation: We have our features. To get to final output we will be forward propogating all our features.

We have our input features. We pass them to first layer. The output of first layers goes to the second layer and so on in the forward direction.

To calculate result for any datapoint we will have to do forward propogation.



////////////////////////////////////////////////////////////////////////////


The error, we get at the last output layer.

We move this error back to figure out to update the to the different weights.

As we move back from output layer towards the input layer, we call this back propogation

///////////////////////////////////////////////////////////////////////////////////////


To get updates:  Forward Propogation

To update weights and push error back through layers: Backward propogation


===================================================================================


Cost Function:


Tells how good our training has been and we try to minimize this using optimizers.

Cost function generally has two parts

 Cost =  Error + lambda * Regularization


We don't want to make error completely zero by making complex models. That leads to overfitting.

lamda is tuning parameter for us with which we can give more or less importance to regularization parameter.

/////////////////////////////////////////////////////////////////


Choice in front of us is which Error function to use and which regularization to use.

Regularization is same as earlier.

We will use  sum(theta j **2)

This is the sum of squares of all the weights inside the network.

//////////////////////////////////////////////////////////////////////////////
Error Function: Many options

1)    MSE:  1/ m  *   sum  (y actual - y predicted)**2


2)  Logistic Regression error func:    1/ m *  sum (  -yi log(y pred) -(1- yi)(1-log(1-y pred)))  

	We can Logistic Regression error func here as well. 


=====================================================================================



How to handle multiclass classification:

So far we have been doing one vs rest approach.

We don't do that in case of Neural Networks:

We only train a single model.

For output layer we have multiple outputs:


Ex: Diagram 7


/////////////////////////////////////////////////////////////


If we have 3 classes to predict:


Output Layer will have 3 units.


Each unit will only predict 0 or 1.

Essentially each unit is doing binary classification.

/////////////////////////////////////////////////////////////

For training datapoints:

Datapoint	Prediction
     x1 --------------------> 1
     x2---------------------> 2
     x3---------------------> 3
     x4-----------------------> 2

The target won't be straight 1, 2 or 3.

Rather it will be a one hot vector

1 will be :  

1
0
0

2 will be:
0
1
0

and so on.


///////////////////////////////////////////////////////////////

While predicting:

For each datapoint
We will get a vector as a result

Suppose for some datapoint we may get:

0.1
0.15
0.99


To make a prediction we pick a class for which output value is maximum.

Here prediction should be class 3.

/////////////////////////////////////////////////////////////////////////////////////////


Changes:

1) Change training data y to be one hot encoded. If we are using some library, we won't have to do this. If we are implementing our own Neural Network, we will have to do this.

2) To predict-> Predict class for which y pred value is maximum.


////////////////////////////////////////////////////////////////////////////////////////


2 classes ->  1 unit in output layer

3 classes -> 3 units in output layer

4 classes -> 4 units in output layer


For k possible classes, we will have k different units in output layer.


//////////////////////////////////////////////////////////////////////////////////////

How cost function changes??

So far:

1/ m * sum(  func (yi pred, yi actual) )  + (lambda / 2*m) * sum(theta **2)

Most stuff remains the same, we just add one additional summation:


1/ m * sum(         sum(         func (yi j pred, yi jactual) ))  + (lambda / 2*m) * sum(theta **2)
            i =1 to m    j = 1 to k


What this means:

For a datapoint, we get error w.r.t all the classes and we sum it.
This sum is error for a single datapoint.

For complete error this sum is taken over all the datapoints.





//////////////////////////////////////////////////////////////////////////////////////////////////////


========================================================================


MLP classifier in sklearn

Implementation in sklearn.

Does have 1 implementation in sklearn but not that great.


/////////////////////////////////////////////////////////////
IMPORTING :

from sklearn.neural_network import MLPClassifier


Here MLP stands for Multi Layer Perceptron. Each unit of neural network is called as a perceptron.


//////////////////////////////////////////////////////////////////////////////////////

Creating object and fitting:

clf=MLPClassifier(max_iter=1000)
clf.fit(x_train,y_train)



# Here we haven't converted y to a one hot encoded form. Library makes that happen on its own.

//////////////////////////////////////////////////////////////////////////////////////

==============================================================================

parameters inside MLP Classifier:


hidden_layer_sizes: No. of hidden layers and their sizes.
 By default (100,). This means we one hidden layer with 100 units.

We can have something like:  (100,50)
Now we will have 2 hidden layers. One hidden layers has 100 units. The second hidden layer has 50.



activation fn: By default Relu.

alpha: the regularizattion factor


batch size:  If we are using Mini Batch Gradient Descent. 




//////////////////////////////////////////////////////////////////////////////////////////////



We can look at individual weights on each of the layers:



clf.coefs_

# clf is the MLP object
# These don't include the bias connections


To get biases:

clf.intercepts_



# For a hidden layer no of intercepts is equal to number of units in the hidden layer.
# This is because bias term is applied to all the terms in the hidden layer




//////////////////////////////////////////////////////////////////////////////////////////////




==================================================================================================



Ques.
Can I use different activation function for different hidden layers in MLPClassifier?

Answer: False


================================================================================================


Forward propogation 



First of all we need to decide on architecture.


If we don't have any hidden layer and activation fn is sigmoid fn, we effectively have a logistic regression architecture.

Something interesting can be when our model has hidden layer.



Idea is we start with some weights and we update the weights accordingly so that error is reduced.


///////////////////////////////////////////////////////////////////////////////////////////////

Writing Code for forward propogation:

We use numpy.

We will use AND function as our dataset. 


///////////////////////////////////////////////////////////////////////////




Code:

x = np.array([[0,0],[0,1], [1,0], [1,1]])
y=np.array([[0,0,0,1]]).transpose()   # Making y in this manner so that for each row we have 1 class
//////////////////////////////////////////////////////////////////////////////////////////////////////////
Sigmoid fn:

def sig(z):
    return  (1/ (1+np.exp(-z)))
////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Initializing the weights and biases:

weights= 2*np.random.random((2,1))-1 # By default weights in range b/w 0 and 1. We multiply by 2 so weights in range 0,2 and then we subtract 1. We now have weights b/w -1 and 1 which is desired
# We want the shape to be (2,1) hence arguement is a tupple with 2,1

bias= 2* np.random.random(1)-1  # Here considering we directly have single output layer, we will only have one bias weight. Otherwise number of bias weights for a layer depends on the number of the following hidden layer units

//////////////////////////////////////////////////////////////////////////////////////////////////////////////

Forward propogation to get the output:

output0= x
output= sig(np.dot(output0,weights)+bias) # using matrix multiplaction to multiply weights with inputs and then simply adding the biases
output

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


This was without any hidden layer.

==================================================================================================

Forward propogation with hidden layers:


We use the same and func as input dataset


Lets have 1 hidden layer with two units.




*********

To visualize, take frame of reference as 1st hidden layer or the output layer(if no hidden layer) instead of input layer.


********************



///////////////////////////////////////////////////////////////////////////////////////


Code:

# Weights in hidden layer. This will be (no of input units coming from previous layer * no of units in hidden layer)
wh= 2*np.random.random((2,2))-1

# biases for a particular hidden layer is equal to number of units in hidden layer


bh=2*np.random.random((1,2))-1   # Its a 1d array when we have done wTx. So we easily add bias to this
 # Input: 4*2 , weights= 2*2, np.dot(output,weights): 4*2, bias : 1*2. Same numbers added in each row.We have 4 rows, each row having 2 cols. We have 2 cols hence we needed bias in 1*2 form

# weight for output layer

wo=2*np.random.random((2,1))-1 # 2 units in hidden layer, 1 unit in output layer


bo=2*np.random.random((1,1))-1 # 1 unit in final output layer

/////////////////////////////////////////////////////////////////////////////////
Forward propogation with hidden layer:

output0=x  #  4*2 , weights= 2*2, np.dot(output,weights): 4*2, bias : 1*2. Same numbers added in each row. We have 2 cols hence we needed bias in 1*2 form
output_hidden=sig(np.dot(output0,wh)+bh)

output_final_layer=sig(np.dot(output_hidden,wo)+bo)

//////////////////////////////////////////////////////////////////////////////////////


=================================================================================================



Error Function in Gradient Descent:


The idea is we have a neural network.

We start with some weights and bias values.

We supply training data.

We get some output. The output won't be same as desired.

We get error/ cost after obtaining predicted values at final output layer.

We update the weights so that this cost gets minimized.

We optimize weights so that cost is minimized.


We saw 2 ways to do this. Either solve the eqn to get minima. Or get approximate best values using gradient descent.


We will use Gradient here to get best weights.


////////////////////////////////////////////////////////////////////////////

Gradient descent:


Diagram 10. For better visualization.



w =  w- alpha * dw/ dcost


We do the same here as well.


//////////////////////////////////////////////////////////////////////////////


What we need to do:


1) Decide on the error function.


2) Calculate    dE/ dW for all the weights be it in any layer.  
We also need to calculate   dE/ dB for all the biases.

For now we can forget about biases, they work exactly like other weights. 


////////////////////////////////////////////////////////////////////////


Deciding error fn:

We choose Mean Square Error:

(1/ 2 ) *sum ((yi actual - yi predicted) ** 2)

//////////////////////////////////////////////////////////////////////

For some weight  wij, here i is the starting unit and j is the ending units, we try to get

dE/ dwij

/////////////////////////////////////////////////////////////////////////



============================================================================


Derivative of sigmoid function:

sigmoid(z)  =   1/ (1+e**-z)

dsig/dz=   dsig/ d(1+e**-z)   * d(1+e**-z)/dz



dsig/dz= -1/ ((1+e**-z)**2)  * (0 - e**-z )                   ## d(e**-z)/dz= -e**-z
 	
	1/ ((1+e**-z)**2)  * (e**-z )      # 1/ (1+e**-z) == sig(z)
	
	sig(z)   *  (e**-z)/( 1+e**-z)   #   (e**-z)/( 1+e**-z)== 1-sig(z)

	sig(z) * (1-sig(z))



					
//////////////////////////////////////////////


============================================================================

Math behind back propogation 1:


wij: weight from unit i to unit j



input j:  combined input coming into j

o j: output from unit j.

o i: output from unit i.




We want to find out:

dE/ dWij  :   



E = 1/ 2 * (y actual - y predicted)**2


Error depends on y predicted.

/////////////////////////////////////////////////
How y predicted impacted by wij??

wij impacts input going into j (input j)

This changes o j.

This o j changes y predicted


///////////////////////////////////////////////////


Lets break the derivative in multiple parts:

dE/ dwij =  dE/ doj * doj/dwij               { Error is affected by output and output is affected wij}


We can break it further:

dE/ doj * doj/d input j * dinput  j /dwij   { Error is affected by output and output is affected input and input affected by wij}


//////////////////////////////////////////////////////////

We focus on the last two parts first

////////////////////////////////////////////////////////////


=======================================================================


Math Behing Backpropogation 2:


dinput j/ dwij:     

Suppose j th unit is getting input from some inputs from previous layer


input j=   sum(wij *ok)            # We assume we have l units which are providing input to j            
	k= 1 to l

  
We assume one is bias as well. For bias, output i.e.  o i will be 1. wij will be the bias term.


Getting d input j / dwij  :   

When we take this we will see that everything is independant except the ith component

So we are left with:

d input j / dwij =  d(wij * oi)/dwij   =  oi



Intution:
                w14
Unit1 --------------------------->  i1

                 w24
Unit 2------------------------------>   i2                                          Unit 4

                w34
Unit 3------------------------------------>  i3


Input to Unit 3:  i1+i2+i3....


Lets assume we are talking about input for Unit 4 from Unit 2.

w24 only impacts i2. It doesn't impact i1 and i3.

Hence we get this.

/////////////////////////////////////////////////////////////////////


d oj/ d input j:        

o j= sig(input j)


So,   we already know:  d sig(z)/z = sig(z) * (1-sig(z))

Here:

d oj / d input j  =   d sig(input j) / d input j=    sig(input j) * (1 - sig(input j))


We have  oj =sig(input j), replacing this

Final result

d oj / d input j =   o j * (1- oj)


/////////////////////////////////////////////////////////////////////////


Getting the first part:


Math behing Backward Propogation 3:


d E / dwij = d E/d oj * d oj / d input j *  d input j/ d wij


Getting dE / d oj 

Lets assume j to be the final layer


For output layer, oj = yp

So,   dE/ d oj = d (sum(1/2 (y actual - y predicted)**2))/ d ypredicted


sum (y actual - y predicted) * (-1)   


sum is over all training datapoints


This is just for final layer.


For final layer, complete dE/ dw ij is


dE/ dwij = sum  (yp-ya)* y p *(1-y p)*  o i

We sum this for all datapoints

//////////////////////////////////////////////////////////////////////

=====================================================================

Implementing a simple Neural Network:


We will work on and function only.

We will have no hidden layers here.


How x looks like:


x1 1    x2 1
x1 2   x2 2
x1 3   x2 3
x1 4   x2 4


dE/ dwij=  sum(yp-ya) * oj (1-oj) * o i


First two terms remain same for both x1 and x2.

So find the first term 

yp-ya:    

4 rows and 1 col


We find the second term:

4 rows and 1 col


Multiply both and we will have 4*1 matrix.

For each term we have calculated the first two terms.


////////////////////////////////////////////////////////////////////

Taking care of the last term:

For feature w11, we have to multiply above multiplication with feature 1. Third term is o i. As there are no hidden layers o i is simply inputs.




Code:


for iter in range(10000):
    output0= x
    output= sig(np.dot(output0,weights)+bias) # using matrix multiplaction to multiply weights with inputs and then simply adding the biases

    learning_rate=0.1


    first_term=output-y
    input_for_last_layer=np.dot(output0,weights)+bias
    second_term= derivativeSig(input_for_last_layer)
    first_two=first_term*second_term

    changes= np.array([[0.0],[0.0]]) # mantains changes for particular feature. Same dimensions as weights. 2rows, 1 col

    for i in range(2):   # For each feature
        for j in range(4): # For each row
            changes[i][0]+=first_two[j][0]*output0[j][i]
    # changes i 2*1 , same as weights. Column always remains 0, hence 2nd index is 0
    # first_two is 4*1. j is represnting the datapoint. Hence for a datapoint we get the first two terms.
    # output0 is actually input to the dataset.
    # j represnts the datapoint, i represnts the feature


    weights=weights- learning_rate*changes
    bias_change=0.0
    for j in range(4):
        bias_change+=first_two[j][0]*1  # For bias output i is always 1
    bias=bias-learning_rate*bias_change
output=sig(np.dot(output0,weights)+bias)
weights,bias


//////////////////////////////////////////////////////////////////////////


=========================================================================


Optimizing our code using vectors:


We have:
 

 output0			first_two
x1 1    x2 1		ft1
x1 2   x2 2		ft2
x1 3   x2 3		ft3
x1 4   x2 4		ft4


To get change for w1, we multplied   ft1 with x1 1 , ft2 with x1 2, ft3 with x1 3, ft4 with x1 4 and added all.
We did the same for w2, we multiplied by feature 2.



To achieve the same we can take a transpose of output 0

x1 1	x1 2	x1 3	x1 4
x2 1	x2 2	x2 3	x2 4


## x1 1 means feature 1 , 1st datapoint

Now we can take a dot product

It will achieve the same thing as for loops.


Size of output0.T:  2*4

Size of ft: 4*1

Size of change :  2*1


/////////////////////////////////////////////////////////////////////////////////


For bias, we are simply adding the four entries of first_two. Hence we simply add them itself.

///////////////////////////////////////////////////////////////////


============================================================================



Total Derivatives:

Suppose we have a fn f, which is a fn of x,y and z and we want to take a derivative w.r.t k. x, y and z can be dependant on k 


df(x,y,z)/ dk=  df(x,y,z)/ dx * dx/dk  + df(x,y,z) / dy * dy/dz + df(x,y,z) / dz* dz/dk



////////////////////////////////////////////////////////////////////////////////////////


Suppose we want to get  dx^5/ dx.   This is 5x^4

Lets assume y=x^2 and z= x^3

So we want to do d yz/ dx

We can   do   d yz/d y * dy /dx + d yz/ dz * dz/dx

z* (2*x)+ y *(3x^2)

2x^4+ 3x^4 =  5 x^4

/////////////////////////////////////////////////////////////////////////////////////////

==================================================================


Math behind backpropogation:



















=======================================================================

Tensorflow:


1) End to end open source for machine learning. APIs for doing various tasks of ML.
2) Useful for both beginers and advanced. Can crete default layers by specifying some parameters or create custom layers as well.
3) Tenserflow was built on the concept of using Computation graphs to represent machine learning algorithms.
4) Tensor: Multi dimensional arrays with uniform type (called a dtype);. Just like numpy arrays.
5) Computational graph: Type of directed graph where nodes describe operations, while edges represent the data (tensor) flowing b/w
those ops.
6) Supports both eager execution (default mode) and graph execution.

Eager execution: Code run immediately after the code is written. Runs line by line.
Graph execution: First graph is built and then the code is run.


When an application is executed to perform operations on Tensor, instead of executing the
operation it is stored in a Computational Graph. A computational graph is a type of
directed graph where nodes describe operations, while edges represent the data (tensor)
flowing between those operations.

It is a type of data flow graph.

a                        b
 \                      /
   \                  /
     \              /
       \          /
           a+b





Graph execution is faster.

Benifits of Eager excecution over graph execution:

1) Easy debugging
2) Natural Control flow


////////////////////////////////////////////////////////////////////////


Tensors:

Tensors are n-dimensional arrays with a uniform type. Especially when referring
specifically of Neural network data representation, this is accomplished via a data
repository known as the tensor. Tensors are generalizations of matrices to N-dimensional
space.


Features:

1) Can also be converted into numpy arrays.

2) We can do basic math on tensors, including addition, element-wise multiplications, matrix multiplication. 

3) All tensors are immutable. You can never update the contents of a tensor, only create a new one.


Scaler (Rank 0) Tensors - A tensor that contains only one number is called a scalar.
Vector (Rank 1) Tensors - An array of numbers is called a vector, or 1-D Tensor.
Matrices (Rank 2) or 2D Tensors - An array of vectors is a matrix or 2-D Tensor.
3-D Tensor and Higher Dimensional Tensor - If you pack such matrices in a new array,
you obtain a 3-D Tensor.

============================================================================================


Constants and variables:

Importing:
import tensorflow as tf


# Creating a constant 

a=tf.constant(5)

a
<tf.Tensor: shape=(), dtype=int32, numpy=5>


# Addition:

tf.add(a,b)


///////////////////////////////////////////////////////////////////


Anything which we create in tensorflow be it constant or variable is a tensor object.


/////////////////////////////////////////////////////////////////////

In tensorflow all the weights and parametes that change to give us the best possible results are stored as variables.

///////////////////////////////////////////////////////////////////////////////


Creating variable:

var1=tf.Variable(20)

var1

<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=20>



var2=tf.Variable([[1,2],[3,4]])
var3=tf.Variable([[5,6],[7,8]])

# Matrix multiplication:
tf.matmul(var2,var3)


<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[19, 22],
       [43, 50]])>



/////////////////////////////////////////////////////////////////////////////////////


==================================================================================

MNIST:  load and process the data:


Given an image of number b/w 0 to 9, we need to figure out what that image is.

Dataset is present in tf.keras api
Already split into training and testing.



(x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data()


//////////////////////////////////////////////////////


Code:

x_train=x_train.reshape(x_train.shape[0], -1)/255.0 # Currently values were b/w 0 and 255. By dividing with 255 we normalize this.
x_test=x_test.reshape(x_test.shape[0],-1)/255.0
# One Hot Encoding the output labels. tf.keras has a function to achieve this

y_train=tf.keras.utils.to_categorical(y_train)
y_test=tf.keras.utils.to_categorical(y_test)


//////////////////////////////////////////////////////////////////

=====================================================================


Model architecture:


We have 784 features. So input layer has 784 units.

Output labels are 10. So final layer will have 10 units.


We take 2 hidden layers.

Its upto us as to how many units we want to keep in our hidden layer. We may have to hyper tune this.

The idea is to use number of units somewhere in order of number of units in input layer.

We keep 2 hidden layers with 256 units



Here we have a straight stack/ sequence of layers. To do this tensorflow provides us sequential api.

Function API habdles more complex tasks. In functional, it is not necessary that layers are stacked sequentially.



/////////////////////////////////////////////////////////////////////////////////

==============================================================================


Building the model using Sequential API:


First we will use predefined layers.


The hidden layers that we discussed of using in previous class are pre configured and are called dense layers.

Here we will simply have to provide the number of hidden units and the activation function for these dense layers.

//////////////////////////////////////////////////
Importing:

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

//////////////////////////////////////////////////


First we need to create a model object.

This helps in initializing the model.


///////////////////////////////////////////////////////


We use relu activation fn here.


Use Diagram 11 for more intuition.

The formula  for  Relu(x) is   max(0, x)

///////////////////////////////////////////////////////////////

Defining the model:


model=Sequential()

model.add(Dense(256, activation='relu', input_shape=(784,)))             # only in the first layer we need to add input_shape
model.add(Dense(256, activation='relu'))
model.add(Dense(10,activation='softmax'))  # For binary classification, we use sigmoid activation fn. For Multiclass classification, we rely upon softmax activation fn.


//////////////////////////////////////////////////////////////////////////////////


Compiling the model:

#  We use adam optimizer for optimizing the weights
#  Loss fn: For Binary classification we may use binary cross entropy/ Log loss entropy. For Multi Class classification, we rely on categorical cross entropy.


# The categorical cross entropy loss fn is:   - sum yi*log(yi pred)

# This is exactly same as the logistic regression loss fn

 

 Code:

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) 

# The metrics are displayed after each epoch so that we have a feel of how a particular metric is changing after each

/////////////////////////////////////////////////////////////////////////////////////////////////////////////
Fitting the model:

# We can pass the number of epochs for which we want to run this model.
# We can also pass the batch size. If a particular batch size is provided, complete data isn't passed in one go. It is passed in batches.

# Code

model.fit(x_train,y_train,epochs=10, batch_size=1024)

# Sample output:

Epoch 10/10
59/59 [==============================] - 1s 13ms/step - loss: 0.0333 - accuracy: 0.9911
 

# Here 59 is the number of batches




//////////////////////////////////////////////////////////////////////////////////////////


# Evaluating the model:

model.evaluate(x_test,y_test)


////////////////////////////////////////////////////////////////////////////////////////////


========================================================================================


Building the model using Functional API:



Now we want to build a complex model.

Ex: Diagram 12.  See Diagram 12 for good visualization



We use functional API in this regards

//////////////////////////////////////////////////////////


Importing:

from tensorflow.keras import Model
from tensorflow.keras.layers import Concatenate, Input

To build these models you have to visualize in this manner: You have input layer. You have to pass data from one path. We pass data through
other paths. Then we use concatenate to concat these different paths. Then we pass the concatenated units through final output layer.

 So we have to understand how our data flows through our model using functional api.


/////////////////////////////////////////////////////////////////////////////////////////


# Here we will have to define the layers as well as perform the connections b/w the layers as well.

Defining the layers:


input_layer=Input(shape=(784,))

hidden_layer1=Dense(256, activation='relu')(input_layer) # hidden_layer1 represnts the output obtained from hidden layer 1
# a= Dense() (b)  , this creates a connection b/w layer b and layer a.

hidden_layer2= Dense(256, activation='relu')(hidden_layer1)
hidden_layer4= Dense(256, activation='relu')(input_layer)

concat=Concatenate()([hidden_layer2,hidden_layer4]) # hidden layer2 has 256 units, hidden layer 4 has 256 units, the concatenate layer will now have 512 units
output=Dense(10, activation='softmax')(concat)

//////////////////////////////////////////////////////////////

Defining the model


# Function footprint to create a Model object, Model(input,output)
model=Model(input_layer,output)



//////////////////////////////////////////////////////////////////

Same steps as before.


Compiling:

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

Fitting:

model.fit(x_train,y_train,epochs=10,batch_size=1024 )


//////////////////////////////////////////////////////////////////////////


==========================================================================================


Creating custom layers:

We will use sequential api to work this.

When we built the model using sequential api, we built the model in this way:

Input(784 units) -----------------------------------------> hl1(256 units) ----------------------------------------> hl2(256 units) -------------> op(10 units)
		   784*256 + 256 paramters			 256 * 256 + 256 parameters			256*10 + 10 paramters


 

//////////////////////////////////////////////////////////////////////////////////////

Why we create custom layers??
Doing computations in our layers which are not covered by predefined layers.

////////////////////////////////////////////////////////////////////////////////

First of all we import layers api from tenserflow.keras.

We have Layer class in layers api.

Whenever we want to create our custom layer, we create a subclass of this Layer class

////////////////////////////////////////////////////////////////////////////////

Importing:

from tensorflow.keras import layers

/////////////////////////////////////////////////////////////////////////////////////////


Constructing the class:


The constructor:

First we build the constructor.

In constructor we initialize all the layer parametes like no of weight, bias , the activation fn etc.
First we call the constructor of parent class
Then we initialize the layer parameters



    def __init__(self, units, input_dim, activation):
        super(MYLayer, self).__init__() # Calling constructor of parent class
        

       # Setting the layer attributes
        
        w_init= tf.random_normal_initializer()                   # Randomly initializes the weights using normal distribution
        self.w=tf.Variable(initial_value=w_init(shape=(input_dim,units), dtype='float32'), trainable=True)
        b_init=tf.random_normal_initializer()
        self.b=tf.Variable(initial_value=b_init(shape=(units,),dtype='float32'), trainable=True)
        self.activation=activation


///////////////////////////////////////////////////////////////////////////////////////////////////////


Forward pass: For each we need to define a forward pass. We already have an implentation in layer class.
We can overload it. All the ops with regards to forward pass can be added here.



The call function defines the main computation. The main computation for a model is calculating the output from a layer.

def call(self, inputs):
        linear_op=tf.add(tf.matmul(inputs,self.w), self.b)             # getting wTx+b
        if self.activation=='relu':                   # Applying the actibvation function
            return tf.nn.relu(linear_op)
        elif self.activation=='softmax':
            return tf.nn.softmax(linear_op)




////////////////////////////////////////////////////////////////////////////////////////////////

================================================================================


Creating custom model:


class Mymodel(tf.keras.Model):
    def __init__(self, n_input, n_hidden1, n_hidden2, n_classes):
        # Calling the parent class constructor
        super(Mymodel,self).__init__() 
        
        # Setting the model attributes
        self.layer1= MYLayer(n_hidden1,n_input,'relu')
        self.layer2=MYLayer(n_hidden2,n_hidden1,'relu')
        self.out_layer=MYLayer(n_classes, n_hidden2, 'softmax')
    
    # Main computation
    def call(self,inputs):
        x=self.layer1(inputs)
        x=self.layer2(x)
        return self.out_layer(x)


In call function we perform the main computation. For model, the main computation is which layer takes which inp[ut

////////////////////////////////////////////////////////////////////////////////////////



===================================================================================


Initializing and training the model:

model=Mymodel(784,256,256,10) # creating model object
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(x_train,y_train,epochs=10, batch_size=512)

model.evaluate(x_test,y_test)

==================================================================================


We have seen 3 approaches

1) Sequential
2) Functional
3) Custom


Custom majorly used for research purpose. Functional does most of the job for us.


========================================================================================
Keras:

Introduction:

Easier than tensorflow.

Keras: Layer over tensorflow



		Keras
		Tensorflow


Easier to write code in Keras.

////////////////////////////////////////////////////////////////////

In most cases keras works. We may need to use tensorflow if we want something specific.



=====================================================================================


Flow of code in keras:
 
1) Creating a Model
2) Define Architecture ->  How many layers, Units, Activation fn etc. We provide number of units in each layer over here.
3) Compile -> Initailize weights and other parameters. We define the loss function and optimizer here
4) Fit ->  We define batch size , no of iterations here
5) evaluate, predict etc


======================================================================================

Keras Models:

1) Sequential Model:  Layers stacked one after the other
2) Model( Functional API) : For more complicated architectures

/////////////////////////////////////////////////////////////////////////


Code:

#Importing the model
from keras.models import Sequential


#Creating the model
model= Sequential()

==================================================================================


Layers:

Before layers we decide on the architecture first.

We use Breast Cancer dataset.

Each row in this has 30 features.

///////////////////////////////////////////////////////////////

Architecture:

Diagram 13


/////////////////////////////////////////////////////


We apply relu activation fns on hidden layer 1 and hidden layer 2.

********
If we don't mention the activation fn, no activation fn is applied.
We can say an identity fn is taken as activation fn.

//////////////////////////////////////////////////////////

B/w Hl1 and HL2 dimension of weights will be 32*16

B/w HL2 and O/P layer, dimension of weights will be 16*1

B/w input layer and HL1, dimension of weights will be 30 * 32

//////////////////////////////////////////////////////////////

If we don't know the shape of input, we cannot know the shape/ dimensions of paramters.
Hence just for the first layer, we need to provide the shape of input layer as a parameter.

////////////////////////////////////////////////////////////////

use_bias: This paramter tells if we want to add a bias added into the layer or not. By default, this is True

////////////////////////////////////////////////////////////////////////

regularization: Do we want to have any regularization or not?

////////////////////////////////////////////////////////////////////////////

initialize weight: How do we want to initialize the weights

///////////////////////////////////////////////////////////////////////////////////////


constraints: any constraints or not?

///////////////////////////////////////////////////////////////////////////////


Default Neural Network is called a Dense Layer

///////////////////////////////////////////////////////////////////////////////////

Code:


from keras.layers import Dense

layer1=Dense(units=32, activation='relu', input_dim=30)
model.add(layer1)
model.add(Dense(units=16, activation='relu'))

model.add(Dense(units=1, activation='sigmoid'))



///////////////////////////////////////////////////////////////////////////////////



=========================================================================================

Compiling the model:

We need to provide 3 main things:

1)  Optimizer
2) Loss Function
3) Metrics:  Print out the metrics withe ach epoch. We can provide a list of all the metrics we want to print.



Here we will use the loss function as crossentropy. For binary classification we use binary_crossentropy.

If we were working with Regression, we could use Regression.


////////////////////////////////////////////////////////////////////////////////////////


Code:
model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])

/////////////////////////////////////////////////////////////////////////////////////////////////



====================================================================================


Fitting Training Data:

Required arg: x and y

Option arg:

epochs: No of times we want to run forward propogation and backward propogation. By default epochs is 1

batch_size= If we want to update the weights without seeing complete input dataset. By default it already uses batch_size and value is 32.


Suppose we have batch size as 50 and no of datapoints are 1000.
For 1 iteration, we will go through our data 40 times. For each time, weights will be updated.

 
Validation test: We can give our testing data here as well. After each epoch it tells us, how is the accuracy looking like on testing data. Doesn't use validation data to train.


//////////////////////////////////////////////////////////////////////////

Code:


model.fit(x_train,y_train,epochs=20,batch_size=50, validation_data=(x_test,y_test))

////////////////////////////////////////////////////////////////////////


When trying to fit the model, we should initialize the weights again as well.

//////////////////////////////////////////////////////////////////////////////



Ques.

How many times will the weights get updated for a network if we provide it 1000 data points with 40 epochs and 20 as the batch size?

Answer : 2000

Soln: No of weight updates per epoch:  1000/ 20 = 50.
Total epoch: 40

/////////////////////////////////////////////////////////////////////////////



Predict/ Evaluate:


predict(x)  ----------->  y

given some input x, it returns us prediction y.

/////////////////////////////////////

Evaluate(x_test, y_test):  Returns us the score

Returns the loss value & metrics values for the model in test mode. Same metrics are provided back to us. Also loss is also provided to us.


Computation is done in batches (see the `batch_size` arg.)


Just like the score function of sklearn
/////////////////////////////////////////////////////////////

Code:
# Predict
model.predict(x_test)

# Evaluate
model.evaluate(x_test,y_test)

==========================================================================


CNNS 1:


Problems in Handling Images:


Little different than basic neural networks.

CNNs are state of the art for lot of image processing tasks.

=======================================


When we have images in our computers, we process the images as pixels.

Each pixel is treated as a feature.



1 2 3 4 5
6 7 8 9 10
11 12 13 14


But when we were working with images earlier, we didn't consider the fact that pixel 1 is very close to pixel 2 and 6.

We didn't consider the fact that some pixels combined form some curves in the image.

But that's not how images look to human brain. We perceive a colloection of pixels forming something meaningful.

So we want to look at combination of pixel values.


/////////////////////////////////////////////////////////////////////////////////////////


One obvious way is we add a preprocessing step and combine some pixels.


But if we do this in preprocessing we will have to figure out how to do this ourselves.
How much weightage  for a collection of pixels is to be given, will have to be decided by ourselves as well.


For example we can have something like:

sum(wi*pi)

For some weight and some pixel value.
But we won't have any way to get best weights.

We want the network to learn these weights.

Neural Networks give us this capability, they club the pixels together and learn the weights as well.

//////////////////////////////////////////////////////////////////////////////////////////////////////



Ques:

A Convolution Neural Network processes images -

Answer:

a group of pixels at a time.

///////////////////////////////////////////////////////////////////////////////////////////////////////////



Ques.

What is the problem with treating each pixel individually for image classification

Answer: 
Some pixels combined together generate patterns which are not recognized if treated seperately.


///////////////////////////////////////////////////////////////////////////////////////////////////////////////

====================================================================================================


Convolution Neural Networks


Like we have units in ANN, we will have units in CNNs as well.


We have 1 CNN unit, when we pass an image through this, it should do some changes in the imgaes, maybe even generate 
a new image which somehow captures not only the pixels but a combination of pixels.
output may still be a 2d array, maybe of same size or different size but it should apply some kind of transformation on the original image based on nearby pixel values.


How CNNs apply this??

CNNs apply a filter on the image. Filter is a k*k matrix( this can be rectangular as well  but generally is a square) 

Filter has weights

 	
	w00 	w01	w02
	w10	w11	w12
	..	..	..	
	

Clearly k is 3.

We will pass this over the image.

Refer Diag 14 for good intution

We may start from the top left. We may decide how we wanna shift the filter over the image.

We can shift the filter by 1 or by 2 and so on.



But whenever we put this filter on some pixels, say the first box in Diagram 14, we will get a list of values.

It looks like:     sum      (      sum (wij * pij) ) 
                      j=0 to k-1         i = 0 to k-1

pij are the pixel values.
wij are the weights.


We get an output from this summation . 
We put that output in one of the parts of our complete output.

Refer diag 14 for good visualization

/////////////////////////////////////////////////////////////////////////////////////


We start with randome weights.

Our CNN learns these weights by doing back propogation

Intuition:
If lot of images of a particular class have some particular curve, the weights of CNN corresponding to that particular layer will get 
more weights.

We will not have one single unit. We will many units. The hope is that some of these units will start learning curves, some of these will start learning
strainght lines etc and so on.


Each of the unit gives a different transformation of the original image.

//////////////////////////////////////////////////////////////////////////////////////////////////////////////


Self intution:
We can say that the model cannot predict completely by just using the raw data as raw data might be too complicated.
For ex: Logistic Regression only predict single decision boundaries.
But we can simplify this and get some new features with these hidden layers. Now if we try to figure out with the help of these features 
NNs can easily predict.

For ex: we are predicting cats and dogs with NNs. It might be very hard for algo to get the prediction right with just the images.

But the hidden layers can get some new features. For example they can identify a curve like their nose, ear etc which may be common to individual class.
Now, with the help of these curves/new features its easy to predict which is cat and which is dog. 
 

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Each k*k filter in a convolutional unit is used to


Identify a pattern anywhere in image.

The different units in the CNN hidden layers will learn some individual patterns


///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

==================================================================================================

Strides and Padding:


We will always have to decide upon the dimensions of filter k. filter can be rectangular as well, but square is preffered.



We also have to decide upon strides and padding.

Suppose we have an image of dimensions   30*30. 
We have a filter of 3*3.


Strides: How much should the filter be shifted after applying filter on one set of pixels.

Diagram 15.

We have applied filter on 0,0 to 2,2.
Which should be the next set of pixels on which filter is applied.

If stride is 1, we chose 0,1 to 2,3
If stride is 1, we will have a result of 18*18 as we will be ably to filter 18 times row wise and 18 times column wise.

This is because if we are not using padding the maximum starting point for a set of pixels can be 17. Here pixels are ranging from 0 to 19.




If stride is 2, we chose 0,2 to 2,4 next.

The result of applying filter will be 16*16 now.
 


We can even shift by more than 3 and we will skip some of the pixels altogether. But this is not desired and shouldmn't be done.


////////////////////////////////////////////////////////////////////////////


Generally if we have a stride s, we reach at somewhere around   n/s.


/////////////////////////////////////////////////////////////////////////////////////////


Hence if we increase our stride, the output is smaller.

////////////////////////////////////////////////////////////////////////////////////////

If stride is 1, we get nearly the same output.
Generally we keep stride 1 only, but we can experiment with this.

///////////////////////////////////////////////////////////////////////////////////////////



Padding:

Diagram 16.

Some pixels contribute more compared to others because of their location on the image.
To make sure all pixels play important part, we apply a padding layer of 0s around the image.

This ensures we don't have a bias again corner pixels.


////////////////////////////////////////////////////////////////////////////////////////


We generally have two types of padding

1) Valid: No padding. 0 padding. Whatever bias, we just live with it.

2) Same: We apply enough padding that if stride is 1, we get the same output size as input size.

If stride isn't 1, size will be n/s



Along with this we have to choose k as well. It also impacts the size of output as well.



We should try to different strides and padding and see which works best for us.

///////////////////////////////////////////////////////////////////////////////////////////////

==============================================================================


Generally if we have a filter size as k, image as n and stride as s, the output size is:

output size= (n-k)/s



=========================================================================================

Channels:


We have assumed our images to be squares.

But images can be 3d arrays as well.


If we have colored images, we will have 3 2d arrays each represnting one of the RGB channel.

Effectively we will have a 3d array as input.


We generally call these 3 layers as channels.


/////////////////////////////////////////////////////////////////////////////

How do we deal with this in CNN??

Suppose we have a colored image.

2 ways to apply filter:

1) Apply the 2d filter along x axis, yaxis and also apply on z axis as well.

2) Make the filter 3d as well. The dimensions of filter will be k*k*3


We take the 2nd approach.


The filter box will completely fill the depth. There will be no space to move along the depth.



///////////////////////////////////////////////////////////////////


Suppose we have an 28 * 28 *3 image, with padding as same and stride as 1. Suppose k=4.The filter needs to be of size k=4.

***
The output will be 28*28*1



=======================================================================================


Even if we are not working with rgb images, we still use this concept


Suppose we have an image. We have 5 units in the layer.

		

		Unit 1		Output1
		Unit 2		Output2
Input	------->	Unit 3   -------->	Output3
		Unit 4		Output4
		Unit 5		Output5



Each output will be 2d. But we can treat the outputs as channels. We can combine them before we feed them to next layer.


So suppose we had a 28*28*3 image

We have 5 units in first layer.

	5 units  in the layer, assume stride as 1 and padding as same
28*28*3-----------------------------------------------> Each unit produces 28*28*1 image. We have 5 units. Combine all to get 28*28*5 before feeding to next layer. Each unit in the next layer gets this 28*28*5 resultant entity.




Diagram: 17 for better visualization


==========================================================================================



Pooling layer:

Very simple layer, added after CNN layers that reduce the size of images.

Very standard way is Avg pool / Max Pool. Max pool is the most popular.



We go through first k*k box. We find the max out of this box. The max obtained serves as 1 unit of the output of the layer.

Then we shift. We don't shift by 1, because if we do that size still remains same.
Default is that once we have taken care of this k box, move to the next box. Essentially we shift by k. 


///////////////////////////////////////////////////////////////////////////////////////////////


Generally we use Convolution Layer and Pooling Layers in tandem/ together.


//////////////////////////////////////////////////////////////////////////////////////////


Ques. Pooling Layer Output:

If our image size is 18x18 and pool size 3x3, what will be the dimensions of the output from the pooling layer?

Output:
6*6

/////////////////////////////////////////////////////////////////////////////////////////////


=========================================================================================

Data Flow in CNN:

				  28*28*10                      14*14*10                            		          14*14*20                    7*7*20
Image (28*28*3) --------> 10 units CNN ------------------> PL ----------------------------> 20 units CNN --------------------------> PL----------------> output
		         stride =1, pading=same           stride=k, k=2                              depth =10(no of units in prev 
						padding=same		layer), stride=1, padding=same
Pooling Layer is not about units. We don't learn anything in the pooling layer.


The pooling layer doesn't affect the no of channels. The number of channels before going into the pooling layer remains same as that of 
coming out of the layer.

////////////////////////////////////////////////////////////////

If we don't keep padding as same, we can get the new dimensions for the image by using:


(n-k+1) / stride

///////////////////////////////////////////////////////////////////////


                                                                                                                                              3*3*64             1*1*64
28*28*1 --------------------> 32 units--------------> pl-------------------> 64 units----------------> pl
                                             s=2, k=2                             k=2          7*7*32         s=2,k=2                     k=2



///////////////////////////////////////////////////////////////////




Completing our network:

Here we complete the network to get to the final classifications.

Lets follow from the previous lecture.

            										                7*7*20
Input layer --------------------> CN Layer 1-------------------------> PL1 -------------------> CN Layer 2------------> PL2--------------------> Flatten the output to 1d array------------------> Dense Layer---------------> Output Layer



Intution:

We play with data, change the data using the CNN layers and then we flatten this out and after that we have our standard Neural Network.


///////////////////////////////////////////////////////////////////////////////////////////////////


Suppose we have layer 1 and layer 2



layer 1----------------------> layer 2


If layer 2 has 5*5*3 filters and 10 units, than we have to learn 10*5*5*3 paramters. Each unit has its own seperate filter.


///////////////////////////////////////////////////////////////////////////////////////////////////////

Its not necessary that we always add a Pooling layer after Convolution Layer.


///////////////////////////////////////////////////////////////////////////////////////////////////////


A Convolutional Layer transform the images and we get new features.

////////////////////////////////////////////////////////////////////////////////////////////////////////

==========================================================================================

CNN2:

Architecture of CNN:

We will use MNIST dataset again.

70 k images

55 k images in training dataset.

/////////////////////////////////////////////////


How architecture looks for our model:

Input layer ------------->   Convolutional Layer ,   32 units, stride=1, padding = same -----> Output: 28*28*32 ---------> Max Pool Layer , k=2 , padding same ------------> Output: 14*14*32------------------> CN layer 2-, units =64-----------> Output: 14*14*64 ----------> Max Pool Layer, k =2 ------------------> Output: 7*7*64--------> Dense Layer, Reduce the number of features to be kept as we have too many features------>  Output : 1024----------------------->  Output layer (10 units)
28*28*1																
 


///////////////////////////////////////////////////////////////////////


====================================================================================

Ques??
How many parameters are needed at CNN layer 1 (as discussed in video) with 32 units. Assuming that we aren’t using any bias on CNN layers. There is one channel and we are using square filter with width and length as 5.


Output: 800
Weights for one unit are  5\*5\*1. Thus weights for 32 units will be  5\*5\*1*32 = 800

=========================================================================


Ques ??

How many parameters are needed at CNN layer 2 (as discussed in video) with 64 units. Assuming that we aren’t using any bias on CNN layers. There is one channel and we are using square filter with width and length as 5.

Answer: 51200
Weights required for one unit are 5*5*32. Thus weights for 64 units will be - 5*5*32*64


================================================================================


Ques??

How many parameters are needed at Dense Layer(as discussed in video) with 1024 units. Assuming that we will use bias for every unit in Dense Layer.

 7*7* 64*1024 + 1024 = 3212288

=================================================================

Ques:

In the current architectures, let’s assume that we have input images of size 64*64 and we use a stride 
of 2 in both convolution layers. Keeping rest of the parameters same in the architecture. 
How many parameters are we going to train our architecture on?



Answer: 	1111850

Solution Description

	Parameters need at every layer 
	CNN Layer 1 :  5 * 5 * 1 * 32  = 800
	CNN Layer 2 : 5 * 5 * 32 * 64 = 51200
    Dense Layer : 4 * 4 * 64 * 1024 + 1024 = 1049600 
    Output Layer : 1024 * 10 + 10 = 10250

    Ans : 800 + 51200 + 1049600 + 10250 = 1111850



No weights are learnt in the pooling layer

///////////////////////////////////////////////////////////////////////////////////


==================================================================================================


Initialing the weights:

Loading the weights and initializing the weights.

///////////////////////////////////////////////////
Code:

# Storing Constants
# If anything needs to be changed, we can simply change from here

input_width=28
input_height=28
input_channels=1
input_pixels=784

n_conv1=32
n_conv2=64
stride_conv1=1
conv1_k=5
conv2_k=5
stride_conv2=1

max_pool1_k=2 # 2*2 max pooling
max_pool2_k=2

n_hidden=1024
n_out=10


input_Size_to_hidden=(input_width//(max_pool1_k*max_pool2_k)) * (input_height//(max_pool1_k*max_pool2_k))* n_conv2



# Creating weights dictionary

# Creating weights dictionary


weights={
    # For the first layer, we need 5*5*1*32 weights , 5 * 5 is the filter size, 1 is the depth, 32 are num_units in CNN layer
    
   
    "wc1": tf.Variable(tf.random.normal(shape=[conv1_k,conv1_k,input_channels, n_conv1])),
    
    # No weights for max pooling
    
    # For 2nd Convolutional Layer, filter is conv_2*conv2, depth is num units from first layer, n_conv2 is the num units in this layer
    #initializer=tf.random_normal_initializer()
    "wc2": tf.Variable(tf.random.normal([conv2_k,conv2_k,n_conv1, n_conv2])),
    
    # No weights for max pooling layer
    #initializer=tf.random_normal_initializer()
    "wh1": tf.Variable(tf.random.normal([input_Size_to_hidden, n_hidden])),
    #initializer=tf.random_normal_initializer()
    "wo": tf.Variable(tf.random.normal([n_hidden,n_out])),
}

biases={
    
    "bc1":tf.Variable(tf.random.normal([n_conv1])),
    "bc2": tf.Variable(tf.random.normal([n_conv2])),
    "bh1": tf.Variable(tf.random.normal([n_hidden])),
    "bo": tf.Variable(tf.random.normal([n_out]))
    
}


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

======================================================================================


Forward Propogation:


def cnn(x, weights, biases):
    x=tf.reshape(x, shape=[-1, input_height, input_width, input_channels]) 
    # No need to pass all 4 values. If we pass n-1 values to reshape, last one gets inferred by default
    
    conv1= conv(x, weights['wc1'], biases['bc1'])
    
    conv1_pool = maxpooling(conv1, max_pool1_k)
    conv2= conv(conv1_pool, weights['wc2'], biases['bc2'])
    
    conv2_pool = maxpooling(conv2, max_pool2_k)
    
    hidden_input= tf.reshape(conv2_pool, shape=[-1,input_Size_to_hidden])
    
    hidden_output_before_activation= tf.add(tf.matmul(hidden_input,weights['wh1']), biases['bh1'])
    hidden_output= tf.nn.relu(hidden_output_before_activation)
    
    # For output we are not applying any activation
    output = tf.add(tf.matmul(hidden_output,weights['wo']), biases['bo'])
    
    return(output)


//////////////////////////////////////////////////////////////////////////////////


Writing Convolution Function and maxpool functions:



In case of convolution we need to do:

1) Do the convolution i.e. make our filter pass through the image
2) Add the biases
3) Apply the activation functions

//////////////////////////////////////////////////////////////

Code:

def conv(x, weights,bias, strides=1):
    # Apply the filter
    out= tf.nn.conv2d(x, weights,padding="SAME",strides=[1,strides,strides,1])
    # strides arguement needs to ba list of the same shape as input shape. 
    # Input shape is num_examples*height*width*depth
    # For num_examples we keep stride as 1 as we operate on 1 image at a time
    # For depth, we have decided that we won't have strides along the depth, so tensorflow says that we keep it 1 as well
    # For strides along height and width, we keep them as k
    
    #Adding the bias
    out=tf.nn.bias_add(out,bias)
    # Can also use tf.add() but we have used tf.nn.bias_add() as this gives us more options, we can add different types of data as well.
    # Hence we have used bias_add
    
    
    # Applying the activation fn
    out=tf.nn.relu(out)
    
    return(out)
    
    
def maxpooling(x, k=2):
    out=tf.nn.max_pool(x,padding='SAME', ksize=[1,k,k,1], strides=[1,k,k,1])
    # ksize stands for window size. The window/filter which applies the max pooling
    # Standard format is num_features*height*width*depth
    # We don't want to window to overlap different images hence 1st parameter is 1
    # Along height and width we want window to be k units.
    # Along depth as well, we don't want to overlap channels.
    
    
    
    return(out)


//////////////////////////////////////////////////////////////////////////////////////////


===========================================================================================



Completing rest of the code:









===========================================================================================


Regularization using dropout:

Dropout: Layer we add in front of some layer


Input Layer ------>  HL1--------> Drop out layer -------------> Hl2------------------------> Output



In drop out layer, suppose we keep probability as 0.8

What this essentially means is that for each forward and backward propogation there is 80% change a particular unit will be kept and a 20% change that a particular unit will be
dropped.


We use dropout layer only while training. While testing dropout isn't used.


Intution:

In Dropout, we randomly drop some of the neurons in the NN in the training phase. 
Let us say we randomly drop 50% of the neurons in our NN. Now, the job of classification has to be done by the remaining 50%
of the neurons. 
And these neurons cannot slack off and pick up Less Significant and Noisy Features, as they would not help us in 
classification and the likes of Gradient Descents would penalize the NN more if the features picked up do not help in 
classification. Hence, these remaining neurons are forced to pick the Most Significant Features, which will reduce 
overfitting and generalize better.


The idea is that our model will be more robust.
Even if our testing data doesn't have some art of training data, performance should be good.


Very effective in case of neural networks.

Sometimes it helps, sometimes it doesn't. We should experiment with it.

///////////////////////////////////////////////////////////////////////////////////////////////////////////

=====================================================================================

Adding dropout layer:


We add dropout just after the dense layer

///////////////////////////////////////////////////////////////
hidden_output_before_dropout= tf.nn.relu(hidden_output_before_activation)
    
   # Adding droput layer
hidden_output= tf.nn.dropout(hidden_output_before_dropout, keep_prob=keep_prob)
    

//////////////////////////////////////////////////////////////////////


=============================================================================

Keras CNN:

1) Get MNIST data
2) Reshape data in our desired format
3) Use Conv2d layer
4) Max Pooling layer
5) Drop out layer, it requires the drop probability
6) Flatten our data, use flatten




===============================================================================================


RNN:


How to handle sequential data:

We don't have right tools yet to deal with sequential data.



Ex: 
From some corpus do Text generation

generate some music. From first few notes, generate further notes in sequence.


/////////////////////////////////////////////////////////////////////


What do we have here??

1) Variable sized input
2) Variable sized output
3) has a sequence nature to it

///////////////////////////////////////////////////////////////////////

================================================================


Recurrent Neural Networks:




Intution:

We give 1st input, RNN stores some information corresponding to it, when we give new input to RNN that stored information
is used as well.


/////////////////////////////////////////////////////////////////////


For now, we assume that our input is a sequence with this form:


x1 x2 x3 x4...............................................


Output is also a sequence

y1 y2 y3......................................................



Both input and output are of same length.


We can even have scenarios where 

1) Input is single entity and output is a sequence.
2) Input is a sequence and output is a single entity.
3) Generic case where input is a sequence and output is  a sequence.

For this lecture, we use 3rd approach.


///////////////////////////////////////////////////////////////////////////////////////



Suppose we have  a RNN unit.

We feed x1 to it. It produces some output y1.

After that we feed x2. RNN unit uses the learning from x1 as well


When x3 is provided, y3 is produced. RNN uses inputs/learnings from both x1 and x2. These learning are clubbed together in a2.



The output is the Y1 Y2 Y3 obtained.


Diagram 19: For better intuition



///////////////////////////////////////////////////////////////////////////////////////////////////////////////


Repeating how its supposed to work:

1) We provide some dummy a0 and x1. It produces y1 and a1.
2) With a1 and input x2, we produce y2 and a2.
3) Using a2 and input x3, we produce y3 and a3.


****************************************************

For a particular input xi, it gets influence from all previous inputs because of ai-1.


******************************************************


Another way to imagine RNN w.r.t is:


We have 1 unit. We get input x1. It produces y1. When we get x2, to produce y2 it uses learnings from x1 as well.
Sequence is being created because of time. So for Time Series data, RNNs are a very good approach.



************************************************

For visualization its better to assume we have a series of units, just like we have in diagra 19. But in reality we may only have 1 unit.



==============================================================================


 How does RNN work??


1) First we have a unit. We feed a dummy a0 and x1. a0 can be even a simple array of 0s.
2) First we find a1. 

A1= A(Wxa X1 +  Waa A0 +b a) 

A1: a for next input
A: activation function
Wxa: Weight b/w input(x) and new a.
Waa : Weight b/w previous a and new a.
b a: bias for a.


3) Then we produce Y1. To produce it, we simply use previously produced A1
Y1= A' ( Way A1 + b y )

Way:  Weights b/w a and y.
b y:  bias for y.

//////////////////////////////////////////////////////////////////////////////////////////////////


This can be applied for any input:


A n = A(Wxa Xn + W an An-1  + b a)

Y n =  A' (Way An + by)


********************************************


*****
Even after unrolling i.e. represnting single RNN unit as in diagram 19,
we need to be sure that same weights are being used.


That is.



For single RNN unit ,  the weights are:

for x1:  Waa, Wxa and Way
for x2: Waa, Wxa and Way
for x3: same as above

and so on.

Weights differ with different layers and units and not with different inputs.


Diagram 20

****************************

RNN take a lot of data and time to train, but they are crazy good.


////////////////////////////////////////////////////////////////////////////////////////


===============================================================================================



Typical RNN structures:



We pass an input.

This input can have many features. Say there are 10 features.

Then we have a RNN layer.

Suppose there are 5 units there.


All the data goes to all the 5 units.



All these units produce some output.

Suppose we are doing Binary Classification and we feed all these to  a single layer. Suppose this layer is a Dense layer.


So after this Dense layer we obtain the output.



Now the input is a sequence of such inputs.


These inputs are supposed to be passed through the network.

Because of these sequences, output produced is also a sequence only.

We may be only interested in the final output.

For example we have a video of a man and we want to see what action the man is performing.





In some cases, we may be interested in the sequence itself.

Diagram 21

Different use cases may have different architectures.


///////////////////////////////////////////////////////////////


We can even have multiple layers of RNNs as well.


/////////////////////////////////////////////////////////////


For example: text generation:

We give a lot of data, feed to the model. We want the model to start producing data.


Most important thing in these type of problem is selecting the right model.


Suppose we have a large text corpus.

We decide to create sequence of 100 characters out of it. The next following character becomes the character that we are trying to predict.

We can create many samples like this.



For example we have data like this:

abcdefgh

Dataset is like:

abcd e
bcde f
cdef g
defg h



While doing testing/prediction:

Give some initial seed, suppose "this".

With this, the model precicts next character, say i.

Now we can create a new seed word  by replacing t(first char in "this") with "i"(predicted letter)

New word becomes hisi. We feed this again to the network to predict next letter.

Continuing so, we can create a sequence.


Diagram 22


We are creating many outputs from many inputs.
////////////////////////////////////////////////////////////////////////////

========================================================================================



Airline Data Analysis:


	# of passengers bt month

Jan 1949
.
.
.
.

.
.

1960


We want to predict how many people will travel going forward.


////////////////////////////////////////////////////////////////////////////////////


We won't use complete dataset to predict next prediction.

We will use smaller windows so that we have many datapoints. Although using all the data grants us more context but still we will 
try to get more datapoints.

/////////////////////////////////////////////////////////////////////////////////////



There might be seasonal trends as well, so last 4 months may not be able to get us a better 5th month.

So what we'll do is, 
1) we will divide our data into training and testing.
2) we will pick a window. We pick 1 year as the window.
3) We will train our model.
4) For testing:

Suppose we want prediction for 1 year.

What we do:

Predict for 1st month using our data.
For predicting next month, add our own prediction to the available data and using this new data, predict for month 2.

Do this for predicting all the 12 months.



////////////////////////////////////////////////////////////////////////////////////////////////////////
=============================================================================================================


Preparing Data for RNN:



In Jupyter notebook





/////////////////////////////////////////////////////////////////////////////////////////////////////

Setting up RNN model:




Default Activation fn used in case of RNN is tanh.


RNN implementation that we use is SimpleRNN.



What is the input shape??

The look_back, no_of_features is the input_shape

No of enteries: look_back
For each entry we have only 1 feature


1st dimensions: # of training data points
2nd dimension: length of sequence
3rd dimension: # of features in each training 


////////////////////////////////////////////////////////////////////


In RNNs, we use back propogation through time. Here, this is back propogation through time because our output depends on previous dates.


===============================================================================

LSTMs





Vanishing and Exploding gradients:

Very common problem that we may face in case of RNNs and very deep neural networks.

In case of RNNs this becomes very prominent as we may be using the same unit again and again.


The problem is vanishing/ exploding gradient.



Suppose we have a very deep neural network.

When we try to pass the error back through the layers, we multiply by some number. So for initial layers of network we have a problem.


If gradients are greater than 1, we end up with very large numbers. We have exploding gradients in this case. Huge changes will be taking place after every iteration and we can even overflow the allowed values leading to nan values.


If gradients are smaller than 1, we end up with very small numbers. We have vanishing gradients in this case. 




///////////////////////////////////////////////////////////////////////////


In case of RNN:

							          output
		 wa                                    wa			               |
a0 ----->  RNN unit-------------> RNN unit------------> RNN unit----------------> RNN units
	|		        |                                    |
	x1		       x2                                  x3

So we  will see that effect of x1 on final output will be very very less if  gradients are less than 1.
Also the changes in weights for x1 will be very very less.


In case of RNNs we face the problem of vanishing gradients a lot more.



Exploding gradients problem is easier to solve but vanishing gradients problem is a little harder to solve.



Repercusion:
Suppose we are working with a paragraph.

The para starts with   "A French Man..........."

The para end with " He likes in India...................."

And we have to predict  "He is fluent in ........................"

The ending line will affect the output  a lot more than first line



////////////////////////////////////////////////////////////////////////////////////////////////////////////


For solving exploding gradients:
We can do gradient clipping. We can have an upper limit saying that we wont change our weights by more than a particular ammount.
Ex: If gradients are comming to be 10,000 but we wont change more than 50.


Much harder to solve vanishing gradients


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////

========================================================================================================



Ques.

Vanishing gradient problem is faced when output is generated by passing input to a large number of hidden layers , why does this problem occurs in RNN where a single layer of RNN units is used:

Output of RNN layer is decided by back propogation on all its initial values resulting in multiple updates



=========================================================================================================


Gated Recurrent Units:


GRUs


What it says:


So far we had 

some input xi incoming, we used ai from previous input. We produced ai and after that using this ai and xi, we produced yi.



Diagram 23


/////////////////////////////////////////////////////////////////////////////////////


In GRU: 

Problem is of vanishing gradients

Problem is coming because at every step, we are using new inputs to do some changes.

When we come back, we have to multiply those weights  back.

What GRU says is, we should have a way to pass ai-1 directly as ai without making any changes.


What happens in GRU:

We produce a potential a (a^) using a-1 and xi.

We also have a gate(uses sigmoid, sigmoid gives values b/w 0 and 1)

Depending on this gate we either use, ai-1 or a^ as ai.

We can call this gate as update gate.

//////////////////////////////////////////////////////////////////////


Eqns:

a^ i = tanh(Waa * a i-1  + Wxa * (xi) + b)

u = sigmoid (W' a *  ai-1 + Wx' * xi + b2  )    # W'a and Wx' are different weights compared to a^ i

ai =  u * a^i + (1-u)* ai-1


If u is 1, a^i is used, else ai-1 is used as new ai.

/////////////////////////////////////////////////////////////////////////////

Intution:
We will let our network decide based on some parameters, if it wants to use new a or the old a.

//////////////////////////////////////////////////////////////////////////////


===================================================================================================

Variations of GRU:



a^ i = tanh(Waa * a i-1  + Wxa * (xi) + b)


u = sigmoid (W' a *  ai-1 + Wx' * xi + b2  )    # W'a and Wx' are different weights compared to a^ i

ai =  u * a^i + (1-u)* ai-1


yi = A(ai)  # Final Output



Variations:

1) We can have a reset gate

reset_gate= sigmoid(W r xi + Wr' ai-1 +br)  # W r , Wr' and br are reset gate weights.. Calculating in same way as update gate

Using this reset_gate, a^i is calculated as:

a^ i = tanh(Waa * a i-1 * reset_gate + Wxa * (xi) + b)


reset_gate tells how much ai-1 contributes to a^.


2) Sometimes for update and reset same gates are used. The gate is then called forget gate.


We can add more gates as well. The more gates we add, the results may be good but we have to train many parameters.
GRUs with 1 or 2 gates perform very well anyways


We can have many variations but core idea is, we should have a way to pass the information by not changing anything in it.

///////////////////////////////////////////////////////////////////////////////////////////



==============================================================================================

LSTM:

Long short term memory

The idea is that we should be able to mantain some memory for a long period of time.

Slight difference from GRU.

We have memory passing through a unit.
We also pass activation from a unit as well


Eqns:

C^t = tanh (Wcx   xt   + Wca   at-1 + bc)   # Potential new Ct.  Wc means weigts for c.  Wcx means weight for c w.r.t x

in = sigmoid( wix xt + wia  at-1  +bi)     # input gate, Tells what will be the contribution of C t-1 to  final C t)


f =  sigmoid ( wfx xt  +  wfa  at-1  + ba)   # forget gate



Ct= i * C^t + f* Ct-1              # Final Ct has some contribution from Potential Ct and old Ct



output = sigmoid(wxo xt + woa at-1 + b0)   # Tells us about the new activation coming from a unit.

at= output*A(t)



////////////////////////////////////////////////////////////////////////////////////////

============================================================================================


Qus.

The memory cell has some activation applied on to give C_t

Ans: False


/////////////////////////////////////////////////////////////////////////////

Ques.
The purpose of not applying activation on memory cell is to :

Answer:
Prevent the vanishing Gradient as without activation, weights are not changed drastically when back propogated through time


///////////////////////////////////////////////////////////////////////////////


===============================================================================================


Unsupervised Learning:

In case of Unsupervised learning, we won't have labels.

We have to find patterns without the labels.


In case of Unsupervised we only have input data and not labels.


PCA is a type of unsupervised learning only because we only used the input data there and not the target labels.


///////////////////////////////////////////////////////////////////////////////////////
Another method is Clusttering

We will form clusters on some property. As this is unsupervised method, we will make sense out of the clusters after we are done with training.


///////////////////////////////////////////////////////////////////////////////////////



Clustering will be all about we are given some data and we want to make some clusters.


////////////////////////////////////////////////////////////////////////////////////////////


Clusters should have meaningful number of enteries


/////////////////////////////////////////////////////////////////////////////////


So in Unsupervised Learning we have:


1) Clustering

2) Anamoly detection.  For Ex: banks have lot of transactions, but some transactions are fraud. These fraud transactions are anomalies. This in a way is like ckustering only.


3) PCA

/////////////////////////////////////////////////////////////////////////////////////////

==============================================================================================



Introduction to clustering:


Clustering:


Problem:

We have a user. We want to show them an add such that chances of them clicking on that add is maximum


Soln: 1) We can use other supervised ML algos to solve this. tHERE ARE unsupervised ways as well which perform really well.




2) One thing we can do is get to know all users simmilar to a particular user. We can get the most popular adds among this cluster and display those adds to the user in question as well.
This can be termed as user cluster.



3) We can find out which adds this user has clicked on. Then using clustering we can get all simmilar adds to the earlier clicked add.

This can be termed as add cluster.


////////////////////////////////////////////////////////////////////////////


While doing cluctering, we always get one problem??

How many clusters do we want to make.
We will talk about one algo with which we can optimize this.

////////////////////////////////////////////////////////////////////////////////


Types of clustering:


1) Flat Clustering:   We have different clusters

2) Hierachial Clustering: All our data is in one cluster. This data is further divided into more clusters. Then again these clusters are further clustered.

We do clustering until we run out of data or some terminating condition is met.
In Hierarchial clustering, one datapoint doesn't belong to 1 cluster, it can belong to many clusters.

In hierarchial clustering, we don't need to decide k/ number of clusters.



=======================================================================

Using K means for clustering:



K reprsents how many clusters we want to make.
We use means.


What K means says:

If we end up with K clusters, for a single cluster, the distance b/w any point with the mean of all the points in the cluster should be minimized.


Mathematically:


Suppose we are done with clustering.

Our clusters are:

          S1    S2     S3                                             Sk
S - > { }  , {  } , {  }, .................................. {  } 

 The thing we want to  minimize:

sum (            sum                           ( ||   dj - mu i ||**2    )      )
i =1 to k       dj belongs to Si



We generally use Euclidean distance.


===========================================================================================


K-Means Algorithm:

Trying to get to K clusters.
These clusters are defined by their mean values.

/////////////////////////////////////////////////////////////////

Suppose we get a new datapoint and we have K clusters. How to decide which cluster this goes to?
We can use the means of the clusters to know which cluster this datapoint belongs to.

////////////////////////////////////////////////////////////////////


While working, we will just store the means. For any point, to know which cluster this point belongs to, just get the distance of this point with all the means.


////////////////////////////////////////////////////////////////


Algo:

1) Let's randomly K mean values.

2) Assign each datapoint a cluster. To do this, for any datapoint just get which mean is nearest.

3) Find out new mean values for these clusters.

4) Go back to step 2 and repeat. To terminate we can decide how many times we want to run this. Or we can have a terminating condn such that it runs as many times as it takes but we stop after there are no more changes to the clusters.

So we can stop after maximum number of iterations or we can stop after there are no changes to the clusters.

We can use combination of the two as well.


//////////////////////////////////////////////////////////////////////////////////////////////////



K means doesn't guarentee that we will end up with best possible clustering. If we pick bad starting points(bad initial means) we can have really bad clusterings as well.

So a lot of times we may run this algorithm multiple times. And we take the best clustering.
///////////////////////////////////////////////////////////////////////////////////////////////////////


=======================================================================================================


Using K means from SKlearn:



Importing:

from sklearn.cluster import KMeans


Creating the object:

k_means= KMeans(n_clusters=2) # Creating the clusters. By default n_clusters is 8


k_means.fit(X)  # Fitting on X



Some arguements:


pre_compute distance: By default auto. Does some calculation so that it doesn't have to do these calculations again.
Makes calculation fast but takes more memory.



init= How to initialize the means. By default uses algo: k-means++. This algo ensures that the initial means are spread out from each other.



n_init: By default 10. How many times to run this algo to get to the best clustering possible.



//////////////////////////////////////////////////////////////////////////////////////////////

k_means.labels_ :  Tells which point got into which cluster.


k_means.cluster_centers_:  Returns the mean values of the clusters.



//////////////////////////////////////////////////////////////////////////////////////////////////////




Code:

import numpy as np
import matplotlib.pyplot as plt

X= np.array([[1,2],[1.5,1.8],[5,8],[8,8],[1,0.6],[9,11]])


plt.scatter(X[:,0],X[:,1])
plt.show()

from sklearn.cluster import KMeans


k_means= KMeans(n_clusters=2) # Creating the clusters. By default n_clusters is 8

k_means.fit(X)


plt.scatter(X[:,0], X[:,1],c=k_means.labels_)
plt.scatter(k_means.cluster_centers_[:,0],k_means.cluster_centers_[:,1])
plt.show()

==================================================================================================================


Starter code for Kmeans:

1) List of means. Initialize randomly
2) Array of Arrays where each array represnts which data points are in cluster 1, which in cluster 2 and so on.
This helps in getting the new mean values.



==========================================================================================================


Implementing fit and predict functions:


def fit(data, k= 2, max_iter=100):
    means= []
    
    # randomly initialize the means. We will keep it very simply and just pick the first k datapoints
    
    for i in range(k):
        means.append(data[i])
        
    for i in range(max_iter):
        # assign the datapoints to the cluster that they belong to
        
        
        
        # creating empty clusters
        clusters=[]
        for j in range(k):
            clusters.append([]) # reassigning all the datapoints
            
        # calculate the new mean values
        # going through each datapoint
        for point in data:
            # find distance from all mean values.
            distance =[((point-m)**2).sum() for m in means]
            # find the min distance
            minDistance= min(distance)
            
            # find the mean which gives the min distance ---> l
            l =distance.index(minDistance)
            
            # add this point to cluster l
            clusters[l].append(point)
            
            
            
            
            
        # calculate the new mean values.
        
        change=False
        
        for j in range(k):
            new_mean= np.average(clusters[j], axis=0)
            if not np.array_equal(means[j], new_mean):
                change=True
            means[j]=new_mean
        
        if not change:
            break
        return(means)
    

def predict(test_data, means):
    predictions=[]
    
    for point in test_data:
        # find distance from all mean values.
        distance =[((point-m)**2).sum() for m in means]
        # find the min distance
        minDistance= min(distance)
            
        # find the mean which gives the min distance ---> l
        l =distance.index(minDistance)
        predictions.append(l)
    return(predictions)

=====================================================================================

Implementing K means class


Defining class:

class ABC:
	def f1()



We have this pointer in Cpp. We pass that arguement in case of python



def f1(self)   # self is the pointer that refers to the current object



//////////////////////////////////////////////


Constructor:


class K_Means():
    def __init__(self, k=2, max_iter=100):
        print("Constructor")
        self.k=k
        self.max_iter=max_iter
    
    def fit(self, data):
        self.means= []

        # randomly initialize the means. We will keep it very simply and just pick the first k datapoints

        for i in range(self.k):
            self.means.append(data[i])

        for i in range(self.max_iter):
            # assign the datapoints to the cluster that they belong to



            # creating empty clusters
            clusters=[]
            for j in range(self.k):
                clusters.append([]) # reassigning all the datapoints

            # calculate the new mean values
            # going through each datapoint
            for point in data:
                # find distance from all mean values.
                distance =[((point-m)**2).sum() for m in self.means]
                # find the min distance
                minDistance= min(distance)

                # find the mean which gives the min distance ---> l
                l =distance.index(minDistance)

                # add this point to cluster l
                clusters[l].append(point)
        





            # calculate the new mean values.

            change=False

            for j in range(self.k):
                new_mean= np.average(clusters[j], axis=0)
                if not np.array_equal(self.means[j], new_mean):  # Checking if there is a change in the means or not
                    change=True

                self.means[j]=new_mean

            if not change:
                break
            
      
    
    def predict(self,test_data):
        predictions=[]

        for point in test_data:
            # find distance from all mean values.
            distance =[((point-m)**2).sum() for m in self.means]
            # find the min distance
            minDistance= min(distance)

            # find the mean which gives the min distance ---> l
            l =distance.index(minDistance)
            predictions.append(l)
        return(predictions)
    



////////////////////////////////////////////////////////////////////////////////////////


We have used self pointer to pass values b/w the functions

//////////////////////////////////////////////////////////////////////////////////////


===================================================================================================


Unsupervised Learning 2:

How to choose optimal K:


The basic idea here is:

1) We would like to have -> Points in same clusters should be nearby.

2) Points in different clusters should be faraway.

//////////////////////////////////////////////////////////////////////////////////////////


==============================================================================


Silhouette distance for getting optimal K:


It defines three values

a(i)  :  average distance of point i from all points in same cluster as i.

b(i):  average distance of point i in the neighbouring cluster


S(i) = (  b(i) - a(i) ) / max( b(i), a(i))   # max(b(i), a(i)) allows us to scale



///////////////////////////////////////////
How we get b(i):

Suppose we have 4 clusters C1, C2, C3 and C4. Suppose point i sits in Cluster 1.

Avg distance b/w point i and C2: average of Distance of all the points in C2 with i.

Avg distance b/w point i and C3: average of Distance of all the points in C3 with i.

Avg distance b/w point i and C4: average of Distance of all points in C4 with i.


b(i) : Minimum of  Avg distance b/w point i and C2,  Avg distance b/w point i and C3 and Avg distance b/w point i and C4


///////////////////////////////////////////////////////////////////////////////



Looking closely at s(i) # silhouette  for i

S(i) :  (b(i) - a(i) )/ ( max(b(i), a(i)) )

:     1  -   (a(i)/b(i))     if   b(i)  >  a(i)

:  0                                if a(i) = b(i)


:   ( b(i)/a(i) )  -  1   if a(i) > b(i)

//////////////////////////////////////////////////////////////////////////////////



So , we have:

S(i) =

 1  -   (a(i)/b(i))              if   b(i)  >  a(i)  ,           Here if b(i) > a(i)  ,  S(i) < 1. At max S(i) can be 1 when b(i) >>>>>>>>> a(i)

  0                                if a(i) == b(i)


   b(i)/a(i) )  -  1            if a(i) > b(i) ,                      S(i) can be min -1, when a(i) >>>>>>>>>>>>>>>>>>> b(i)



/////////////////////////////////////////////////////////////


So  

-1 <=  S(i) <=  1


We want to reach 1, rather than -1.


/////////////////////////////////////////////////////////////////////////////////////////


****************************
We can get silhouette score for all the points for a particular clustering. We can take the average of all the scores for this particular clustering to get 
overall score for this clustering.

Then we can take the best possible clustering by comparing  the scores for different clustering.


We can also plot the silhouette scores to get the best possible clustering.

////////////////////////////////////////////////////////////////////////////////////////////


S(i) will be 0 if cluster containing point i has no other datapoint.


///////////////////////////////////////////////////////////////////////////////////////////////


make_blob:  allows us to build clusters around a particular datapoint

/////////////////////////////////////////////////////////////////////////////////////////


=================================================================================


Introduction to K medoids:

Little differences with K means.


1) It wants that we make an actual point in data as medoid.

2) We use l1 distance insead of l2 distance. l2 scores penalizes a lot as compared to l1 scores. So outliers may have very huge impacts.



///////////////////////////////////////////


========================================================
	
K - medoids:

1) Select K random mediods.

2) Assign each datapoint one of the clusters depending on their distance to medoids

3) Find replacement for our current medoids.

4) Go to step 2.




Finding replacement for our current medoids:


For a particular medoid.

Go to all the points in the same cluster.

How do we define if  areplacement is better or not??

We define a score.


score = sum     (       sum                       (    | dj - mi|       )   )
	i=1 to k     dj belongs to ci

For each point in a particular cluster we calculate this score.


For a particular new medoid, we not only see the difference with all the datapoints in the same cluster, but with all the points in the data.


We only select the new medoids if the overall scores in new clustering increase.


 ////////////////////////////////////////////////////////////////////////////////////////////////////////


===========================================================================================


Hierarchial Clustering:



We want to get this sort of a structure:


		node
	node		node

node		node



//////////////////////////////////////////////////////////

Each datapoint not only belongs to 1 cluster but it can belongs to many clutsers




/////////////////////////////////////////////////////////////////////////////////


Two ways of building these:

1) Top - down

2) Bottom - up


////////////////////////////////////////////////////////////////////////////////


Defining distance b/w 2 clusters in case of Hierarchial Clustering:

1) max distance:   max( d(x,y) )   where x belongs to c1 and y belongs to c2

2) min distance:  min (d(x,y) )    where x belongs to c1 and y belongs to c2


///////////////////////////////////////////////////////////////////////////////////

============================================================================


Top Down Approach/ divisive approach:


	n datapoints

divide into 2 parts.

Keep on dividing the datapoints till we reach the leaf nodes  i.e. only single nodes remain.


How to divide the datapoints??
For n datapoints we will have 2**n possible ways of dividing.

We can go through all the possible divisions and see which leads to max distance b/w the clusters.


But this is very expensive.

////////////////////////////////////////////////////////////////////////////
What we generally do:

We can do something like k means to divide our data. k means also doesn't gets us the best possible clusters.


//////////////////////////////////////////////////////////////////////////////////////////////////////////


What we can do:

Call K means to give us 2 clusters.
On each clusters call k means recursively.


/////////////////////////////////////////////////////////////////////////////////////////////


===========================================================================================

Bottom Up/ Agglomerative clustering:

It aggreagates clusters to reach final cluster.


Idea:
Each datapoint is individual cluster.
We try to combine these clusters 2 at a time to get a new candidate.


We merge those 2 clusters which are closest to each other.

We can use the distances already discussed earlier.



//////////////////////////////////////////////////////////////////////////////////////////////////////////


Very expensive algo
We should store pair wise distance before trying to build the tree.

O(n**2) space to store pair wise distances. O(n**2) space to build the tree.

O(n**3) Time Complexity.


We can use heaps to build this tree.

////////////////////////////////////////////////////////////////////////////////////////////////////////////////

Agglomerative clustering is already implemented in sklearn.
====================================================================================================


GIT:

Version Control System



Everytime we roll out a feature we create a copy of our folder.

But this is very hard to mantain.



Other solution can be we can ask a software to note down what changes are we making with each different roll out.

This noting down is done by GIT.


If some feature fails we can ask GIT to know what changes were made to the last stable version and roll back to that version.



/////////////////////////////////////////////////////////////////////////////////



When we have multiple team members:


GIT can oversee code changes done by many team members so that if one team member makes some change, others can know about conflicts as well as they 
may have older version of software.


///////////////////////////////////////////////////////////////////////////////////

Installing GIT:

///////////////////////////////////////////////////////////////
To make a folder as a GIT repository:


Command:

git init

Output:

Initialized empty Git repository in C:/Users/rachi/Desktop/Git Practice/.git/


A hidden folder .git is created.

We can see hidden files from cmd.  Command:   dir/a



.git folder contains all the snapshots and changes that we have done in the code. When we make the commits, those changes are recorded in .git folder.


//////////////////////////////////////////////////

Command:


git status:  Tells us the status of the snapshots.


Output:

On branch master

No commits yet

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        Git practice txt file.txt
        phone.txt.txt

nothing added to commit but untracked files present (use "git add" to track)




Analysing the output:

The output is tellings us there are Untracked files. 
This means that we have made changes in these files but we haven't created any snapshots out of these yet.


///////////////////////////////////////////////////////////////////////////


Making the snapshot


git add filname.extension


This takes the snapshot of the changes in these files.


///////////////////////////////////////////////////////////////////////////


Adding message to the snapshots:

Whatever snapshot that we have taken, we may want to give some name/ message to it. We use git commit for that.


Command:

git commit -m "message"

Ex:

git commit -m "create phone"  

Message should be like a command.

Output:

[master (root-commit) 7bbca08] create phone
 1 file changed, 1 insertion(+)
 create mode 100644 phone.txt


////////////////////////////////////////////////////////////////////////////////

commit makes a snapshot named.


/////////////////////////////////////////////////////////////////////////////////


To see differences from previous version:


Command:

git diff


Output:

diff --git a/phone.txt b/phone.txt
index cd222a4..7ff9d38 100644
--- a/phone.txt
+++ b/phone.txt
@@ -1 +1,7 @@
-phone
\ No newline at end of file
+phone
+
+
+touchscreen
+
+
+scratchless- gorilla glass
\ No newline at end of file



///////////////////////////////////////////////////////////////////


Adding all the files in the path in one go:


Command:
git add .



////////////////////////////////////////////////////////////////////////


Logs:

Commands:

git log

Explanation: Gets us the logs of the commits.


Example:

Output:

commit e60380b204e923253e2ec4067f51580a5572b875 (HEAD -> master)
Author: Rachinder3 <rachindersingh@gmail.com>
Date:   Sat Oct 30 19:37:33 2021 +0530

    add touchscreen and scratch less to phone

commit 7bbca083d76163d195f658bf9b122d355f3ccdc5
Author: Rachinder3 <rachindersingh@gmail.com>
Date:   Sat Oct 30 19:17:17 2021 +0530

    create phone


////////////////////////////////////////////////////////////////////////////////////


We get some commit ids for each commit in the logs.

Ex: in above  7bbca083d76163d195f658bf9b122d355f3ccdc5 is 1 such commit id.


///////////////////////////////////////////////////////////////////////////////////


Checking what happened in each commit:

git checkout commit-id


Ex:

git checkout e60380b204e923253e2ec4067f51580a5572b875



Output:

Note: switching to 'e60380b204e923253e2ec4067f51580a5572b875'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at e60380b add touchscreen and scratch less to phone


Explanation:

In the last line we have a message "HEAD is now at e60380b".

e60380b is the starting of the commit id.


/////////////////////////////////////////////////////////////////////////////////////////////////////////////


**************
As soon as we have done this checkout, the code reverts to the that position whose id we have added.



Ex:
git checkout 7bbca083d76163d195f658bf9b122d355f3ccdc5


The status we had at this particular id is restored.
The files/code we had at this particular checkpoint is made permanent.


//////////////////////////////////////////////////////////////////////////////////////////////////


Explanation of checkout:


When we were commiting, suppose we commited twice and rolled out 2 features, say A and B.


Timeline:


A		B


We have a head. This head can be assumed as head of a linked list.

When we make new commit, the head pointer shifts and points to this new commit.

Whenever we do git checkout, whichever id we are checking out, head pointer goes to that particular commit.


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


=============================================================================================


Branches:





phone -----------------  Gorilla screen, touchscreen


Now we want to add internet service and music player.

We can give one piece of code to team internet and one piece of code to music player.


										time------------>

                       Internet---------------------------------------
                            |				    |
phone-------------------------------------------------------------------------------Gorilla,   Touch screen
	          |			|
        	        Music Player----------------


Now both Internet team and Music Player team will make their own branches. 


Now suppose music player developed first and added a button say play button. 

At certain point of time internet also gets added and suppose it adds a button called refresh.


When we keep both the codes side by side, we will have a conflict.

Git should ideally tell us about the conflict or tell the internet team there is a conflict and someone should decide which code to keep.

This case arised because changes were made on the same file. If changes were made on different files, git combines the codes on its own.


///////////////////////////////////////////////////////////////////////////////////////////////////////////////

================================================================================================

Git Making Branches:

Head pointer points two things:

1) The current version of the code..

2) Group of commits

Suppose we have this scenario:


A-----------B----------------------------------------------------------   Master


head points to B. It is also pointing towards master. 

when we do checkout to A, that reference to Master gets erased.

Even if we do checkout to B that reference doesn't come back.


If we start building on B, git doesn't know to which branch currently we are on.

It can create its own branch.


////////////////////////////////////////////////////////////////
To counter this:


Command:

git branch


Ex Output:

* (HEAD detached at e60380b)
  master
 

*****
We can do:

git checkout master


Output:

* master


The reference to master branch and the most recent commit on  this branch is restored.



////////////////////////////////////////////////////////////////////////////


What is branch:

The two teams in the earlier example, Internet and Music player are branches.


Command:

git checkout -b New Branch name


ex:

git checkout -b Internet



///////////////////////////////////////////////////////////////

We have created a new file called "Internet.txt"

Adding and commiting:

git add "internet.txt"


When we go back to master branch:


git checkout master



The file "Internet.txt" vanishes

///////////////////////////////////////////////////////////////



Git is called DCVS. De centralized Control Version system.


In above code, internet and music teams work independant of each other.


/////////////////////////////////////////////////////////////////////

When creating branch, better checkout to main first if we want to create branch from master

/////////////////////////////////////////////////////////////////////////

Adding music player branch:

git checkout -b music_player

git add .

git commit -m "Add Music Player"

///////////////////////////////////////////////////////////////////////////



===============================================================================================


GIT merging branches:



                       Internet---------------------------------------
                            |				    |
phone--------------------------------------------------------------------------------------------------------------------
	          |						|	
        	        Music Player------------------------------------------------------




Master Branch is the most stable version hence its termed as master branch.



All the tested features with no known bugs are merged into master. Unknown bugs may be there.


Command:

git merge Branch Name


Ex:

git merge internet



///////////////////////////////////////////////////////////////////////////////////////////////////


Deleting the useless branches:


git branch -d Branch name

ex:

git branch -d internet

=======================================================================================

Don't delete the master branch and we can't delete branch we are on.


==========================================================================================



Conflict resolution in git


When two teams working on same piece of code and they change same lines than conflict arises.

Git asks us which code is to be kept, maybe both, maybe one or maybe none.

/////////////////////////////////////////////////
When we are in vim editor.

Press i to insert data.

Press esc+: to give command.
wq commands closes and saves.

//////////////////////////////////////////////

If we have conflict, this error comes:


Auto-merging apps.txt
CONFLICT (content): Merge conflict in apps.txt
Automatic merge failed; fix conflicts and then commit the result.



/////////////////////////////////////////////////////////
The file which caused the conflict, when we open it we will see different files have been merged.

It also tells where the head was.

We can edit it accordingly and keep the desired data.


///////////////////////////////////////////////////////////////

===================================================================================================



GIT understanding workflows:


 Server: Computer located somewhere, accessible to all via IP address.

Server has master branch.

Different Teams work on different features.

Different teams have different branches.

Suppose T1 has branch B1, T2 has branch B2.



Master may be matained by github.


Team T1 and Team T2 pulled(download) this code to their systems.



When team T1 and T2 are done with their work, they send a request to main branch.

We can directly merge as well but bigger teams send a request called pull request to the main  team.



Once their branches are merged, someone from the main team sends this request to the main code kept on github.
The merged code gets updated on the server.


Repository just means a folder.


/////////////////////////////////////////////////////////////////////////////////////////////////


Feature Branch workflow:



yOU HAVE MAIN BRANCH, YOU create different branches to add and work on  features and then merge back the branches to main branch.

/////////////////////////////////////////////////////////////////


Git Flow Workflow:

You have a Test Server and Production Server.

All the new features are sent to test server where it is tested by the inner circle.

We have a master branch connected to production server.
We have a dev branch which is connected to the test server. All the feature branches make branch from this dev branch and merge back to dev branch.
Dev branch merges back to master.

We also have a hotfix branch which is used to fix the bugs. 
It merges into master and also merges to dev branch as well.
									Test Server		Production server

			---------Feature branch---------------------
 			|				|
 		----------------------------------------------------------------------------------------------------------------->  dev
		^					|                                                            ^
		|					   Merging back to master                 |		
--------------------------------------------------------------------------------------------------------------------------------->  Master
								|		        |		
								------------Hotfix branch--



Bigger teams practice git flow workflow, smaller teams practice Feature Branch workflow


////////////////////////////////////////////////////////////////////////////////////////////////////


============================================================================================


We have  a common repository from where all teams take code and pushes back to it. This repository is connected to the server
which is hosting our website.

This repository is hosted on websites like github, gitlabs etc.


=========================================================================================================

Create repository on github (already know how)

read me tells how to use the project


Local Repository and remote repository need to be connected.



======================================================================================================


Configuring somethings on system:


Command:
git config user.name  (helps establish who is pushing these changes, if we somethings goes wrong tommorow, the original dev will be able to fix the issues the fastest)



Ex:

git config user.name "Rachinder"

////////////////////////////////////////////////////////////////////////////////////////////

Configuring using email:

git config user.email "Email"


Ex:
git config user.email "rachindersingh@gmail.com"


///////////////////////////////////////////////////////////////////////////////


Connecting the remote repository with the local repository:

git remote add origin RepositoryLink


RepositoryLink can be found on the repository web page


Here, Ex:

git remote add origin https://github.com/Rachinder3/git_it.git


After previous command do:

git push origin master

Credential Manager will open, configure it to connect local and remote repositories.


//////////////////////////////////////////////////////////////////////


Only master branch is pushed with this.

To push other branches:

checkout to that branch.

Do git push origin branchname

Ex:

git checkout game
git push origin game


///////////////////////////////////////////////////////////////////////////////////


=======================================================================================


Github with a team memeber:


Downloading code from github:




Creating a directory:

mkdir directory name

///////////////////////////////////////////////////////////////////

Cloning the repository:


git clone RepositoryUrl

Ex:

git clone https://github.com/Rachinder3/git_it.git


//////////////////////////////////////////////////////////


By default branches may be hidden


To see all the branches:

git branch -r


Then we can checkout to these hidden branches.



////////////////////////////////////////////////////////////////////////////////////////////////////



Commands for seeting username on new system:

git config --list

git config credential.helper


git config credential.helper ''


Credential helper helps in clearing the stored username and password.


git config user.name "Team Member Name"

git config user.email "Team Member email"


///////////////////////////////////////////


When team mate tries to push the changes:

git  push origin Branch Name


Before doing this, we have to give access to the team member as well. To do this go to your github.
Go to your repository.
Go to seeting
Go to manage access
Add people from here


/////////////////////////////////////////////////////////////


The team member will get an email to give confirmation so that he/she can start contribuing to our repository.

Right now, the repository won't show up in Team Member's github but the Team Member can make contributions to our repository.



/////////////////////////////////////////////////////////////////

We can add other people's repositories to our github as well.

This is done via forking.

We can go to any repository and click fork there.


////////////////////////////////////////////////////////////////////


============================================================================================

Pull Requests and Workflows of Large Teams:

Team members can fork the code from main repository and add their features and changes.
They can push the codes to their repositories.


They can then send push requests to main repository which is connected to live server.


This is done via Pull request.


////////////////////////////////////////////////////////////////////////////////////


What is Pull request??

Team can fork the main repository to their own github.
They can then clone this, make changes to it and push the chages back to their repository.

They then create a pull request to the main team which is handling the main repository.

The main team verifies he code and merges it.


/////////////////////////////////////////////////////////////////////////////

Creating Pull request:

We can create a pull request in our repository. We need to be on the correct branch to create a pull request.

After the pull request is created, the owner can verify and merge. This is done in code reviews.


//////////////////////////////////////////////////////////////////////////////


While merging, we have three options to merge


1) Merge commit: The default coomitting and mergin of branches.
2) Squash and merge: Directly merged without creating any commit.
3) Rebase: Preffered while doing open source contributions. 


Merge:

o--------o------------o-------------o------------------------------> Main Branch
    |                   |  				|
    |                   |Commits merged w.r.t time	|
     -------------o------------------------------------------ o



Rebase:


o--------o------------o-------------o------------------------------> Main Branch            o-----------------o---------->  Commits added in one go after the main branch
    |                     				
    |                  	
     -------------o------------------------------------------ o



If anything goes wrong you can simply remove the added Commits. It won't be easy to do that in normal merge as they are merged w.r.t time
That's why open source platforms prefer Rebase.


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////


=================================================================================


In linux, touch filename creates a blank file with name "filename"


//////////////////////////////////////////////////////////////////////////////////////////////////////////////////


Creating directories from within python:

os.mkdir


////////////////////////////////////////////////////////////////////////////////////////////////////////////

os.path.join(path1,path2)
Helps in creating new paths


////////////////////////////////////////////////////////////////////////////////////////////////////////

Copy file from one location to another:


from shutil import copyfile
copyfile(src, dst)

//////////////////////////////////////////////////////////////////////////////////////////////////////


Image Data Generator: Used for working with images.
Can also perform data augmentation with this. Data Augmentation helps in data augmentation.

Helps in one hot encoding the targets.


importing:

from keras.preprocessing.image import ImageDataGenerator



We have to provide a source to Image Data Generator class. This source tells from where to get the data.
The source can be a directory as well as dataframe.


Code:


train_dir="master_data\\training"
test_dir="master_data\\testing"


train_data_gen=ImageDataGenerator(rescale=1.0/255)

train_data_generater=train_data_gen.flow_from_directory(
                                                    train_dir, 
                                                    target_size=(100,100),
                                                    class_mode='categorical'              
                                                                        )


We use fit_generator instead of fit to work with data generator


////////////////////////////////////////////////////////////////////////////////////////////////////////


Adam optimizer:

We can import adam optimizer from tf.keras.optimizers.Adam


What this ensures is we can pass learning rate and other parameters as well to this optimizer


////////////////////////////////////////////////////////////////////////////////////////////////////////


Early stopping:
Importing:
from tensorflow.keras.callbacks import EarlyStopping

Code:

es=EarlyStopping(monitor="val_acc", patience=2)



We can pass this early stoping
////////////////////////////////////////////////////////////////////////////////////////////////////////

For prediction, we can use np.array module and PIL.Image modules.

While giving for prediction, for any image:

from PIL import Image
img=Image.open("Image Path")
img=img.resize((100,100))

input_image=np.array(img)/255
input_image[np.newaxis, ...]

res=model.predict(input_image[np.newaxis,...])



Can follow this:
https://www.youtube.com/watch?v=LsdxvjLWkIY&t=890s


////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

================================================================


keras.preprocessing.text.Tokenizer:

This class allows to vectorize a text corpus, by turning each
text into either a sequence of integers (each integer being the index
of a token in a dictionary) or into a vector where the coefficient
for each token could be binary, based on word count, based on tf-idf...

///////////////////////////////////////////////////////////////////////////////
Parameters:

oov_token: if given, it will be added to word_index and used to
        replace out-of-vocabulary words during text_to_sequence calls

char_level: if True, every character will be treated as a token.

//////////////////////////////////////////////////////////////////////////////

Code:

tokenizer=Tokenizer(oov_token='<UNK>')
tokenizer.fit_on_texts(sentences)

To see the word indexes:

tokenizer.word_index


sequences= tokenizer.texts_to_sequence()

/////////////////////////////////////////////////////////////////////////////////



padding:

Pads sequences to the same length.


padded_sequence= pad_sequences(input_sequences,maxlen=max_sequence_len)


/////////////////////////////////////////////////////////////////////////////////////////


One hot encoding targets:

tf.keras.util.to_categorical

////////////////////////////////////////////////////////////////////////////////////////////////


Embedding:

Turns positive integers (indexes) into dense vectors of fixed size.

We have text data. We converted the text data into sequence of numbers. We build our model around these embeddings.
Embeddings represent word in n dimensional space.

 
During the training, the layer learns the weights to build these vectors. The real gets these vectors only for different different words.


Bidirectional:

Bidirectional wrapper for RNNs.
Bidirectional means it is taking the context from both the directions while predicting.


////////////////////////////////////////////////////////////////////////


To see how training and testing loss changed:


import keras
from matplotlib import pyplot as plt
history = model1.fit(train_x, train_y,validation_split = 0.1, epochs=50, batch_size=4)
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()


////////////////////////////////////////////////////////////////////////////////////




